/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 0.7339
Epoch 2 Loss: 0.7546
Epoch 3 Loss: 0.5752
Epoch 4 Loss: 0.5651
Epoch 5 Loss: 0.5580
Epoch 6 Loss: 0.5487
Epoch 7 Loss: 0.5408
Epoch 8 Loss: 0.5329
Epoch 9 Loss: 0.5258
Epoch 10 Loss: 0.5173
Epoch 11 Loss: 0.5080
Epoch 12 Loss: 0.5005
Epoch 13 Loss: 0.4969
Epoch 14 Loss: 0.4916
Epoch 14 best val f1 0.0188 test f1 0.3536597788333893
Epoch 15 Loss: 0.4843
Epoch 15 best val f1 0.1478 test f1 0.3757784068584442
Epoch 16 Loss: 0.4792
Epoch 16 best val f1 0.2946 test f1 0.39309990406036377
Epoch 17 Loss: 0.4748
Epoch 17 best val f1 0.4207 test f1 0.4059448540210724
Epoch 18 Loss: 0.4656
Epoch 18 best val f1 0.4889 test f1 0.4159332513809204
Epoch 19 Loss: 0.4607
Epoch 19 best val f1 0.4938 test f1 0.4252660572528839
Epoch 20 Loss: 0.4510
Epoch 20 best val f1 0.4984 test f1 0.4323091506958008
Epoch 21 Loss: 0.4441
Epoch 21 best val f1 0.5077 test f1 0.4386726915836334
Epoch 22 Loss: 0.4378
Epoch 23 Loss: 0.4315
Epoch 24 Loss: 0.4256
Epoch 25 Loss: 0.4314
Epoch 26 Loss: 0.4201
Epoch 27 Loss: 0.4110
Epoch 28 Loss: 0.4143
Epoch 29 Loss: 0.4114
Epoch 30 Loss: 0.4011
Epoch 31 Loss: 0.3920
Epoch 32 Loss: 0.3823
Epoch 33 Loss: 0.3820
Epoch 34 Loss: 0.3713
Epoch 34 best val f1 0.5088 test f1 0.4793378412723541
Epoch 35 Loss: 0.3820
Epoch 36 Loss: 0.3794
Epoch 37 Loss: 0.3829
Epoch 38 Loss: 0.3706
Epoch 39 Loss: 0.3651
Epoch 40 Loss: 0.3667
Epoch 41 Loss: 0.3584
Epoch 42 Loss: 0.3583
Epoch 42 best val f1 0.5194 test f1 0.4675154983997345
Epoch 43 Loss: 0.3600
Epoch 44 Loss: 0.3511
Epoch 45 Loss: 0.3483
Epoch 46 Loss: 0.3480
Epoch 47 Loss: 0.3449
Epoch 48 Loss: 0.3508
Epoch 49 Loss: 0.3440
Epoch 50 Loss: 0.3522
Epoch 51 Loss: 0.3480
Epoch 52 Loss: 0.3452
Epoch 53 Loss: 0.3420
Epoch 54 Loss: 0.3433
Epoch 55 Loss: 0.3373
Epoch 56 Loss: 0.3381
Epoch 57 Loss: 0.3381
Epoch 58 Loss: 0.3380
Epoch 59 Loss: 0.3354
Epoch 60 Loss: 0.3375
Epoch 61 Loss: 0.3349
Epoch 62 Loss: 0.3338
Epoch 63 Loss: 0.3331
Epoch 64 Loss: 0.3332
Epoch 65 Loss: 0.3346
Epoch 66 Loss: 0.3614
Epoch 67 Loss: 0.3752
Epoch 68 Loss: 0.3829
Epoch 69 Loss: 0.4111
Epoch 70 Loss: 0.4027
Epoch 71 Loss: 0.3950
Epoch 72 Loss: 0.3861
Epoch 73 Loss: 0.3876
Epoch 74 Loss: 0.3801
Epoch 75 Loss: 0.3721
Epoch 76 Loss: 0.3695
Epoch 77 Loss: 0.3605
Epoch 78 Loss: 0.3564
Epoch 79 Loss: 0.3527
Epoch 80 Loss: 0.3558
Epoch 81 Loss: 0.3678
Epoch 82 Loss: 0.3675
Epoch 83 Loss: 0.3727
Epoch 84 Loss: 0.3858
Epoch 85 Loss: 0.3827
Epoch 86 Loss: 0.3758
Epoch 87 Loss: 0.3711
Epoch 88 Loss: 0.3593
Epoch 89 Loss: 0.3578
Epoch 90 Loss: 0.3533
Epoch 91 Loss: 0.3490
Epoch 92 Loss: 0.3452
Epoch 93 Loss: 0.3441
Epoch 94 Loss: 0.3760
Epoch 95 Loss: 0.3888
Epoch 96 Loss: 0.3942
Epoch 97 Loss: 0.3935
Epoch 98 Loss: 0.3861
Epoch 99 Loss: 0.3771
Epoch 100 Loss: 0.3686
Epoch 101 Loss: 0.3622
Epoch 102 Loss: 0.3585
Epoch 103 Loss: 0.3906
Epoch 104 Loss: 0.4104
Epoch 104 best val f1 0.5237 test f1 0.45531484484672546
Epoch 105 Loss: 0.4133
Epoch 106 Loss: 0.4144
Epoch 107 Loss: 0.4062
Epoch 108 Loss: 0.4061
Epoch 109 Loss: 0.4029
Epoch 110 Loss: 0.4015
Epoch 111 Loss: 0.3972
Epoch 112 Loss: 0.3911
Epoch 113 Loss: 0.3934
Epoch 114 Loss: 0.3979
Epoch 115 Loss: 0.3949
Epoch 116 Loss: 0.3858
Epoch 117 Loss: 0.3820
Epoch 118 Loss: 0.3772
Epoch 119 Loss: 0.3711
Epoch 120 Loss: 0.3863
Epoch 121 Loss: 0.3921
Epoch 122 Loss: 0.3933
Epoch 123 Loss: 0.3929
Epoch 124 Loss: 0.3973
Epoch 125 Loss: 0.4024
Epoch 126 Loss: 0.3972
Epoch 127 Loss: 0.3923
Epoch 128 Loss: 0.3846
Epoch 129 Loss: 0.3813
Epoch 130 Loss: 0.3950
Epoch 131 Loss: 0.4030
Epoch 132 Loss: 0.3966
Epoch 133 Loss: 0.3940
Epoch 134 Loss: 0.3890
Epoch 135 Loss: 0.3851
Epoch 136 Loss: 0.3835
Epoch 137 Loss: 0.3816
Epoch 138 Loss: 0.3858
Epoch 139 Loss: 0.3872
Epoch 140 Loss: 0.3836
Epoch 141 Loss: 0.3824
Epoch 142 Loss: 0.3732
Epoch 143 Loss: 0.3725
Epoch 144 Loss: 0.3739
Epoch 145 Loss: 0.3783
Epoch 146 Loss: 0.3742
Epoch 147 Loss: 0.3702
Epoch 148 Loss: 0.3645
Epoch 149 Loss: 0.3632
Epoch 150 Loss: 0.3588
Epoch 151 Loss: 0.3567
Epoch 152 Loss: 0.3582
Epoch 153 Loss: 0.3572
Epoch 154 Loss: 0.3529
Epoch 155 Loss: 0.3481
Epoch 156 Loss: 0.3488
Epoch 157 Loss: 0.3455
Epoch 158 Loss: 0.3455
Epoch 159 Loss: 0.3420
Epoch 160 Loss: 0.3473
Epoch 161 Loss: 0.3415
Epoch 162 Loss: 0.3424
Epoch 163 Loss: 0.3404
Epoch 164 Loss: 0.3397
Epoch 165 Loss: 0.3386
Epoch 166 Loss: 0.3391
Epoch 167 Loss: 0.3390
Epoch 168 Loss: 0.3350
Epoch 169 Loss: 0.3458
Epoch 170 Loss: 0.3795
Epoch 171 Loss: 0.3922
Epoch 172 Loss: 0.3947
Epoch 173 Loss: 0.3907
Epoch 174 Loss: 0.3879
Epoch 175 Loss: 0.3836
Epoch 176 Loss: 0.3798
Epoch 177 Loss: 0.3769
Epoch 178 Loss: 0.3730
Epoch 179 Loss: 0.3714
Epoch 180 Loss: 0.3787
Epoch 181 Loss: 0.3778
Epoch 182 Loss: 0.3760
Epoch 183 Loss: 0.3768
Epoch 184 Loss: 0.3716
Epoch 185 Loss: 0.3753
Epoch 186 Loss: 0.4174
Epoch 187 Loss: 0.4227
Epoch 188 Loss: 0.4279
Epoch 189 Loss: 0.4306
Epoch 190 Loss: 0.4255
Epoch 191 Loss: 0.4247
Epoch 192 Loss: 0.4153
Epoch 193 Loss: 0.4119
Epoch 194 Loss: 0.4070
Epoch 195 Loss: 0.4080
Epoch 196 Loss: 0.4000
Epoch 197 Loss: 0.3941
Epoch 198 Loss: 0.3919
Epoch 199 Loss: 0.3894
Epoch 200 Loss: 0.3819