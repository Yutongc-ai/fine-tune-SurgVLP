/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 1.2488
Epoch 1 best val f1 0.4241 test f1 0.26460421085357666
Epoch 2 Loss: 0.6075
Epoch 3 Loss: 0.5970
Epoch 4 Loss: 0.5846
Epoch 5 Loss: 0.5761
Epoch 6 Loss: 0.5687
Epoch 7 Loss: 0.5620
Epoch 8 Loss: 0.5576
Epoch 9 Loss: 0.5525
Epoch 10 Loss: 0.5476
Epoch 11 Loss: 0.5445
Epoch 12 Loss: 0.5408
Epoch 13 Loss: 0.5381
Epoch 14 Loss: 0.5347
Epoch 15 Loss: 0.5308
Epoch 16 Loss: 0.5292
Epoch 17 Loss: 0.5269
Epoch 18 Loss: 0.5246
Epoch 18 best val f1 0.5000 test f1 0.377758264541626
Epoch 19 Loss: 0.5225
Epoch 19 best val f1 0.5077 test f1 0.38029003143310547
Epoch 20 Loss: 0.5200
Epoch 21 Loss: 0.5167
Epoch 22 Loss: 0.5175
Epoch 23 Loss: 0.5175
Epoch 24 Loss: 0.5126
Epoch 25 Loss: 0.5119
Epoch 26 Loss: 0.5123
Epoch 27 Loss: 0.5113
Epoch 28 Loss: 0.5119
Epoch 29 Loss: 0.5116
Epoch 30 Loss: 0.5094
Epoch 31 Loss: 0.5093
Epoch 32 Loss: 0.5069
Epoch 33 Loss: 0.5074
Epoch 34 Loss: 0.5079
Epoch 35 Loss: 0.5080
Epoch 36 Loss: 0.5093
Epoch 37 Loss: 0.5057
Epoch 38 Loss: 0.5074
Epoch 39 Loss: 0.5078
Epoch 40 Loss: 0.5069
Epoch 41 Loss: 0.5076
Epoch 42 Loss: 0.5056
Epoch 43 Loss: 0.5059
Epoch 44 Loss: 0.5059
Epoch 45 Loss: 0.5060
Epoch 46 Loss: 0.5057
Epoch 47 Loss: 0.5033
Epoch 48 Loss: 0.5028
Epoch 49 Loss: 0.5029
Epoch 50 Loss: 0.5054
Epoch 51 Loss: 0.5004
Epoch 52 Loss: 0.5040
Epoch 53 Loss: 0.5033
Epoch 54 Loss: 0.4996
Epoch 55 Loss: 0.5012
Epoch 56 Loss: 0.5009
Epoch 57 Loss: 0.5000
Epoch 58 Loss: 0.5004
Epoch 59 Loss: 0.5023
Epoch 60 Loss: 0.5031
Epoch 61 Loss: 0.5027
Epoch 62 Loss: 0.5011
Epoch 63 Loss: 0.5028
Epoch 64 Loss: 0.5028
Epoch 65 Loss: 0.4990
Epoch 66 Loss: 0.5008
Epoch 67 Loss: 0.4990
Epoch 68 Loss: 0.4986
Epoch 69 Loss: 0.4964
Epoch 70 Loss: 0.4960
Epoch 71 Loss: 0.4950
Epoch 72 Loss: 0.4982
Epoch 73 Loss: 0.4944
Epoch 74 Loss: 0.4948
Epoch 75 Loss: 0.4991
Epoch 76 Loss: 0.4953
Epoch 77 Loss: 0.4967
Epoch 78 Loss: 0.4980
Epoch 79 Loss: 0.4944
Epoch 80 Loss: 0.4937
Epoch 81 Loss: 0.4989
Epoch 82 Loss: 0.5009
Epoch 83 Loss: 0.5047
Epoch 84 Loss: 0.5047
Epoch 85 Loss: 0.5032
Epoch 86 Loss: 0.5019
Epoch 87 Loss: 0.4999
Epoch 88 Loss: 0.5015
Epoch 89 Loss: 0.4993
Epoch 90 Loss: 0.5011
Epoch 91 Loss: 0.4982
Epoch 92 Loss: 0.4968
Epoch 93 Loss: 0.4957
Epoch 94 Loss: 0.4998
Epoch 95 Loss: 0.4974
Epoch 96 Loss: 0.4975
Epoch 97 Loss: 0.4984
Epoch 98 Loss: 0.5003
Epoch 99 Loss: 0.5015
Epoch 100 Loss: 0.4998