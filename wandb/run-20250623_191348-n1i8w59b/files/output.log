/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  4
num_shots is  4
Epoch 1 Loss: 1.0332
Epoch 1 best val f1 0.4194 test f1 0.3052513003349304
Epoch 2 Loss: 0.8236
Epoch 3 Loss: 0.7459
Epoch 4 Loss: 0.7026
Epoch 5 Loss: 0.6738
Epoch 6 Loss: 0.6520
Epoch 7 Loss: 0.6339
Epoch 8 Loss: 0.6186
Epoch 9 Loss: 0.6056
Epoch 10 Loss: 0.5940
Epoch 11 Loss: 0.5837
Epoch 12 Loss: 0.5747
Epoch 13 Loss: 0.5664
Epoch 13 best val f1 0.4310 test f1 0.3044707775115967
Epoch 14 Loss: 0.5585
Epoch 14 best val f1 0.4513 test f1 0.30440399050712585
Epoch 15 Loss: 0.5510
Epoch 15 best val f1 0.4844 test f1 0.30429136753082275
Epoch 16 Loss: 0.5439
Epoch 17 Loss: 0.5368
Epoch 18 Loss: 0.5298
Epoch 19 Loss: 0.5229
Epoch 20 Loss: 0.5161
Epoch 21 Loss: 0.5093
Epoch 22 Loss: 0.5026
Epoch 23 Loss: 0.4985
Epoch 24 Loss: 0.4957
Epoch 25 Loss: 0.4934
Epoch 26 Loss: 0.4908
Epoch 27 Loss: 0.4876
Epoch 28 Loss: 0.4835
Epoch 29 Loss: 0.4787
Epoch 30 Loss: 0.4734
Epoch 31 Loss: 0.4676
Epoch 32 Loss: 0.4616
Epoch 33 Loss: 0.4556
Epoch 33 best val f1 0.5385 test f1 0.3003953993320465
Epoch 34 Loss: 0.4496
Epoch 35 Loss: 0.4439
Epoch 36 Loss: 0.4384
Epoch 37 Loss: 0.4329
Epoch 38 Loss: 0.4278
Epoch 39 Loss: 0.4229
Epoch 40 Loss: 0.4182
Epoch 41 Loss: 0.4138
Epoch 42 Loss: 0.4096
Epoch 43 Loss: 0.4056
Epoch 44 Loss: 0.4019
Epoch 45 Loss: 0.3985
Epoch 46 Loss: 0.3955
Epoch 47 Loss: 0.3925
Epoch 48 Loss: 0.3895
Epoch 49 Loss: 0.3863
Epoch 50 Loss: 0.3876
Epoch 51 Loss: 0.3910
Epoch 52 Loss: 0.3925
Epoch 53 Loss: 0.3918
Epoch 54 Loss: 0.3900
Epoch 55 Loss: 0.3873
Epoch 56 Loss: 0.3838
Epoch 57 Loss: 0.3800
Epoch 58 Loss: 0.3765
Epoch 59 Loss: 0.3729
Epoch 60 Loss: 0.3693
Epoch 61 Loss: 0.3660
Epoch 62 Loss: 0.3680
Epoch 63 Loss: 0.3680
Epoch 64 Loss: 0.3653
Epoch 65 Loss: 0.3626
Epoch 66 Loss: 0.3609
Epoch 67 Loss: 0.3670
Epoch 68 Loss: 0.3760
Epoch 69 Loss: 0.3799
Epoch 70 Loss: 0.3787
Epoch 71 Loss: 0.3753
Epoch 72 Loss: 0.3713
Epoch 73 Loss: 0.3672
Epoch 74 Loss: 0.3631
Epoch 75 Loss: 0.3588
Epoch 76 Loss: 0.3548
Epoch 77 Loss: 0.3513
Epoch 78 Loss: 0.3481
Epoch 79 Loss: 0.3448
Epoch 80 Loss: 0.3418
Epoch 81 Loss: 0.3390
Epoch 82 Loss: 0.3422
Epoch 83 Loss: 0.3492
Epoch 84 Loss: 0.3525
Epoch 85 Loss: 0.3518
Epoch 86 Loss: 0.3493
Epoch 87 Loss: 0.3468
Epoch 88 Loss: 0.3447
Epoch 89 Loss: 0.3420
Epoch 90 Loss: 0.3387
Epoch 91 Loss: 0.3356
Epoch 92 Loss: 0.3329
Epoch 93 Loss: 0.3314
Epoch 94 Loss: 0.3296
Epoch 95 Loss: 0.3273
Epoch 96 Loss: 0.3252
Epoch 97 Loss: 0.3226
Epoch 98 Loss: 0.3202
Epoch 99 Loss: 0.3180
Epoch 100 Loss: 0.3157