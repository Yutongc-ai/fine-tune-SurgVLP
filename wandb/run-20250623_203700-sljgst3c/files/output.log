/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 1.4651
Epoch 1 best val f1 0.4209 test f1 0.30564677715301514
Epoch 2 Loss: 0.7837
Epoch 3 Loss: 0.6629
Epoch 4 Loss: 0.6166
Epoch 5 Loss: 0.5924
Epoch 6 Loss: 0.5823
Epoch 7 Loss: 0.5790
Epoch 8 Loss: 0.5682
Epoch 9 Loss: 0.5609
Epoch 10 Loss: 0.5525
Epoch 11 Loss: 0.5475
Epoch 12 Loss: 0.5443
Epoch 13 Loss: 0.5381
Epoch 14 Loss: 0.5334
Epoch 15 Loss: 0.5278
Epoch 16 Loss: 0.5244
Epoch 17 Loss: 0.5216
Epoch 18 Loss: 0.5167
Epoch 18 best val f1 0.5047 test f1 0.30313414335250854
Epoch 19 Loss: 0.5136
Epoch 20 Loss: 0.5098
Epoch 21 Loss: 0.5097
Epoch 22 Loss: 0.5085
Epoch 23 Loss: 0.5069
Epoch 24 Loss: 0.5044
Epoch 25 Loss: 0.5027
Epoch 26 Loss: 0.5038
Epoch 27 Loss: 0.5011
Epoch 28 Loss: 0.4998
Epoch 29 Loss: 0.5032
Epoch 30 Loss: 0.5027
Epoch 31 Loss: 0.5036
Epoch 32 Loss: 0.5007
Epoch 33 Loss: 0.5006
Epoch 34 Loss: 0.4979
Epoch 35 Loss: 0.5003
Epoch 36 Loss: 0.4953
Epoch 37 Loss: 0.4923
Epoch 38 Loss: 0.4890
Epoch 39 Loss: 0.4873
Epoch 40 Loss: 0.4915
Epoch 41 Loss: 0.4903
Epoch 42 Loss: 0.4887
Epoch 43 Loss: 0.4899
Epoch 43 best val f1 0.5260 test f1 0.22608433663845062
Epoch 44 Loss: 0.4991
Epoch 45 Loss: 0.4983
Epoch 46 Loss: 0.4956
Epoch 47 Loss: 0.4902
Epoch 48 Loss: 0.4868
Epoch 48 best val f1 0.5276 test f1 0.21276156604290009
Epoch 49 Loss: 0.4842
Epoch 49 best val f1 0.5319 test f1 0.21013064682483673
Epoch 50 Loss: 0.4803
Epoch 50 best val f1 0.5333 test f1 0.20754095911979675
Epoch 51 Loss: 0.4752
Epoch 52 Loss: 0.4762
Epoch 53 Loss: 0.4705
Epoch 54 Loss: 0.4664
Epoch 55 Loss: 0.4752
Epoch 56 Loss: 0.4777
Epoch 57 Loss: 0.4803
Epoch 58 Loss: 0.4744
Epoch 59 Loss: 0.4721
Epoch 60 Loss: 0.4728
Epoch 61 Loss: 0.4663
Epoch 62 Loss: 0.4628
Epoch 63 Loss: 0.4601
Epoch 64 Loss: 0.4546
Epoch 65 Loss: 0.4529
Epoch 66 Loss: 0.4534
Epoch 67 Loss: 0.4645
Epoch 68 Loss: 0.4939
Epoch 69 Loss: 0.4957
Epoch 70 Loss: 0.4963
Epoch 71 Loss: 0.4971
Epoch 72 Loss: 0.4972
Epoch 73 Loss: 0.4925
Epoch 74 Loss: 0.4915
Epoch 75 Loss: 0.4922
Epoch 76 Loss: 0.4924
Epoch 77 Loss: 0.4923
Epoch 78 Loss: 0.4896
Epoch 79 Loss: 0.4884
Epoch 80 Loss: 0.4840
Epoch 81 Loss: 0.4814
Epoch 82 Loss: 0.4769
Epoch 83 Loss: 0.4749
Epoch 84 Loss: 0.4744
Epoch 85 Loss: 0.4717
Epoch 86 Loss: 0.4689
Epoch 87 Loss: 0.4617
Epoch 88 Loss: 0.4601
Epoch 89 Loss: 0.4562
Epoch 90 Loss: 0.4492
Epoch 91 Loss: 0.4441
Epoch 92 Loss: 0.4396
Epoch 93 Loss: 0.4334
Epoch 94 Loss: 0.4336
Epoch 95 Loss: 0.4293
Epoch 96 Loss: 0.4285
Epoch 97 Loss: 0.4315
Epoch 98 Loss: 0.4455
Epoch 99 Loss: 0.4502
Epoch 100 Loss: 0.4449