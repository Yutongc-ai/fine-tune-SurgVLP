/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  4
num_shots is  4
Epoch 1 Loss: 0.8502
Epoch 1 best val f1 0.3934 test f1 0.3057743310928345
Epoch 2 Loss: 0.8362
Epoch 3 Loss: 0.8235
Epoch 4 Loss: 0.8119
Epoch 5 Loss: 0.8013
Epoch 6 Loss: 0.7916
Epoch 7 Loss: 0.7825
Epoch 8 Loss: 0.7740
Epoch 9 Loss: 0.7676
Epoch 10 Loss: 0.7619
Epoch 11 Loss: 0.7567
Epoch 12 Loss: 0.7518
Epoch 13 Loss: 0.7471
Epoch 14 Loss: 0.7428
Epoch 15 Loss: 0.7386
Epoch 16 Loss: 0.7345
Epoch 17 Loss: 0.7306
Epoch 18 Loss: 0.7268
Epoch 19 Loss: 0.7231
Epoch 20 Loss: 0.7195
Epoch 21 Loss: 0.7160
Epoch 21 best val f1 0.3951 test f1 0.3057771325111389
Epoch 22 Loss: 0.7126
Epoch 23 Loss: 0.7093
Epoch 23 best val f1 0.3967 test f1 0.3057785630226135
Epoch 24 Loss: 0.7061
Epoch 24 best val f1 0.3983 test f1 0.3057785630226135
Epoch 25 Loss: 0.7030
Epoch 26 Loss: 0.7001
Epoch 27 Loss: 0.6973
Epoch 28 Loss: 0.6945
Epoch 29 Loss: 0.6918
Epoch 30 Loss: 0.6892
Epoch 30 best val f1 0.4017 test f1 0.30582401156425476
Epoch 31 Loss: 0.6866
Epoch 32 Loss: 0.6842
Epoch 33 Loss: 0.6818
Epoch 34 Loss: 0.6795
Epoch 34 best val f1 0.4053 test f1 0.30597037076950073
Epoch 35 Loss: 0.6773
Epoch 35 best val f1 0.4126 test f1 0.3060314953327179
Epoch 36 Loss: 0.6751
Epoch 36 best val f1 0.4201 test f1 0.30612680315971375
Epoch 37 Loss: 0.6729
Epoch 37 best val f1 0.4240 test f1 0.3062705993652344
Epoch 38 Loss: 0.6708
Epoch 39 Loss: 0.6688
Epoch 39 best val f1 0.4286 test f1 0.3066670000553131
Epoch 40 Loss: 0.6668
Epoch 40 best val f1 0.4327 test f1 0.3069557249546051
Epoch 41 Loss: 0.6650
Epoch 41 best val f1 0.4412 test f1 0.3073224127292633
Epoch 42 Loss: 0.6632
Epoch 42 best val f1 0.4455 test f1 0.3077685534954071
Epoch 43 Loss: 0.6613
Epoch 44 Loss: 0.6596
Epoch 45 Loss: 0.6577
Epoch 46 Loss: 0.6560
Epoch 47 Loss: 0.6542
Epoch 48 Loss: 0.6525
Epoch 48 best val f1 0.4497 test f1 0.31271836161613464
Epoch 49 Loss: 0.6508
Epoch 49 best val f1 0.4540 test f1 0.3141142427921295
Epoch 50 Loss: 0.6490
Epoch 51 Loss: 0.6473
Epoch 51 best val f1 0.4586 test f1 0.31744518876075745
Epoch 52 Loss: 0.6456
Epoch 52 best val f1 0.4706 test f1 0.31932130455970764
Epoch 53 Loss: 0.6449
Epoch 53 best val f1 0.4768 test f1 0.3214225471019745
Epoch 54 Loss: 0.6447
Epoch 54 best val f1 0.4933 test f1 0.32387998700141907
Epoch 55 Loss: 0.6444
Epoch 55 best val f1 0.5000 test f1 0.3265724182128906
Epoch 56 Loss: 0.6437
Epoch 56 best val f1 0.5034 test f1 0.32962948083877563
Epoch 57 Loss: 0.6430
Epoch 58 Loss: 0.6423
Epoch 59 Loss: 0.6416
Epoch 60 Loss: 0.6406
Epoch 61 Loss: 0.6396
Epoch 62 Loss: 0.6386
Epoch 63 Loss: 0.6375
Epoch 64 Loss: 0.6362
Epoch 65 Loss: 0.6349
Epoch 65 best val f1 0.5120 test f1 0.36533427238464355
Epoch 66 Loss: 0.6336
Epoch 66 best val f1 0.5203 test f1 0.3703303933143616
Epoch 67 Loss: 0.6321
Epoch 68 Loss: 0.6306
Epoch 68 best val f1 0.5289 test f1 0.3803609609603882
Epoch 69 Loss: 0.6291
Epoch 69 best val f1 0.5424 test f1 0.3847784698009491
Epoch 70 Loss: 0.6276
Epoch 71 Loss: 0.6260
Epoch 71 best val f1 0.5487 test f1 0.3942742347717285
Epoch 72 Loss: 0.6245
Epoch 72 best val f1 0.5636 test f1 0.3989221751689911
Epoch 73 Loss: 0.6229
Epoch 74 Loss: 0.6213
Epoch 75 Loss: 0.6197
Epoch 76 Loss: 0.6181
Epoch 77 Loss: 0.6164
Epoch 78 Loss: 0.6148
Epoch 79 Loss: 0.6133
Epoch 80 Loss: 0.6117
Epoch 81 Loss: 0.6101
Epoch 82 Loss: 0.6085
Epoch 83 Loss: 0.6070
Epoch 84 Loss: 0.6055
Epoch 85 Loss: 0.6039
Epoch 86 Loss: 0.6024
Epoch 87 Loss: 0.6009
Epoch 88 Loss: 0.5994
Epoch 89 Loss: 0.5980
Epoch 90 Loss: 0.5965
Epoch 91 Loss: 0.5951
Epoch 92 Loss: 0.5936
Epoch 93 Loss: 0.5922
Epoch 94 Loss: 0.5908
Epoch 95 Loss: 0.5895
Epoch 96 Loss: 0.5881
Epoch 97 Loss: 0.5868
Epoch 98 Loss: 0.5855
Epoch 99 Loss: 0.5842
Epoch 100 Loss: 0.5829