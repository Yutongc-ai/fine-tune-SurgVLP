/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 1.2305
Epoch 1 best val f1 0.4241 test f1 0.26057732105255127
Epoch 2 Loss: 0.6376
Epoch 3 Loss: 0.6088
Epoch 4 Loss: 0.5935
Epoch 5 Loss: 0.5815
Epoch 6 Loss: 0.5686
Epoch 7 Loss: 0.5580
Epoch 8 Loss: 0.5514
Epoch 9 Loss: 0.5439
Epoch 10 Loss: 0.5372
Epoch 11 Loss: 0.5304
Epoch 12 Loss: 0.5284
Epoch 13 Loss: 0.5278
Epoch 14 Loss: 0.5233
Epoch 15 Loss: 0.5193
Epoch 15 best val f1 0.5204 test f1 0.34291595220565796
Epoch 16 Loss: 0.5153
Epoch 16 best val f1 0.5325 test f1 0.344896525144577
Epoch 17 Loss: 0.5114
Epoch 18 Loss: 0.5111
Epoch 19 Loss: 0.5121
Epoch 20 Loss: 0.5140
Epoch 21 Loss: 0.5129
Epoch 22 Loss: 0.5124
Epoch 23 Loss: 0.5121
Epoch 24 Loss: 0.5101
Epoch 25 Loss: 0.5087
Epoch 26 Loss: 0.5039
Epoch 27 Loss: 0.5015
Epoch 28 Loss: 0.4996
Epoch 28 best val f1 0.5399 test f1 0.35862359404563904
Epoch 29 Loss: 0.4957
Epoch 30 Loss: 0.4935
Epoch 31 Loss: 0.4928
Epoch 32 Loss: 0.4862
Epoch 33 Loss: 0.4796
Epoch 34 Loss: 0.4731
Epoch 35 Loss: 0.4667
Epoch 36 Loss: 0.4623
Epoch 37 Loss: 0.4565
Epoch 38 Loss: 0.4525
Epoch 39 Loss: 0.4482
Epoch 40 Loss: 0.4417
Epoch 41 Loss: 0.4377
Epoch 41 best val f1 0.5415 test f1 0.37906166911125183
Epoch 42 Loss: 0.4328
Epoch 42 best val f1 0.5432 test f1 0.38179317116737366
Epoch 43 Loss: 0.4291
Epoch 44 Loss: 0.4252
Epoch 45 Loss: 0.4667
Epoch 46 Loss: 0.4829
Epoch 47 Loss: 0.4839
Epoch 48 Loss: 0.4797
Epoch 49 Loss: 0.4718
Epoch 50 Loss: 0.4671
Epoch 51 Loss: 0.4631
Epoch 52 Loss: 0.4583
Epoch 53 Loss: 0.4556
Epoch 54 Loss: 0.4508
Epoch 55 Loss: 0.4501
Epoch 56 Loss: 0.4456
Epoch 57 Loss: 0.4473
Epoch 58 Loss: 0.4474
Epoch 59 Loss: 0.4471
Epoch 59 best val f1 0.5579 test f1 0.39380529522895813
Epoch 60 Loss: 0.4436
Epoch 61 Loss: 0.4408
Epoch 62 Loss: 0.4393
Epoch 63 Loss: 0.4336
Epoch 64 Loss: 0.4320
Epoch 65 Loss: 0.4304
Epoch 66 Loss: 0.4288
Epoch 67 Loss: 0.4311
Epoch 68 Loss: 0.4374
Epoch 69 Loss: 0.4391
Epoch 70 Loss: 0.4343
Epoch 71 Loss: 0.4418
Epoch 72 Loss: 0.4374
Epoch 73 Loss: 0.4393
Epoch 74 Loss: 0.4646
Epoch 75 Loss: 0.4747
Epoch 76 Loss: 0.4784
Epoch 77 Loss: 0.4830
Epoch 78 Loss: 0.4797
Epoch 79 Loss: 0.4821
Epoch 80 Loss: 0.4802
Epoch 81 Loss: 0.4769
Epoch 82 Loss: 0.4742
Epoch 83 Loss: 0.4723
Epoch 84 Loss: 0.4699
Epoch 85 Loss: 0.4654
Epoch 86 Loss: 0.4604
Epoch 87 Loss: 0.4574
Epoch 88 Loss: 0.4540
Epoch 89 Loss: 0.4462
Epoch 90 Loss: 0.4419
Epoch 91 Loss: 0.4364
Epoch 92 Loss: 0.4294
Epoch 93 Loss: 0.4328
Epoch 94 Loss: 0.4523
Epoch 95 Loss: 0.4628
Epoch 96 Loss: 0.4613
Epoch 97 Loss: 0.4598
Epoch 98 Loss: 0.4568
Epoch 99 Loss: 0.4533
Epoch 100 Loss: 0.4707