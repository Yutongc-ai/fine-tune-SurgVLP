/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  64
num_shots is  64
Epoch 1 Loss: 0.9524
Epoch 1 best val f1 0.4154 test f1 0.3057743310928345
Epoch 2 Loss: 0.8503
Epoch 3 Loss: 0.7859
Epoch 3 best val f1 0.4156 test f1 0.3057743310928345
Epoch 4 Loss: 0.7409
Epoch 4 best val f1 0.4164 test f1 0.30577999353408813
Epoch 5 Loss: 0.7081
Epoch 5 best val f1 0.4255 test f1 0.30590498447418213
Epoch 6 Loss: 0.6809
Epoch 6 best val f1 0.4423 test f1 0.30652639269828796
Epoch 7 Loss: 0.6565
Epoch 7 best val f1 0.4849 test f1 0.3093560039997101
Epoch 8 Loss: 0.6317
Epoch 8 best val f1 0.5177 test f1 0.3155604898929596
Epoch 9 Loss: 0.6054
Epoch 9 best val f1 0.5218 test f1 0.3252722918987274
Epoch 10 Loss: 0.5788
Epoch 11 Loss: 0.5567
Epoch 12 Loss: 0.5425
Epoch 13 Loss: 0.5360
Epoch 14 Loss: 0.5323
Epoch 15 Loss: 0.5287
Epoch 16 Loss: 0.5256
Epoch 17 Loss: 0.5244
Epoch 18 Loss: 0.5238
Epoch 19 Loss: 0.5239
Epoch 20 Loss: 0.5211
Epoch 21 Loss: 0.5188
Epoch 22 Loss: 0.5192
Epoch 23 Loss: 0.5196
Epoch 24 Loss: 0.5175
Epoch 25 Loss: 0.5167
Epoch 26 Loss: 0.5163
Epoch 27 Loss: 0.5162
Epoch 28 Loss: 0.5162
Epoch 29 Loss: 0.5148
Epoch 30 Loss: 0.5141
Epoch 31 Loss: 0.5148
Epoch 32 Loss: 0.5167
Epoch 33 Loss: 0.5166
Epoch 34 Loss: 0.5116
Epoch 35 Loss: 0.5128
Epoch 36 Loss: 0.5141
Epoch 37 Loss: 0.5135
Epoch 38 Loss: 0.5134
Epoch 39 Loss: 0.5125
Epoch 40 Loss: 0.5124
Epoch 41 Loss: 0.5120
Epoch 42 Loss: 0.5127
Epoch 43 Loss: 0.5130
Epoch 44 Loss: 0.5124
Epoch 45 Loss: 0.5113
Epoch 46 Loss: 0.5117
Epoch 47 Loss: 0.5107
Epoch 48 Loss: 0.5109
Epoch 49 Loss: 0.5111
Epoch 50 Loss: 0.5103
Epoch 51 Loss: 0.5098
Epoch 52 Loss: 0.5105
Epoch 53 Loss: 0.5095
Epoch 54 Loss: 0.5086
Epoch 55 Loss: 0.5086
Epoch 56 Loss: 0.5088
Epoch 57 Loss: 0.5079
Epoch 58 Loss: 0.5080
Epoch 59 Loss: 0.5070
Epoch 60 Loss: 0.5064
Epoch 61 Loss: 0.5065
Epoch 62 Loss: 0.5054
Epoch 63 Loss: 0.5032
Epoch 64 Loss: 0.5080
Epoch 65 Loss: 0.5116
Epoch 66 Loss: 0.5102
Epoch 67 Loss: 0.5106
Epoch 68 Loss: 0.5099
Epoch 69 Loss: 0.5094
Epoch 70 Loss: 0.5090
Epoch 71 Loss: 0.5093
Epoch 72 Loss: 0.5089
Epoch 73 Loss: 0.5089
Epoch 74 Loss: 0.5086
Epoch 75 Loss: 0.5081
Epoch 76 Loss: 0.5079
Epoch 77 Loss: 0.5077
Epoch 78 Loss: 0.5080
Epoch 79 Loss: 0.5082
Epoch 80 Loss: 0.5076
Epoch 81 Loss: 0.5056
Epoch 82 Loss: 0.5032
Epoch 83 Loss: 0.5008
Epoch 84 Loss: 0.4993
Epoch 85 Loss: 0.4984
Epoch 86 Loss: 0.4981
Epoch 87 Loss: 0.4966
Epoch 88 Loss: 0.4960
Epoch 89 Loss: 0.4960
Epoch 90 Loss: 0.4951
Epoch 91 Loss: 0.5046
Epoch 92 Loss: 0.5106
Epoch 93 Loss: 0.5095
Epoch 94 Loss: 0.5088
Epoch 95 Loss: 0.5078
Epoch 96 Loss: 0.5077
Epoch 97 Loss: 0.5080
Epoch 98 Loss: 0.5069
Epoch 99 Loss: 0.5068
Epoch 100 Loss: 0.5065
Epoch 101 Loss: 0.5069
Epoch 102 Loss: 0.5091
Epoch 103 Loss: 0.5069
Epoch 104 Loss: 0.5062
Epoch 105 Loss: 0.5075
Epoch 106 Loss: 0.5068
Epoch 107 Loss: 0.5069
Epoch 108 Loss: 0.5064
Epoch 109 Loss: 0.5070
Epoch 110 Loss: 0.5078
Epoch 111 Loss: 0.5068
Epoch 112 Loss: 0.5077
Epoch 113 Loss: 0.5067
Epoch 114 Loss: 0.5064
Epoch 115 Loss: 0.5055
Epoch 116 Loss: 0.5047
Epoch 117 Loss: 0.5044
Epoch 118 Loss: 0.5033
Epoch 119 Loss: 0.5021
Epoch 120 Loss: 0.5010
Epoch 121 Loss: 0.5018
Epoch 122 Loss: 0.5008
Epoch 123 Loss: 0.5036
Epoch 124 Loss: 0.5034
Epoch 125 Loss: 0.5032
Epoch 126 Loss: 0.5023
Epoch 127 Loss: 0.5010
Epoch 128 Loss: 0.5004
Epoch 129 Loss: 0.4999
Epoch 130 Loss: 0.4974
Epoch 131 Loss: 0.4961
Epoch 132 Loss: 0.4953
Epoch 133 Loss: 0.4923
Epoch 134 Loss: 0.4926
Epoch 135 Loss: 0.4914
Epoch 136 Loss: 0.4885
Epoch 137 Loss: 0.4907
Epoch 138 Loss: 0.5033
Epoch 139 Loss: 0.5007
Epoch 140 Loss: 0.4974
Epoch 141 Loss: 0.4934
Epoch 142 Loss: 0.4893
Epoch 143 Loss: 0.4837
Epoch 144 Loss: 0.4787
Epoch 145 Loss: 0.4968
Epoch 146 Loss: 0.5054
Epoch 147 Loss: 0.5036
Epoch 148 Loss: 0.5020
Epoch 149 Loss: 0.5018
Epoch 150 Loss: 0.5001
Epoch 151 Loss: 0.4984
Epoch 152 Loss: 0.4976
Epoch 153 Loss: 0.4960
Epoch 154 Loss: 0.4936
Epoch 155 Loss: 0.4901
Epoch 156 Loss: 0.4884
Epoch 157 Loss: 0.4870
Epoch 158 Loss: 0.4910
Epoch 159 Loss: 0.4927
Epoch 160 Loss: 0.4911
Epoch 161 Loss: 0.4890
Epoch 162 Loss: 0.4874
Epoch 163 Loss: 0.4863
Epoch 164 Loss: 0.4831
Epoch 165 Loss: 0.4810
Epoch 166 Loss: 0.4812
Epoch 167 Loss: 0.4871
Epoch 168 Loss: 0.4869
Epoch 169 Loss: 0.4821
Epoch 170 Loss: 0.4781
Epoch 171 Loss: 0.4756
Epoch 172 Loss: 0.4657
Epoch 173 Loss: 0.4748
Epoch 174 Loss: 0.5035
Epoch 175 Loss: 0.5000
Epoch 176 Loss: 0.4976
Epoch 177 Loss: 0.4964
Epoch 178 Loss: 0.4997
Epoch 179 Loss: 0.4973
Epoch 180 Loss: 0.4971
Epoch 181 Loss: 0.4950
Epoch 182 Loss: 0.4955
Epoch 183 Loss: 0.4924
Epoch 184 Loss: 0.4906
Epoch 185 Loss: 0.5032
Epoch 186 Loss: 0.5072
Epoch 187 Loss: 0.5084
Epoch 188 Loss: 0.5067
Epoch 189 Loss: 0.5052
Epoch 190 Loss: 0.5057
Epoch 191 Loss: 0.5048
Epoch 192 Loss: 0.5043
Epoch 193 Loss: 0.5046
Epoch 194 Loss: 0.5161
Epoch 195 Loss: 0.5103
Epoch 196 Loss: 0.5090
Epoch 197 Loss: 0.5100
Epoch 198 Loss: 0.5105
Epoch 199 Loss: 0.5092
Epoch 200 Loss: 0.5088