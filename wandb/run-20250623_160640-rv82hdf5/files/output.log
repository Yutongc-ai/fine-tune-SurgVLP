/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 0.9974
Epoch 1 best val f1 0.3951 test f1 0.3057743310928345
Epoch 2 Loss: 0.9542
Epoch 3 Loss: 0.9182
Epoch 4 Loss: 0.8863
Epoch 5 Loss: 0.8579
Epoch 6 Loss: 0.8337
Epoch 7 Loss: 0.8123
Epoch 8 Loss: 0.7951
Epoch 9 Loss: 0.7775
Epoch 10 Loss: 0.7622
Epoch 11 Loss: 0.7487
Epoch 11 best val f1 0.3955 test f1 0.3057743310928345
Epoch 12 Loss: 0.7359
Epoch 12 best val f1 0.3959 test f1 0.3057743310928345
Epoch 13 Loss: 0.7241
Epoch 13 best val f1 0.3967 test f1 0.3057771325111389
Epoch 14 Loss: 0.7121
Epoch 14 best val f1 0.4008 test f1 0.30577999353408813
Epoch 15 Loss: 0.7024
Epoch 15 best val f1 0.4034 test f1 0.30578848719596863
Epoch 16 Loss: 0.6916
Epoch 16 best val f1 0.4055 test f1 0.3058083951473236
Epoch 17 Loss: 0.6812
Epoch 17 best val f1 0.4096 test f1 0.30586233735084534
Epoch 18 Loss: 0.6720
Epoch 18 best val f1 0.4101 test f1 0.30592915415763855
Epoch 19 Loss: 0.6614
Epoch 19 best val f1 0.4195 test f1 0.3060314953327179
Epoch 20 Loss: 0.6507
Epoch 20 best val f1 0.4240 test f1 0.3061894476413727
Epoch 21 Loss: 0.6415
Epoch 21 best val f1 0.4428 test f1 0.3064051568508148
Epoch 22 Loss: 0.6317
Epoch 22 best val f1 0.4577 test f1 0.30674898624420166
Epoch 23 Loss: 0.6224
Epoch 24 Loss: 0.6124
Epoch 25 Loss: 0.6007
Epoch 26 Loss: 0.5908
Epoch 27 Loss: 0.5804
Epoch 28 Loss: 0.5711
Epoch 29 Loss: 0.5622
Epoch 30 Loss: 0.5549
Epoch 31 Loss: 0.5462
Epoch 32 Loss: 0.5410
Epoch 33 Loss: 0.5363
Epoch 34 Loss: 0.5321
Epoch 35 Loss: 0.5308
Epoch 36 Loss: 0.5303
Epoch 37 Loss: 0.5277
Epoch 38 Loss: 0.5272
Epoch 39 Loss: 0.5264
Epoch 40 Loss: 0.5254
Epoch 41 Loss: 0.5268
Epoch 42 Loss: 0.5237
Epoch 43 Loss: 0.5199
Epoch 44 Loss: 0.5203
Epoch 45 Loss: 0.5184
Epoch 46 Loss: 0.5187
Epoch 47 Loss: 0.5180
Epoch 48 Loss: 0.5172
Epoch 49 Loss: 0.5161
Epoch 50 Loss: 0.5176
Epoch 51 Loss: 0.5168
Epoch 52 Loss: 0.5144
Epoch 53 Loss: 0.5129
Epoch 54 Loss: 0.5175
Epoch 55 Loss: 0.5120
Epoch 56 Loss: 0.5119
Epoch 57 Loss: 0.5114
Epoch 58 Loss: 0.5082
Epoch 59 Loss: 0.5102
Epoch 60 Loss: 0.5100
Epoch 61 Loss: 0.5096
Epoch 62 Loss: 0.5101
Epoch 63 Loss: 0.5100
Epoch 64 Loss: 0.5081
Epoch 65 Loss: 0.5102
Epoch 66 Loss: 0.5072
Epoch 67 Loss: 0.5088
Epoch 68 Loss: 0.5062
Epoch 69 Loss: 0.5082
Epoch 70 Loss: 0.5084
Epoch 71 Loss: 0.5080
Epoch 72 Loss: 0.5062
Epoch 73 Loss: 0.5050
Epoch 74 Loss: 0.5028
Epoch 75 Loss: 0.5092
Epoch 76 Loss: 0.5083
Epoch 77 Loss: 0.5042
Epoch 78 Loss: 0.5019
Epoch 79 Loss: 0.5032
Epoch 80 Loss: 0.5016
Epoch 81 Loss: 0.5026
Epoch 82 Loss: 0.5050
Epoch 83 Loss: 0.5004
Epoch 84 Loss: 0.5028
Epoch 85 Loss: 0.5040
Epoch 86 Loss: 0.4986
Epoch 87 Loss: 0.5033
Epoch 88 Loss: 0.5008
Epoch 89 Loss: 0.4993
Epoch 90 Loss: 0.5017
Epoch 91 Loss: 0.4992
Epoch 92 Loss: 0.5005
Epoch 93 Loss: 0.5010
Epoch 94 Loss: 0.4995
Epoch 95 Loss: 0.4990
Epoch 96 Loss: 0.4979
Epoch 97 Loss: 0.4991
Epoch 98 Loss: 0.4980
Epoch 99 Loss: 0.4971
Epoch 100 Loss: 0.4984
Epoch 101 Loss: 0.4976
Epoch 102 Loss: 0.4976
Epoch 103 Loss: 0.4993
Epoch 104 Loss: 0.4965
Epoch 105 Loss: 0.4959
Epoch 106 Loss: 0.4968
Epoch 107 Loss: 0.4962
Epoch 108 Loss: 0.4949
Epoch 109 Loss: 0.4939
Epoch 110 Loss: 0.4952
Epoch 111 Loss: 0.4952
Epoch 112 Loss: 0.4937
Epoch 113 Loss: 0.4938
Epoch 114 Loss: 0.4910
Epoch 115 Loss: 0.4930
Epoch 116 Loss: 0.4936
Epoch 117 Loss: 0.4947
Epoch 118 Loss: 0.4926
Epoch 119 Loss: 0.4935
Epoch 120 Loss: 0.4919
Epoch 121 Loss: 0.4913
Epoch 122 Loss: 0.4954
Epoch 123 Loss: 0.5145
Epoch 124 Loss: 0.5056
Epoch 125 Loss: 0.5053
Epoch 126 Loss: 0.5041
Epoch 127 Loss: 0.5015
Epoch 128 Loss: 0.5020
Epoch 129 Loss: 0.5027
Epoch 130 Loss: 0.5040
Epoch 131 Loss: 0.5054
Epoch 132 Loss: 0.5048
Epoch 133 Loss: 0.5041
Epoch 134 Loss: 0.5059
Epoch 135 Loss: 0.5024
Epoch 136 Loss: 0.5011
Epoch 137 Loss: 0.5012
Epoch 138 Loss: 0.4982
Epoch 139 Loss: 0.5034
Epoch 140 Loss: 0.5055
Epoch 141 Loss: 0.4994
Epoch 142 Loss: 0.4995
Epoch 143 Loss: 0.4988
Epoch 144 Loss: 0.4984
Epoch 145 Loss: 0.4971
Epoch 146 Loss: 0.4972
Epoch 147 Loss: 0.4977
Epoch 148 Loss: 0.4977
Epoch 149 Loss: 0.4978
Epoch 150 Loss: 0.4956
Epoch 151 Loss: 0.4990
Epoch 152 Loss: 0.4956
Epoch 153 Loss: 0.4947
Epoch 154 Loss: 0.4969
Epoch 155 Loss: 0.4940
Epoch 156 Loss: 0.4942
Epoch 157 Loss: 0.4922
Epoch 158 Loss: 0.4926
Epoch 159 Loss: 0.4920
Epoch 160 Loss: 0.4910
Epoch 161 Loss: 0.4920
Epoch 162 Loss: 0.4886
Epoch 163 Loss: 0.4920
Epoch 164 Loss: 0.4878
Epoch 165 Loss: 0.4908
Epoch 166 Loss: 0.4903
Epoch 167 Loss: 0.4895
Epoch 168 Loss: 0.4874
Epoch 169 Loss: 0.4888
Epoch 170 Loss: 0.4897
Epoch 171 Loss: 0.4877
Epoch 172 Loss: 0.4880
Epoch 173 Loss: 0.4889
Epoch 174 Loss: 0.4886
Epoch 175 Loss: 0.4861
Epoch 176 Loss: 0.4861
Epoch 177 Loss: 0.4853
Epoch 178 Loss: 0.4846
Epoch 179 Loss: 0.4845
Epoch 180 Loss: 0.4846
Epoch 181 Loss: 0.4874
Epoch 182 Loss: 0.4962
Epoch 183 Loss: 0.4951
Epoch 184 Loss: 0.4915
Epoch 185 Loss: 0.4916
Epoch 186 Loss: 0.4889
Epoch 187 Loss: 0.4900
Epoch 188 Loss: 0.4886
Epoch 189 Loss: 0.4900
Epoch 190 Loss: 0.4896
Epoch 191 Loss: 0.4886
Epoch 192 Loss: 0.4893
Epoch 193 Loss: 0.4865
Epoch 194 Loss: 0.4873
Epoch 195 Loss: 0.4849
Epoch 196 Loss: 0.4832
Epoch 197 Loss: 0.4816
Epoch 198 Loss: 0.4835
Epoch 199 Loss: 0.4796
Epoch 200 Loss: 0.4855