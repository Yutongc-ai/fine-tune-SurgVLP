/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.8369
Epoch 1 best val f1 0.4444 test f1 0.3057870864868164
Epoch 2 Loss: 0.8146
Epoch 3 Loss: 0.7972
Epoch 4 Loss: 0.7823
Epoch 5 Loss: 0.7697
Epoch 6 Loss: 0.7581
Epoch 7 Loss: 0.7480
Epoch 8 Loss: 0.7385
Epoch 9 Loss: 0.7299
Epoch 10 Loss: 0.7220
Epoch 11 Loss: 0.7147
Epoch 12 Loss: 0.7081
Epoch 13 Loss: 0.7019
Epoch 14 Loss: 0.6960
Epoch 15 Loss: 0.6906
Epoch 16 Loss: 0.6854
Epoch 17 Loss: 0.6806
Epoch 18 Loss: 0.6759
Epoch 19 Loss: 0.6712
Epoch 20 Loss: 0.6667
Epoch 21 Loss: 0.6625
Epoch 21 best val f1 0.4516 test f1 0.30591273307800293
Epoch 22 Loss: 0.6585
Epoch 23 Loss: 0.6545
Epoch 24 Loss: 0.6506
Epoch 25 Loss: 0.6468
Epoch 26 Loss: 0.6430
Epoch 27 Loss: 0.6392
Epoch 27 best val f1 0.4590 test f1 0.30622637271881104
Epoch 28 Loss: 0.6355
Epoch 28 best val f1 0.4667 test f1 0.30633893609046936
Epoch 29 Loss: 0.6318
Epoch 30 Loss: 0.6283
Epoch 30 best val f1 0.4828 test f1 0.30664485692977905
Epoch 31 Loss: 0.6244
Epoch 31 best val f1 0.4912 test f1 0.3068477511405945
Epoch 32 Loss: 0.6207
Epoch 33 Loss: 0.6171
Epoch 34 Loss: 0.6136
Epoch 35 Loss: 0.6100
Epoch 35 best val f1 0.5098 test f1 0.30834460258483887
Epoch 36 Loss: 0.6064
Epoch 36 best val f1 0.5200 test f1 0.30891942977905273
Epoch 37 Loss: 0.6027
Epoch 37 best val f1 0.5532 test f1 0.30959856510162354
Epoch 38 Loss: 0.5989
Epoch 38 best val f1 0.5652 test f1 0.31043416261672974
Epoch 39 Loss: 0.5952
Epoch 39 best val f1 0.5778 test f1 0.3113841116428375
Epoch 40 Loss: 0.5915
Epoch 40 best val f1 0.5909 test f1 0.3125656843185425
Epoch 41 Loss: 0.5878
Epoch 42 Loss: 0.5842
Epoch 43 Loss: 0.5806
Epoch 44 Loss: 0.5798
Epoch 45 Loss: 0.5787
Epoch 46 Loss: 0.5765
Epoch 47 Loss: 0.5732
Epoch 48 Loss: 0.5696
Epoch 49 Loss: 0.5654
Epoch 50 Loss: 0.5610
Epoch 51 Loss: 0.5566
Epoch 52 Loss: 0.5522
Epoch 53 Loss: 0.5478
Epoch 54 Loss: 0.5432
Epoch 55 Loss: 0.5389
Epoch 56 Loss: 0.5346
Epoch 57 Loss: 0.5303
Epoch 58 Loss: 0.5261
Epoch 59 Loss: 0.5224
Epoch 60 Loss: 0.5187
Epoch 61 Loss: 0.5149
Epoch 62 Loss: 0.5109
Epoch 63 Loss: 0.5068
Epoch 64 Loss: 0.5029
Epoch 65 Loss: 0.4989
Epoch 66 Loss: 0.4950
Epoch 67 Loss: 0.4912
Epoch 68 Loss: 0.4871
Epoch 69 Loss: 0.4832
Epoch 70 Loss: 0.4794
Epoch 71 Loss: 0.4761
Epoch 72 Loss: 0.4727
Epoch 73 Loss: 0.4692
Epoch 74 Loss: 0.4658
Epoch 75 Loss: 0.4625
Epoch 76 Loss: 0.4593
Epoch 77 Loss: 0.4563
Epoch 78 Loss: 0.4532
Epoch 79 Loss: 0.4500
Epoch 80 Loss: 0.4469
Epoch 81 Loss: 0.4440
Epoch 82 Loss: 0.4411
Epoch 83 Loss: 0.4381
Epoch 84 Loss: 0.4367
Epoch 85 Loss: 0.4357
Epoch 86 Loss: 0.4338
Epoch 87 Loss: 0.4314
Epoch 88 Loss: 0.4287
Epoch 89 Loss: 0.4260
Epoch 90 Loss: 0.4231
Epoch 91 Loss: 0.4347
Epoch 92 Loss: 0.4484
Epoch 93 Loss: 0.4585
Epoch 94 Loss: 0.4640
Epoch 95 Loss: 0.4659
Epoch 96 Loss: 0.4645
Epoch 97 Loss: 0.4611
Epoch 98 Loss: 0.4565
Epoch 99 Loss: 0.4513
Epoch 100 Loss: 0.4458