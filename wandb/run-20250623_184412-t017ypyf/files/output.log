loading num parts:  0
loading num parts:  1
loading num parts:  2
loading num parts:  3
loading num parts:  4
/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
num_shots is  1
num_shots is  1
Epoch 1 Loss: 1.3444
Epoch 1 best val f1 0.4194 test f1 0.30569323897361755
Epoch 2 Loss: 0.7065
Epoch 3 Loss: 0.5959
Epoch 4 Loss: 0.5780
Epoch 5 Loss: 0.5709
Epoch 6 Loss: 0.5634
Epoch 7 Loss: 0.5585
Epoch 7 best val f1 0.4231 test f1 0.30939796566963196
Epoch 8 Loss: 0.5542
Epoch 9 Loss: 0.5497
Epoch 10 Loss: 0.5450
Epoch 11 Loss: 0.5397
Epoch 12 Loss: 0.5339
Epoch 13 Loss: 0.5279
Epoch 14 Loss: 0.5221
Epoch 15 Loss: 0.5163
Epoch 16 Loss: 0.5111
Epoch 17 Loss: 0.5068
Epoch 18 Loss: 0.5025
Epoch 19 Loss: 0.4990
Epoch 20 Loss: 0.4958
Epoch 21 Loss: 0.4927
Epoch 22 Loss: 0.4899
Epoch 23 Loss: 0.4869
Epoch 24 Loss: 0.4837
Epoch 25 Loss: 0.4801
Epoch 26 Loss: 0.4762
Epoch 27 Loss: 0.4715
Epoch 28 Loss: 0.4667
Epoch 29 Loss: 0.4617
Epoch 30 Loss: 0.4561
Epoch 31 Loss: 0.4535
Epoch 31 best val f1 0.4444 test f1 0.3376305401325226
Epoch 32 Loss: 0.4535
Epoch 32 best val f1 0.6000 test f1 0.3392542600631714
Epoch 33 Loss: 0.4512
Epoch 34 Loss: 0.4468
Epoch 35 Loss: 0.4409
Epoch 36 Loss: 0.4349
Epoch 37 Loss: 0.4278
Epoch 38 Loss: 0.4212
Epoch 39 Loss: 0.4139
Epoch 40 Loss: 0.4058
Epoch 41 Loss: 0.3970
Epoch 42 Loss: 0.3879
Epoch 43 Loss: 0.3775
Epoch 44 Loss: 0.3669
Epoch 45 Loss: 0.3559
Epoch 46 Loss: 0.3447
Epoch 47 Loss: 0.3326
Epoch 48 Loss: 0.3220
Epoch 49 Loss: 0.3108
Epoch 50 Loss: 0.3001
Epoch 51 Loss: 0.2901
Epoch 52 Loss: 0.2791
Epoch 53 Loss: 0.2690
Epoch 54 Loss: 0.2601
Epoch 55 Loss: 0.2542
Epoch 56 Loss: 0.2485
Epoch 57 Loss: 0.2431
Epoch 58 Loss: 0.2382
Epoch 59 Loss: 0.2355
Epoch 60 Loss: 0.2338
Epoch 61 Loss: 0.2318
Epoch 62 Loss: 0.2288
Epoch 63 Loss: 0.2256
Epoch 64 Loss: 0.2211
Epoch 65 Loss: 0.2154
Epoch 66 Loss: 0.2085
Epoch 67 Loss: 0.2008
Epoch 68 Loss: 0.1925
Epoch 69 Loss: 0.1837
Epoch 70 Loss: 0.1755
Epoch 71 Loss: 0.1689
Epoch 72 Loss: 0.1648
Epoch 73 Loss: 0.1628
Epoch 74 Loss: 0.1625
Epoch 75 Loss: 0.1636
Epoch 76 Loss: 0.1725
Epoch 77 Loss: 0.1737
Epoch 78 Loss: 0.1733
Epoch 79 Loss: 0.1701
Epoch 80 Loss: 0.1719
Epoch 81 Loss: 0.1647
Epoch 82 Loss: 0.1610
Epoch 83 Loss: 0.1566
Epoch 84 Loss: 0.1534
Epoch 85 Loss: 0.1634
Epoch 86 Loss: 0.1563
Epoch 87 Loss: 0.1506
Epoch 88 Loss: 0.1455
Epoch 89 Loss: 0.1419
Epoch 90 Loss: 0.1377
Epoch 91 Loss: 0.1341
Epoch 92 Loss: 0.1303
Epoch 93 Loss: 0.1255
Epoch 94 Loss: 0.1191
Epoch 95 Loss: 0.1111
Epoch 96 Loss: 0.1023
Epoch 97 Loss: 0.0954
Epoch 98 Loss: 0.0926
Epoch 99 Loss: 0.0900
Epoch 100 Loss: 0.0891
Epoch 101 Loss: 0.0901
Epoch 102 Loss: 0.0920
Epoch 103 Loss: 0.0933
Epoch 104 Loss: 0.0936
Epoch 105 Loss: 0.0927
Epoch 106 Loss: 0.0916
Epoch 107 Loss: 0.0894
Epoch 108 Loss: 0.0870
Epoch 109 Loss: 0.0844
Epoch 110 Loss: 0.0826
Epoch 111 Loss: 0.0810
Epoch 112 Loss: 0.0801
Epoch 113 Loss: 0.0798
Epoch 114 Loss: 0.0795
Epoch 115 Loss: 0.0787
Epoch 116 Loss: 0.0781
Epoch 117 Loss: 0.0775
Epoch 118 Loss: 0.0759
Epoch 119 Loss: 0.0741
Epoch 120 Loss: 0.0726
Epoch 121 Loss: 0.0711
Epoch 122 Loss: 0.0700
Epoch 123 Loss: 0.0697
Epoch 124 Loss: 0.0695
Epoch 125 Loss: 0.0694
Epoch 126 Loss: 0.0691
Epoch 127 Loss: 0.0684
Epoch 128 Loss: 0.0675
Epoch 129 Loss: 0.0663
Epoch 130 Loss: 0.0656
Epoch 131 Loss: 0.0649
Epoch 132 Loss: 0.0645
Epoch 133 Loss: 0.0641
Epoch 134 Loss: 0.0638
Epoch 135 Loss: 0.0637
Epoch 136 Loss: 0.0648
Epoch 137 Loss: 0.0648
Epoch 138 Loss: 0.0637
Epoch 139 Loss: 0.1594
Epoch 140 Loss: 0.1151
Epoch 141 Loss: 0.0875
Epoch 142 Loss: 0.0641
Epoch 143 Loss: 0.0531
Epoch 144 Loss: 0.0494
Epoch 145 Loss: 0.0515
Epoch 146 Loss: 0.0583
Epoch 147 Loss: 0.0672
Epoch 148 Loss: 0.0749
Epoch 149 Loss: 0.0787
Epoch 150 Loss: 0.0781
Epoch 151 Loss: 0.0739
Epoch 152 Loss: 0.0675
Epoch 153 Loss: 0.0612
Epoch 154 Loss: 0.0563
Epoch 155 Loss: 0.0538
Epoch 156 Loss: 0.0539
Epoch 157 Loss: 0.0557
Epoch 158 Loss: 0.0584
Epoch 159 Loss: 0.0607
Epoch 160 Loss: 0.0619
Epoch 161 Loss: 0.0618
Epoch 162 Loss: 0.0606
Epoch 163 Loss: 0.0589
Epoch 164 Loss: 0.0573
Epoch 165 Loss: 0.0561
Epoch 166 Loss: 0.0555
Epoch 167 Loss: 0.0558
Epoch 168 Loss: 0.0562
Epoch 169 Loss: 0.0564
Epoch 170 Loss: 0.0565
Epoch 171 Loss: 0.0566
Epoch 172 Loss: 0.0567
Epoch 173 Loss: 0.0568
Epoch 174 Loss: 0.0569
Epoch 175 Loss: 0.0569
Epoch 176 Loss: 0.0766
Epoch 177 Loss: 0.0929
Epoch 178 Loss: 0.0788
Epoch 179 Loss: 0.0665
Epoch 180 Loss: 0.0586
Epoch 181 Loss: 0.0553
Epoch 182 Loss: 0.0563
Epoch 183 Loss: 0.0604
Epoch 184 Loss: 0.0661
Epoch 185 Loss: 0.0708
Epoch 186 Loss: 0.0728
Epoch 187 Loss: 0.0716
Epoch 188 Loss: 0.0676
Epoch 189 Loss: 0.0628
Epoch 190 Loss: 0.0583
Epoch 191 Loss: 0.0552
Epoch 192 Loss: 0.0542
Epoch 193 Loss: 0.0547
Epoch 194 Loss: 0.0564
Epoch 195 Loss: 0.0583
Epoch 196 Loss: 0.0594
Epoch 197 Loss: 0.0593
Epoch 198 Loss: 0.0581
Epoch 199 Loss: 0.0565
Epoch 200 Loss: 0.0551