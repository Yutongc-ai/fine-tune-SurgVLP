/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  4
num_shots is  4
Epoch 1 Loss: 1.3483
Epoch 1 best val f1 0.4000 test f1 0.3058601915836334
Epoch 2 Loss: 0.8681
Epoch 3 Loss: 0.6332
Epoch 4 Loss: 0.5908
Epoch 5 Loss: 0.5812
Epoch 6 Loss: 0.5732
Epoch 7 Loss: 0.5653
Epoch 8 Loss: 0.5587
Epoch 8 best val f1 0.4118 test f1 0.306381493806839
Epoch 9 Loss: 0.5526
Epoch 10 Loss: 0.5468
Epoch 11 Loss: 0.5414
Epoch 12 Loss: 0.5369
Epoch 13 Loss: 0.5321
Epoch 14 Loss: 0.5270
Epoch 15 Loss: 0.5215
Epoch 16 Loss: 0.5156
Epoch 17 Loss: 0.5097
Epoch 18 Loss: 0.5037
Epoch 19 Loss: 0.4976
Epoch 20 Loss: 0.4913
Epoch 21 Loss: 0.4847
Epoch 22 Loss: 0.4774
Epoch 23 Loss: 0.4698
Epoch 24 Loss: 0.4684
Epoch 25 Loss: 0.4638
Epoch 26 Loss: 0.4597
Epoch 27 Loss: 0.4548
Epoch 28 Loss: 0.4488
Epoch 29 Loss: 0.4420
Epoch 29 best val f1 0.4324 test f1 0.31429558992385864
Epoch 30 Loss: 0.4351
Epoch 31 Loss: 0.4277
Epoch 32 Loss: 0.4202
Epoch 32 best val f1 0.4416 test f1 0.3166823387145996
Epoch 33 Loss: 0.4125
Epoch 34 Loss: 0.4047
Epoch 35 Loss: 0.3976
Epoch 36 Loss: 0.3907
Epoch 37 Loss: 0.3838
Epoch 38 Loss: 0.3772
Epoch 39 Loss: 0.3706
Epoch 40 Loss: 0.3641
Epoch 41 Loss: 0.3583
Epoch 42 Loss: 0.3526
Epoch 43 Loss: 0.3471
Epoch 44 Loss: 0.3440
Epoch 45 Loss: 0.3403
Epoch 46 Loss: 0.3677
Epoch 47 Loss: 0.4034
Epoch 48 Loss: 0.4148
Epoch 49 Loss: 0.4183
Epoch 50 Loss: 0.4160
Epoch 51 Loss: 0.4070
Epoch 52 Loss: 0.4005
Epoch 53 Loss: 0.3940
Epoch 54 Loss: 0.3862
Epoch 55 Loss: 0.3785
Epoch 56 Loss: 0.3730
Epoch 57 Loss: 0.3675
Epoch 58 Loss: 0.3618
Epoch 59 Loss: 0.3572
Epoch 60 Loss: 0.3531
Epoch 61 Loss: 0.3489
Epoch 62 Loss: 0.3442
Epoch 63 Loss: 0.3713
Epoch 64 Loss: 0.3954
Epoch 65 Loss: 0.3960
Epoch 66 Loss: 0.3959
Epoch 67 Loss: 0.3941
Epoch 68 Loss: 0.3876
Epoch 69 Loss: 0.3821
Epoch 70 Loss: 0.3789
Epoch 71 Loss: 0.3736
Epoch 72 Loss: 0.3719
Epoch 73 Loss: 0.3808
Epoch 73 best val f1 0.4471 test f1 0.34001773595809937
Epoch 74 Loss: 0.3862
Epoch 74 best val f1 0.4494 test f1 0.3403026759624481
Epoch 75 Loss: 0.3868
Epoch 75 best val f1 0.4835 test f1 0.3408600986003876
Epoch 76 Loss: 0.3853
Epoch 77 Loss: 0.3822
Epoch 78 Loss: 0.3788
Epoch 79 Loss: 0.3748
Epoch 79 best val f1 0.4941 test f1 0.3439140319824219
Epoch 80 Loss: 0.3704
Epoch 81 Loss: 0.3666
Epoch 82 Loss: 0.3629
Epoch 83 Loss: 0.3593
Epoch 84 Loss: 0.3576
Epoch 85 Loss: 0.3602
Epoch 86 Loss: 0.3618
Epoch 87 Loss: 0.3617
Epoch 88 Loss: 0.3597
Epoch 89 Loss: 0.3557
Epoch 90 Loss: 0.3512
Epoch 91 Loss: 0.3472
Epoch 92 Loss: 0.3438
Epoch 93 Loss: 0.3405
Epoch 94 Loss: 0.3370
Epoch 95 Loss: 0.3334
Epoch 96 Loss: 0.3301
Epoch 97 Loss: 0.3269
Epoch 98 Loss: 0.3242
Epoch 99 Loss: 0.3219
Epoch 100 Loss: 0.3194