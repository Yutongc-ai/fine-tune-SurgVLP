/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
loading num parts:  2
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 0.8652
Epoch 1 best val f1 0.3951 test f1 0.3057743310928345
Epoch 2 Loss: 0.8463
Epoch 3 Loss: 0.8295
Epoch 4 Loss: 0.8138
Epoch 5 Loss: 0.7987
Epoch 6 Loss: 0.7861
Epoch 7 Loss: 0.7742
Epoch 8 Loss: 0.7650
Epoch 9 Loss: 0.7556
Epoch 10 Loss: 0.7473
Epoch 11 Loss: 0.7396
Epoch 12 Loss: 0.7322
Epoch 12 best val f1 0.3955 test f1 0.3057743310928345
Epoch 13 Loss: 0.7255
Epoch 13 best val f1 0.3959 test f1 0.3057743310928345
Epoch 14 Loss: 0.7189
Epoch 14 best val f1 0.3963 test f1 0.3057757318019867
Epoch 15 Loss: 0.7133
Epoch 15 best val f1 0.3983 test f1 0.3057785630226135
Epoch 16 Loss: 0.7079
Epoch 16 best val f1 0.4004 test f1 0.3057842552661896
Epoch 17 Loss: 0.7032
Epoch 17 best val f1 0.4059 test f1 0.30580976605415344
Epoch 18 Loss: 0.6976
Epoch 18 best val f1 0.4104 test f1 0.3058609366416931
Epoch 19 Loss: 0.6926
Epoch 20 Loss: 0.6882
Epoch 21 Loss: 0.6839
Epoch 21 best val f1 0.4144 test f1 0.30628979206085205
Epoch 22 Loss: 0.6793
Epoch 22 best val f1 0.4291 test f1 0.30664774775505066
Epoch 23 Loss: 0.6747
Epoch 23 best val f1 0.4408 test f1 0.3072269856929779
Epoch 24 Loss: 0.6706
Epoch 24 best val f1 0.4505 test f1 0.308194637298584
Epoch 25 Loss: 0.6669
Epoch 25 best val f1 0.4542 test f1 0.3097149729728699
Epoch 26 Loss: 0.6620
Epoch 27 Loss: 0.6573
Epoch 28 Loss: 0.6532
Epoch 28 best val f1 0.4632 test f1 0.3185582160949707
Epoch 29 Loss: 0.6484
Epoch 30 Loss: 0.6436
Epoch 31 Loss: 0.6383
Epoch 32 Loss: 0.6346
Epoch 33 Loss: 0.6286
Epoch 34 Loss: 0.6233
Epoch 35 Loss: 0.6177
Epoch 36 Loss: 0.6123
Epoch 37 Loss: 0.6063
Epoch 38 Loss: 0.6001
Epoch 39 Loss: 0.5945
Epoch 40 Loss: 0.5886
Epoch 41 Loss: 0.5830
Epoch 42 Loss: 0.5782
Epoch 43 Loss: 0.5752
Epoch 44 Loss: 0.5726
Epoch 45 Loss: 0.5679
Epoch 46 Loss: 0.5639
Epoch 47 Loss: 0.5586
Epoch 48 Loss: 0.5556
Epoch 49 Loss: 0.5511
Epoch 50 Loss: 0.5487
Epoch 51 Loss: 0.5458
Epoch 52 Loss: 0.5447
Epoch 53 Loss: 0.5414
Epoch 54 Loss: 0.5437
Epoch 55 Loss: 0.5403
Epoch 56 Loss: 0.5386
Epoch 57 Loss: 0.5379
Epoch 58 Loss: 0.5383
Epoch 59 Loss: 0.5365
Epoch 60 Loss: 0.5337
Epoch 61 Loss: 0.5341
Epoch 62 Loss: 0.5335
Epoch 63 Loss: 0.5320
Epoch 64 Loss: 0.5328
Epoch 65 Loss: 0.5339
Epoch 66 Loss: 0.5361
Epoch 67 Loss: 0.5393
Epoch 68 Loss: 0.5396
Epoch 69 Loss: 0.5396
Epoch 70 Loss: 0.5407
Epoch 71 Loss: 0.5403
Epoch 72 Loss: 0.5387
Epoch 73 Loss: 0.5373
Epoch 74 Loss: 0.5380
Epoch 75 Loss: 0.5360
Epoch 76 Loss: 0.5357
Epoch 77 Loss: 0.5327
Epoch 78 Loss: 0.5339
Epoch 79 Loss: 0.5302
Epoch 80 Loss: 0.5297
Epoch 81 Loss: 0.5316
Epoch 82 Loss: 0.5313
Epoch 83 Loss: 0.5287
Epoch 84 Loss: 0.5276
Epoch 85 Loss: 0.5303
Epoch 86 Loss: 0.5274
Epoch 87 Loss: 0.5276
Epoch 88 Loss: 0.5280
Epoch 89 Loss: 0.5277
Epoch 90 Loss: 0.5266
Epoch 91 Loss: 0.5257
Epoch 92 Loss: 0.5245
Epoch 93 Loss: 0.5261
Epoch 94 Loss: 0.5254
Epoch 95 Loss: 0.5244
Epoch 96 Loss: 0.5271
Epoch 97 Loss: 0.5239
Epoch 98 Loss: 0.5238
Epoch 99 Loss: 0.5225
Epoch 100 Loss: 0.5247