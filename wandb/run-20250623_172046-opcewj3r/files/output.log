/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  64
num_shots is  64
Epoch 1 Loss: 1.6701
Epoch 1 best val f1 0.4170 test f1 0.2943364381790161
Epoch 2 Loss: 0.6107
Epoch 3 Loss: 0.5700
Epoch 4 Loss: 0.5490
Epoch 5 Loss: 0.5349
Epoch 6 Loss: 0.5273
Epoch 7 Loss: 0.5214
Epoch 8 Loss: 0.5181
Epoch 9 Loss: 0.5174
Epoch 10 Loss: 0.5158
Epoch 11 Loss: 0.5150
Epoch 12 Loss: 0.5161
Epoch 13 Loss: 0.5160
Epoch 14 Loss: 0.5164
Epoch 15 Loss: 0.5161
Epoch 16 Loss: 0.5152
Epoch 17 Loss: 0.5152
Epoch 18 Loss: 0.5188
Epoch 19 Loss: 0.5170
Epoch 20 Loss: 0.5137
Epoch 21 Loss: 0.5169
Epoch 22 Loss: 0.5173
Epoch 23 Loss: 0.5167
Epoch 24 Loss: 0.5149
Epoch 25 Loss: 0.5152
Epoch 26 Loss: 0.5150
Epoch 27 Loss: 0.5166
Epoch 28 Loss: 0.5182
Epoch 29 Loss: 0.5175
Epoch 30 Loss: 0.5165
Epoch 31 Loss: 0.5161
Epoch 32 Loss: 0.5156
Epoch 33 Loss: 0.5167
Epoch 34 Loss: 0.5185
Epoch 35 Loss: 0.5168
Epoch 36 Loss: 0.5147
Epoch 37 Loss: 0.5137
Epoch 38 Loss: 0.5147
Epoch 39 Loss: 0.5160
Epoch 40 Loss: 0.5155
Epoch 41 Loss: 0.5146
Epoch 42 Loss: 0.5140
Epoch 43 Loss: 0.5147
Epoch 44 Loss: 0.5142
Epoch 45 Loss: 0.5140
Epoch 46 Loss: 0.5136
Epoch 47 Loss: 0.5139
Epoch 48 Loss: 0.5139
Epoch 49 Loss: 0.5140
Epoch 50 Loss: 0.5133
Epoch 51 Loss: 0.5131
Epoch 52 Loss: 0.5134
Epoch 53 Loss: 0.5132
Epoch 54 Loss: 0.5129
Epoch 55 Loss: 0.5130
Epoch 56 Loss: 0.5126
Epoch 57 Loss: 0.5123
Epoch 58 Loss: 0.5118
Epoch 59 Loss: 0.5117
Epoch 60 Loss: 0.5117
Epoch 61 Loss: 0.5124
Epoch 62 Loss: 0.5121
Epoch 63 Loss: 0.5110
Epoch 64 Loss: 0.5130
Epoch 65 Loss: 0.5119
Epoch 66 Loss: 0.5119
Epoch 67 Loss: 0.5146
Epoch 68 Loss: 0.5143
Epoch 69 Loss: 0.5121
Epoch 70 Loss: 0.5115
Epoch 71 Loss: 0.5133
Epoch 72 Loss: 0.5143
Epoch 73 Loss: 0.5151
Epoch 74 Loss: 0.5123
Epoch 75 Loss: 0.5115
Epoch 76 Loss: 0.5138
Epoch 77 Loss: 0.5146
Epoch 78 Loss: 0.5118
Epoch 79 Loss: 0.5112
Epoch 80 Loss: 0.5122
Epoch 81 Loss: 0.5107
Epoch 82 Loss: 0.5098
Epoch 83 Loss: 0.5094
Epoch 84 Loss: 0.5092
Epoch 85 Loss: 0.5092
Epoch 86 Loss: 0.5093
Epoch 87 Loss: 0.5082
Epoch 88 Loss: 0.5073
Epoch 89 Loss: 0.5073
Epoch 90 Loss: 0.5066
Epoch 91 Loss: 0.5060
Epoch 92 Loss: 0.5054
Epoch 93 Loss: 0.5041
Epoch 94 Loss: 0.5052
Epoch 95 Loss: 0.5037
Epoch 96 Loss: 0.5030
Epoch 97 Loss: 0.5019
Epoch 98 Loss: 0.5009
Epoch 99 Loss: 0.4983
Epoch 100 Loss: 0.4986
Epoch 101 Loss: 0.4946
Epoch 102 Loss: 0.4932
Epoch 103 Loss: 0.4904
Epoch 104 Loss: 0.4904
Epoch 105 Loss: 0.4876
Epoch 106 Loss: 0.4856
Epoch 107 Loss: 0.4863
Epoch 108 Loss: 0.4885
Epoch 109 Loss: 0.4838
Epoch 110 Loss: 0.4805
Epoch 111 Loss: 0.5041
Epoch 112 Loss: 0.5088
Epoch 113 Loss: 0.5076
Epoch 114 Loss: 0.5088
Epoch 115 Loss: 0.5103
Epoch 116 Loss: 0.5096
Epoch 117 Loss: 0.5102
Epoch 118 Loss: 0.5100
Epoch 119 Loss: 0.5094
Epoch 120 Loss: 0.5092
Epoch 121 Loss: 0.5083
Epoch 122 Loss: 0.5074
Epoch 123 Loss: 0.5066
Epoch 124 Loss: 0.5063
Epoch 125 Loss: 0.5062
Epoch 126 Loss: 0.5045
Epoch 127 Loss: 0.5037
Epoch 128 Loss: 0.5038
Epoch 129 Loss: 0.5030
Epoch 130 Loss: 0.5019
Epoch 131 Loss: 0.5006
Epoch 132 Loss: 0.5003
Epoch 133 Loss: 0.4989
Epoch 134 Loss: 0.4974
Epoch 135 Loss: 0.4960
Epoch 136 Loss: 0.4918
Epoch 137 Loss: 0.4889
Epoch 138 Loss: 0.4855
Epoch 139 Loss: 0.4881
Epoch 140 Loss: 0.4893
Epoch 141 Loss: 0.4900
Epoch 142 Loss: 0.4867
Epoch 143 Loss: 0.4843
Epoch 144 Loss: 0.4829
Epoch 145 Loss: 0.4806
Epoch 146 Loss: 0.4758
Epoch 147 Loss: 0.4725
Epoch 148 Loss: 0.4676
Epoch 149 Loss: 0.4636
Epoch 150 Loss: 0.4611
Epoch 151 Loss: 0.4563
Epoch 152 Loss: 0.4567
Epoch 153 Loss: 0.4581
Epoch 154 Loss: 0.4577
Epoch 155 Loss: 0.4579
Epoch 156 Loss: 0.4537
Epoch 157 Loss: 0.4517
Epoch 158 Loss: 0.4550
Epoch 159 Loss: 0.4563
Epoch 160 Loss: 0.4548
Epoch 161 Loss: 0.4516
Epoch 162 Loss: 0.4489
Epoch 163 Loss: 0.4480
Epoch 164 Loss: 0.4511
Epoch 165 Loss: 0.4486
Epoch 166 Loss: 0.4466
Epoch 167 Loss: 0.4432
Epoch 168 Loss: 0.4401
Epoch 169 Loss: 0.4377
Epoch 170 Loss: 0.4361
Epoch 171 Loss: 0.4354
Epoch 172 Loss: 0.4353
Epoch 173 Loss: 0.4363
Epoch 174 Loss: 0.4340
Epoch 175 Loss: 0.4344
Epoch 176 Loss: 0.4345
Epoch 177 Loss: 0.4350
Epoch 178 Loss: 0.4461
Epoch 179 Loss: 0.4532
Epoch 180 Loss: 0.4513
Epoch 181 Loss: 0.4478
Epoch 182 Loss: 0.4410
Epoch 183 Loss: 0.4371
Epoch 184 Loss: 0.4346
Epoch 185 Loss: 0.4316
Epoch 186 Loss: 0.4318
Epoch 187 Loss: 0.4352
Epoch 188 Loss: 0.4345
Epoch 189 Loss: 0.4375
Epoch 190 Loss: 0.4366
Epoch 191 Loss: 0.4350
Epoch 192 Loss: 0.4341
Epoch 192 best val f1 0.4366 test f1 0.22540733218193054
Epoch 193 Loss: 0.4359
Epoch 194 Loss: 0.4349
Epoch 195 Loss: 0.4387
Epoch 196 Loss: 0.4397
Epoch 197 Loss: 0.4436
Epoch 198 Loss: 0.4442
Epoch 199 Loss: 0.4412
Epoch 200 Loss: 0.4403