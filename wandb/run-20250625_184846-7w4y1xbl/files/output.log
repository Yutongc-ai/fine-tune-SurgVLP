/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  0
num_shots is  1
num_shots is  1
===================vision logits
tensor([[0.5381, 0.5883, 0.5839, 0.3086, 0.5878, 0.3556, 0.3723],
        [0.6690, 0.5018, 0.5692, 0.7722, 0.3942, 0.5676, 0.4980],
        [0.5926, 0.4872, 0.4427, 0.6352, 0.3125, 0.6298, 0.4937],
        [0.7031, 0.5508, 0.8477, 0.4580, 0.5730, 0.5032, 0.5039],
        [0.6390, 0.5164, 0.5500, 0.3466, 0.7919, 0.4074, 0.4021],
        [0.7407, 0.5393, 0.6004, 0.5624, 0.6443, 0.4988, 0.4595],
        [0.6253, 0.6517, 0.5512, 0.5056, 0.4068, 0.7081, 0.7233]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.1212, -0.0318, -0.0945,  0.0624, -0.1693, -0.1214, -0.0121],
        [-0.0149,  0.0877, -0.1678, -0.0968, -0.0940,  0.0081, -0.1278],
        [-0.2130,  0.1264, -0.2738, -0.2090, -0.1786, -0.1872, -0.0562],
        [-0.1954,  0.0261, -0.2391,  0.0432, -0.1832,  0.1477, -0.1103],
        [ 0.0315,  0.0506, -0.0059, -0.0496, -0.0313, -0.1143, -0.0812],
        [ 0.0120,  0.0086, -0.0157, -0.0635,  0.0885, -0.0276,  0.0290],
        [-0.2281,  0.1510, -0.1678, -0.1967, -0.0804, -0.0475, -0.0640]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.4282, 0.4849, 0.4708, 0.2676, 0.4616, 0.2761, 0.3083],
        [0.5550, 0.4328, 0.4464, 0.6274, 0.3128, 0.4744, 0.3937],
        [0.4583, 0.4270, 0.3233, 0.4945, 0.2307, 0.4937, 0.4021],
        [0.5534, 0.4633, 0.6665, 0.3889, 0.4470, 0.4439, 0.4016],
        [0.5378, 0.4388, 0.4574, 0.2805, 0.6547, 0.3204, 0.3216],
        [0.6192, 0.4508, 0.4977, 0.4581, 0.5517, 0.4111, 0.3878],
        [0.4830, 0.5683, 0.4314, 0.3885, 0.3256, 0.5822, 0.5920]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 1 Loss: 0.7917 Alpha: tensor([0.1990, 0.1990, 0.2010, 0.2010, 0.2010, 0.1990, 0.2010],
       device='cuda:0', grad_fn=<SelectBackward0>)
Epoch 1 best val f1 0.4194 test f1 0.3069670498371124
===================vision logits
tensor([[0.5913, 0.4637, 0.5040, 0.2971, 0.7646, 0.3516, 0.3460],
        [0.6560, 0.4450, 0.5275, 0.4877, 0.5762, 0.4015, 0.3633],
        [0.4739, 0.5363, 0.5237, 0.2544, 0.5317, 0.3005, 0.3187],
        [0.6169, 0.4363, 0.5139, 0.7440, 0.3407, 0.5103, 0.4366],
        [0.6579, 0.4984, 0.8182, 0.4157, 0.5328, 0.4509, 0.4508],
        [0.5919, 0.6431, 0.5112, 0.4757, 0.3630, 0.7085, 0.7243],
        [0.5600, 0.4558, 0.4045, 0.6092, 0.2761, 0.6103, 0.4709]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.0581,  0.0371,  0.0118, -0.0282, -0.0233, -0.1086, -0.0714],
        [ 0.0452, -0.0324, -0.0060, -0.0402,  0.0967, -0.0113,  0.0558],
        [-0.1204, -0.0414, -0.0881,  0.0835, -0.1721, -0.1354, -0.0075],
        [ 0.0188,  0.0748, -0.1730, -0.0818, -0.0959,  0.0126, -0.1353],
        [-0.1773,  0.0093, -0.2383,  0.0649, -0.1867,  0.1651, -0.1140],
        [-0.2733,  0.1619, -0.1730, -0.2263, -0.0733, -0.0698, -0.0607],
        [-0.2227,  0.1215, -0.2715, -0.2243, -0.1880, -0.1917, -0.0602]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.5028, 0.3929, 0.4216, 0.2427, 0.6328, 0.2752, 0.2762],
        [0.5546, 0.3658, 0.4382, 0.3993, 0.4960, 0.3330, 0.3118],
        [0.3752, 0.4404, 0.4213, 0.2258, 0.4139, 0.2281, 0.2641],
        [0.5176, 0.3763, 0.3989, 0.6058, 0.2676, 0.4277, 0.3409],
        [0.5192, 0.4172, 0.6414, 0.3570, 0.4124, 0.4035, 0.3563],
        [0.4483, 0.5632, 0.3967, 0.3582, 0.2900, 0.5794, 0.5929],
        [0.4301, 0.4003, 0.2913, 0.4697, 0.1985, 0.4772, 0.3820]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 2 Loss: 0.7745 Alpha: tensor([0.1980, 0.1980, 0.2020, 0.2017, 0.2019, 0.1980, 0.2010],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[0.5759, 0.3602, 0.4574, 0.4175, 0.5104, 0.3142, 0.2772],
        [0.6167, 0.4518, 0.7885, 0.3777, 0.4957, 0.4041, 0.4031],
        [0.5568, 0.4238, 0.4690, 0.2626, 0.7419, 0.3096, 0.3038],
        [0.5285, 0.4272, 0.3690, 0.5847, 0.2391, 0.5903, 0.4489],
        [0.4212, 0.4923, 0.4710, 0.2121, 0.4841, 0.2558, 0.2743],
        [0.5711, 0.3780, 0.4678, 0.7157, 0.2949, 0.4597, 0.3834],
        [0.5610, 0.6320, 0.4763, 0.4471, 0.3316, 0.7021, 0.7202]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.0724, -0.0707,  0.0042, -0.0199,  0.1048,  0.0039,  0.0799],
        [-0.1632, -0.0027, -0.2405,  0.0836, -0.1898,  0.1752, -0.1209],
        [ 0.0821,  0.0304,  0.0247, -0.0091, -0.0161, -0.1021, -0.0668],
        [-0.2331,  0.1150, -0.2753, -0.2392, -0.1940, -0.1933, -0.0618],
        [-0.1190, -0.0505, -0.0847,  0.1014, -0.1745, -0.1454, -0.0043],
        [ 0.0477,  0.0706, -0.1762, -0.0671, -0.0954,  0.0205, -0.1404],
        [-0.3021,  0.1681, -0.1705, -0.2428, -0.0670, -0.0865, -0.0620]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.4927, 0.2890, 0.3812, 0.3441, 0.4423, 0.2629, 0.2442],
        [0.4878, 0.3767, 0.6156, 0.3283, 0.3805, 0.3663, 0.3154],
        [0.4784, 0.3588, 0.3944, 0.2170, 0.6146, 0.2415, 0.2418],
        [0.4026, 0.3756, 0.2607, 0.4464, 0.1664, 0.4608, 0.3635],
        [0.3319, 0.4026, 0.3776, 0.1935, 0.3735, 0.1895, 0.2276],
        [0.4846, 0.3272, 0.3596, 0.5843, 0.2294, 0.3871, 0.2957],
        [0.4183, 0.5553, 0.3676, 0.3313, 0.2646, 0.5717, 0.5893]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 3 Loss: 0.7602 Alpha: tensor([0.1970, 0.1970, 0.2028, 0.2018, 0.2027, 0.1970, 0.2005],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[0.5230, 0.3859, 0.4351, 0.2301, 0.7159, 0.2710, 0.2634],
        [0.5322, 0.6184, 0.4439, 0.4215, 0.3045, 0.6923, 0.7115],
        [0.5028, 0.2816, 0.3909, 0.3565, 0.4462, 0.2373, 0.1994],
        [0.5852, 0.4157, 0.7631, 0.3481, 0.4658, 0.3671, 0.3647],
        [0.3799, 0.4594, 0.4295, 0.1774, 0.4440, 0.2255, 0.2441],
        [0.4999, 0.3993, 0.3380, 0.5601, 0.2087, 0.5711, 0.4284],
        [0.5317, 0.3278, 0.4247, 0.6901, 0.2549, 0.4156, 0.3360]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.1012,  0.0233,  0.0369,  0.0063, -0.0089, -0.1015, -0.0610],
        [-0.3291,  0.1682, -0.1717, -0.2561, -0.0691, -0.1028, -0.0692],
        [ 0.0915, -0.1026,  0.0119, -0.0012,  0.1081,  0.0168,  0.0970],
        [-0.1534, -0.0087, -0.2441,  0.0970, -0.1892,  0.1800, -0.1224],
        [-0.1251, -0.0566, -0.0799,  0.1067, -0.1772, -0.1553,  0.0016],
        [-0.2403,  0.1115, -0.2734, -0.2528, -0.2030, -0.1929, -0.0648],
        [ 0.0752,  0.0645, -0.1772, -0.0556, -0.0938,  0.0259, -0.1446]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.4536, 0.3262, 0.3679, 0.1925, 0.5937, 0.2097, 0.2092],
        [0.3904, 0.5443, 0.3401, 0.3077, 0.2415, 0.5615, 0.5811],
        [0.4351, 0.2184, 0.3270, 0.2964, 0.3893, 0.2010, 0.1823],
        [0.4636, 0.3459, 0.5933, 0.3060, 0.3554, 0.3363, 0.2834],
        [0.2968, 0.3745, 0.3436, 0.1655, 0.3393, 0.1628, 0.2036],
        [0.3780, 0.3519, 0.2349, 0.4236, 0.1393, 0.4453, 0.3461],
        [0.4566, 0.2845, 0.3232, 0.5649, 0.1961, 0.3515, 0.2557]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 4 Loss: 0.7481 Alpha: tensor([0.1960, 0.1960, 0.2036, 0.2014, 0.2031, 0.1960, 0.1997],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[0.4790, 0.3789, 0.3159, 0.5403, 0.1850, 0.5574, 0.4133],
        [0.4983, 0.2848, 0.3893, 0.6654, 0.2224, 0.3745, 0.2937],
        [0.3417, 0.4285, 0.3908, 0.1448, 0.4065, 0.1980, 0.2170],
        [0.4430, 0.2211, 0.3356, 0.3054, 0.3926, 0.1772, 0.1399],
        [0.5517, 0.3795, 0.7354, 0.3149, 0.4351, 0.3300, 0.3275],
        [0.5076, 0.6066, 0.4213, 0.3985, 0.2839, 0.6806, 0.7012],
        [0.4946, 0.3540, 0.4055, 0.2047, 0.6921, 0.2392, 0.2307]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.2505,  0.1083, -0.2781, -0.2636, -0.2087, -0.1962, -0.0704],
        [ 0.1015,  0.0640, -0.1778, -0.0393, -0.0862,  0.0350, -0.1455],
        [-0.1264, -0.0690, -0.0737,  0.1134, -0.1778, -0.1609,  0.0076],
        [ 0.1067, -0.1331,  0.0151,  0.0130,  0.1084,  0.0232,  0.1063],
        [-0.1424, -0.0151, -0.2448,  0.1090, -0.1907,  0.1869, -0.1237],
        [-0.3521,  0.1770, -0.1754, -0.2645, -0.0683, -0.1136, -0.0725],
        [ 0.1171,  0.0239,  0.0479,  0.0196, -0.0012, -0.0994, -0.0567]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.3595, 0.3346, 0.2154, 0.4055, 0.1185, 0.4339, 0.3328],
        [0.4332, 0.2486, 0.2934, 0.5472, 0.1703, 0.3188, 0.2206],
        [0.2650, 0.3470, 0.3123, 0.1395, 0.3078, 0.1392, 0.1821],
        [0.3879, 0.1631, 0.2814, 0.2564, 0.3446, 0.1520, 0.1343],
        [0.4380, 0.3148, 0.5696, 0.2804, 0.3294, 0.3066, 0.2524],
        [0.3667, 0.5362, 0.3204, 0.2874, 0.2244, 0.5505, 0.5724],
        [0.4327, 0.2999, 0.3450, 0.1737, 0.5750, 0.1837, 0.1829]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 5 Loss: 0.7381 Alpha: tensor([0.1950, 0.1950, 0.2041, 0.2008, 0.2032, 0.1950, 0.1989],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[0.4574, 0.3590, 0.2926, 0.5178, 0.1625, 0.5422, 0.3974],
        [0.3129, 0.4058, 0.3605, 0.1194, 0.3764, 0.1787, 0.1975],
        [0.5246, 0.3491, 0.7110, 0.2877, 0.4099, 0.2978, 0.2948],
        [0.3934, 0.1726, 0.2898, 0.2619, 0.3484, 0.1271, 0.0924],
        [0.4699, 0.3266, 0.3793, 0.1812, 0.6713, 0.2115, 0.2027],
        [0.4836, 0.5931, 0.3978, 0.3783, 0.2642, 0.6692, 0.6913],
        [0.4664, 0.2428, 0.3526, 0.6424, 0.1929, 0.3348, 0.2529]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.2587,  0.1075, -0.2822, -0.2745, -0.2106, -0.1981, -0.0744],
        [-0.1302, -0.0780, -0.0717,  0.1197, -0.1792, -0.1686,  0.0100],
        [-0.1338, -0.0190, -0.2457,  0.1208, -0.1906,  0.1927, -0.1242],
        [ 0.1231, -0.1542,  0.0218,  0.0272,  0.1119,  0.0285,  0.1183],
        [ 0.1273,  0.0209,  0.0588,  0.0280,  0.0033, -0.0963, -0.0508],
        [-0.3659,  0.1779, -0.1739, -0.2729, -0.0687, -0.1223, -0.0770],
        [ 0.1262,  0.0571, -0.1719, -0.0237, -0.0780,  0.0392, -0.1455]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.3406, 0.3180, 0.1952, 0.3853, 0.0995, 0.4214, 0.3191],
        [0.2406, 0.3269, 0.2872, 0.1194, 0.2826, 0.1220, 0.1664],
        [0.4171, 0.2890, 0.5488, 0.2598, 0.3085, 0.2806, 0.2253],
        [0.3493, 0.1193, 0.2444, 0.2227, 0.3084, 0.1110, 0.0967],
        [0.4140, 0.2767, 0.3250, 0.1556, 0.5584, 0.1613, 0.1606],
        [0.3449, 0.5254, 0.3009, 0.2694, 0.2080, 0.5401, 0.5638],
        [0.4109, 0.2125, 0.2637, 0.5310, 0.1471, 0.2866, 0.1868]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 6 Loss: 0.7297 Alpha: tensor([0.1940, 0.1940, 0.2043, 0.2000, 0.2030, 0.1940, 0.1980],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[0.3602, 0.1392, 0.2567, 0.2325, 0.3152, 0.0956, 0.0614],
        [0.4621, 0.5787, 0.3803, 0.3611, 0.2466, 0.6570, 0.6771],
        [0.5081, 0.3304, 0.6912, 0.2685, 0.3946, 0.2788, 0.2747],
        [0.4483, 0.3033, 0.3525, 0.1621, 0.6501, 0.1888, 0.1779],
        [0.4405, 0.2071, 0.3351, 0.6209, 0.1667, 0.2899, 0.2153],
        [0.4321, 0.3359, 0.2653, 0.4933, 0.1384, 0.5224, 0.3755],
        [0.2945, 0.3923, 0.3367, 0.0995, 0.3595, 0.1684, 0.1849]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.1284, -0.1677,  0.0298,  0.0305,  0.1157,  0.0316,  0.1275],
        [-0.3809,  0.1912, -0.1809, -0.2837, -0.0608, -0.1311, -0.0762],
        [-0.1308, -0.0264, -0.2411,  0.1180, -0.1910,  0.1905, -0.1289],
        [ 0.1359,  0.0161,  0.0630,  0.0343,  0.0090, -0.0987, -0.0459],
        [ 0.1482,  0.0359, -0.1779,  0.0380, -0.0862,  0.0483, -0.1493],
        [-0.2626,  0.1069, -0.2825, -0.2885, -0.2133, -0.2073, -0.0741],
        [-0.1296, -0.0735, -0.0677,  0.1086, -0.1752, -0.1675,  0.0131]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.3225, 0.0893, 0.2182, 0.1988, 0.2815, 0.0852, 0.0723],
        [0.3251, 0.5157, 0.2851, 0.2536, 0.1947, 0.5290, 0.5526],
        [0.4043, 0.2724, 0.5330, 0.2434, 0.2958, 0.2644, 0.2080],
        [0.3975, 0.2567, 0.3034, 0.1408, 0.5419, 0.1421, 0.1409],
        [0.3930, 0.1793, 0.2481, 0.5237, 0.1240, 0.2506, 0.1550],
        [0.3192, 0.2987, 0.1723, 0.3630, 0.0790, 0.4038, 0.3012],
        [0.2256, 0.3166, 0.2681, 0.1010, 0.2693, 0.1138, 0.1565]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 7 Loss: 0.7231 Alpha: tensor([0.1930, 0.1930, 0.2043, 0.1992, 0.2026, 0.1930, 0.1971],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[0.4406, 0.5638, 0.3625, 0.3444, 0.2299, 0.6434, 0.6629],
        [0.3325, 0.1113, 0.2286, 0.2070, 0.2875, 0.0670, 0.0342],
        [0.4147, 0.1748, 0.3101, 0.5950, 0.1461, 0.2506, 0.1794],
        [0.4159, 0.3208, 0.2481, 0.4766, 0.1230, 0.5095, 0.3616],
        [0.4329, 0.2847, 0.3334, 0.1480, 0.6326, 0.1697, 0.1578],
        [0.4898, 0.3094, 0.6694, 0.2497, 0.3764, 0.2580, 0.2530],
        [0.2768, 0.3787, 0.3154, 0.0818, 0.3430, 0.1568, 0.1733]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.3915,  0.1998, -0.1847, -0.2879, -0.0562, -0.1392, -0.0751],
        [ 0.1377, -0.1778,  0.0325,  0.0345,  0.1182,  0.0339,  0.1347],
        [ 0.1648,  0.0188, -0.1753,  0.0765, -0.0890,  0.0566, -0.1458],
        [-0.2673,  0.1093, -0.2835, -0.2989, -0.2145, -0.2121, -0.0761],
        [ 0.1448,  0.0066,  0.0653,  0.0400,  0.0146, -0.0953, -0.0433],
        [-0.1261, -0.0360, -0.2377,  0.1196, -0.1936,  0.1874, -0.1330],
        [-0.1264, -0.0692, -0.0652,  0.1062, -0.1710, -0.1673,  0.0149]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.3060, 0.5049, 0.2696, 0.2394, 0.1818, 0.5168, 0.5414],
        [0.3010, 0.0645, 0.1953, 0.1783, 0.2590, 0.0617, 0.0508],
        [0.3742, 0.1496, 0.2277, 0.5089, 0.1065, 0.2192, 0.1259],
        [0.3054, 0.2866, 0.1579, 0.3477, 0.0662, 0.3927, 0.2895],
        [0.3863, 0.2398, 0.2879, 0.1301, 0.5285, 0.1268, 0.1247],
        [0.3902, 0.2535, 0.5155, 0.2281, 0.2804, 0.2466, 0.1894],
        [0.2115, 0.3063, 0.2508, 0.0858, 0.2564, 0.1043, 0.1472]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 8 Loss: 0.7175 Alpha: tensor([0.1921, 0.1919, 0.2041, 0.1983, 0.2020, 0.1920, 0.1962],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[0.4140, 0.2615, 0.3108, 0.1310, 0.6113, 0.1479, 0.1354],
        [0.2590, 0.3649, 0.2936, 0.0645, 0.3244, 0.1459, 0.1614],
        [0.3914, 0.1474, 0.2847, 0.5709, 0.1286, 0.2192, 0.1492],
        [0.4214, 0.5500, 0.3473, 0.3290, 0.2168, 0.6302, 0.6486],
        [0.4027, 0.3069, 0.2337, 0.4616, 0.1100, 0.4969, 0.3479],
        [0.4743, 0.2913, 0.6497, 0.2327, 0.3600, 0.2401, 0.2347],
        [0.3092, 0.0873, 0.2046, 0.1857, 0.2631, 0.0424, 0.0107]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.1527, -0.0017,  0.0693,  0.0455,  0.0191, -0.0938, -0.0398],
        [-0.1283, -0.0733, -0.0637,  0.1024, -0.1707, -0.1694,  0.0176],
        [ 0.1784,  0.0035, -0.1678,  0.0980, -0.0931,  0.0614, -0.1438],
        [-0.3990,  0.2045, -0.1865, -0.2962, -0.0524, -0.1474, -0.0743],
        [-0.2699,  0.1081, -0.2880, -0.3067, -0.2162, -0.2121, -0.0778],
        [-0.1212, -0.0437, -0.2356,  0.1209, -0.1942,  0.1877, -0.1367],
        [ 0.1450, -0.1842,  0.0348,  0.0411,  0.1197,  0.0384,  0.1391]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.3719, 0.2191, 0.2699, 0.1168, 0.5118, 0.1090, 0.1067],
        [0.1966, 0.2943, 0.2330, 0.0708, 0.2412, 0.0951, 0.1378],
        [0.3571, 0.1242, 0.2080, 0.4926, 0.0914, 0.1938, 0.1011],
        [0.2892, 0.4943, 0.2568, 0.2256, 0.1715, 0.5050, 0.5301],
        [0.2943, 0.2749, 0.1452, 0.3345, 0.0552, 0.3827, 0.2781],
        [0.3784, 0.2374, 0.4996, 0.2142, 0.2669, 0.2317, 0.1738],
        [0.2827, 0.0436, 0.1758, 0.1617, 0.2390, 0.0417, 0.0318]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 9 Loss: 0.7124 Alpha: tensor([0.1911, 0.1909, 0.2037, 0.1974, 0.2013, 0.1909, 0.1952],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.3858,  0.2913,  0.2157,  0.4438,  0.0953,  0.4826,  0.3330],
        [ 0.4038,  0.5390,  0.3361,  0.3142,  0.2072,  0.6177,  0.6360],
        [ 0.2874,  0.0644,  0.1807,  0.1651,  0.2402,  0.0193, -0.0104],
        [ 0.4581,  0.2714,  0.6296,  0.2163,  0.3436,  0.2200,  0.2145],
        [ 0.2415,  0.3504,  0.2734,  0.0489,  0.3057,  0.1344,  0.1494],
        [ 0.3961,  0.2402,  0.2920,  0.1137,  0.5902,  0.1261,  0.1138],
        [ 0.3735,  0.1253,  0.2648,  0.5501,  0.1139,  0.1938,  0.1251]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.2720,  0.1045, -0.2884, -0.3135, -0.2192, -0.2135, -0.0821],
        [-0.4060,  0.2089, -0.1880, -0.2993, -0.0531, -0.1515, -0.0759],
        [ 0.1509, -0.1910,  0.0378,  0.0464,  0.1199,  0.0410,  0.1410],
        [-0.1149, -0.0501, -0.2302,  0.1252, -0.1936,  0.1915, -0.1391],
        [-0.1309, -0.0760, -0.0639,  0.1023, -0.1721, -0.1738,  0.0176],
        [ 0.1629, -0.0084,  0.0710,  0.0531,  0.0263, -0.0898, -0.0346],
        [ 0.1887, -0.0067, -0.1674,  0.1148, -0.0937,  0.0638, -0.1452]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[0.2803, 0.2614, 0.1304, 0.3190, 0.0426, 0.3710, 0.2652],
        [0.2739, 0.4860, 0.2474, 0.2131, 0.1636, 0.4944, 0.5198],
        [0.2655, 0.0235, 0.1565, 0.1455, 0.2200, 0.0228, 0.0144],
        [0.3662, 0.2199, 0.4841, 0.2013, 0.2536, 0.2154, 0.1568],
        [0.1818, 0.2820, 0.2163, 0.0577, 0.2256, 0.0850, 0.1279],
        [0.3587, 0.2004, 0.2546, 0.1037, 0.4957, 0.0915, 0.0896],
        [0.3438, 0.1042, 0.1916, 0.4783, 0.0791, 0.1730, 0.0810]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 10 Loss: 0.7077 Alpha: tensor([0.1901, 0.1899, 0.2032, 0.1964, 0.2005, 0.1899, 0.1942],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.2682,  0.0437,  0.1575,  0.1481,  0.2186, -0.0013, -0.0297],
        [ 0.2266,  0.3378,  0.2565,  0.0341,  0.2889,  0.1243,  0.1388],
        [ 0.3853,  0.5271,  0.3223,  0.2992,  0.1966,  0.6043,  0.6225],
        [ 0.3806,  0.2217,  0.2738,  0.0990,  0.5718,  0.1072,  0.0955],
        [ 0.3730,  0.2793,  0.2019,  0.4274,  0.0848,  0.4705,  0.3208],
        [ 0.4430,  0.2528,  0.6104,  0.2011,  0.3279,  0.2001,  0.1944],
        [ 0.3587,  0.1064,  0.2453,  0.5337,  0.1018,  0.1720,  0.1044]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.1579, -0.1972,  0.0429,  0.0481,  0.1216,  0.0438,  0.1439],
        [-0.1340, -0.0787, -0.0644,  0.1004, -0.1698, -0.1753,  0.0215],
        [-0.4116,  0.2145, -0.1888, -0.3023, -0.0497, -0.1551, -0.0735],
        [ 0.1736, -0.0147,  0.0764,  0.0578,  0.0321, -0.0869, -0.0310],
        [-0.2745,  0.1011, -0.2911, -0.3186, -0.2181, -0.2141, -0.0838],
        [-0.1097, -0.0536, -0.2263,  0.1295, -0.1931,  0.1915, -0.1390],
        [ 0.2014, -0.0098, -0.1620,  0.1301, -0.0837,  0.0666, -0.1392]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.2506,  0.0052,  0.1381,  0.1316,  0.2024,  0.0059, -0.0015],
        [ 0.1690,  0.2714,  0.2023,  0.0450,  0.2123,  0.0765,  0.1197],
        [ 0.2580,  0.4772,  0.2360,  0.2004,  0.1555,  0.4831,  0.5093],
        [ 0.3475,  0.1840,  0.2404,  0.0922,  0.4817,  0.0762,  0.0750],
        [ 0.2695,  0.2509,  0.1186,  0.3049,  0.0342,  0.3612,  0.2550],
        [ 0.3547,  0.2039,  0.4691,  0.1893,  0.2409,  0.1987,  0.1402],
        [ 0.3335,  0.0878,  0.1765,  0.4674,  0.0708,  0.1552,  0.0648]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 11 Loss: 0.7035 Alpha: tensor([0.1891, 0.1888, 0.2026, 0.1955, 0.1997, 0.1888, 0.1932],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.2516,  0.0246,  0.1386,  0.1323,  0.1986, -0.0211, -0.0480],
        [ 0.3593,  0.2655,  0.1885,  0.4108,  0.0729,  0.4569,  0.3066],
        [ 0.4283,  0.2347,  0.5906,  0.1868,  0.3118,  0.1816,  0.1759],
        [ 0.2110,  0.3243,  0.2389,  0.0213,  0.2705,  0.1143,  0.1281],
        [ 0.3674,  0.2049,  0.2582,  0.0863,  0.5540,  0.0901,  0.0785],
        [ 0.3431,  0.0862,  0.2256,  0.5146,  0.0889,  0.1508,  0.0839],
        [ 0.3693,  0.5164,  0.3114,  0.2855,  0.1884,  0.5917,  0.6089]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.1634, -0.2012,  0.0449,  0.0552,  0.1226,  0.0434,  0.1451],
        [-0.2762,  0.0965, -0.2942, -0.3224, -0.2222, -0.2154, -0.0884],
        [-0.1037, -0.0571, -0.2224,  0.1336, -0.1935,  0.1914, -0.1389],
        [-0.1386, -0.0801, -0.0659,  0.1004, -0.1711, -0.1793,  0.0231],
        [ 0.1818, -0.0194,  0.0807,  0.0628,  0.0362, -0.0829, -0.0274],
        [ 0.2109, -0.0164, -0.1554,  0.1408, -0.0816,  0.0695, -0.1406],
        [-0.4201,  0.2160, -0.1911, -0.3040, -0.0526, -0.1597, -0.0738]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.2376, -0.0113,  0.1228,  0.1197,  0.1860, -0.0109, -0.0167],
        [ 0.2582,  0.2387,  0.1072,  0.2909,  0.0238,  0.3501,  0.2427],
        [ 0.3437,  0.1884,  0.4536,  0.1781,  0.2277,  0.1831,  0.1249],
        [ 0.1554,  0.2600,  0.1875,  0.0342,  0.1970,  0.0677,  0.1111],
        [ 0.3379,  0.1693,  0.2283,  0.0824,  0.4678,  0.0626,  0.0614],
        [ 0.3220,  0.0699,  0.1614,  0.4535,  0.0605,  0.1379,  0.0476],
        [ 0.2438,  0.4687,  0.2267,  0.1891,  0.1483,  0.4723,  0.4984]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 12 Loss: 0.6995 Alpha: tensor([0.1882, 0.1878, 0.2018, 0.1945, 0.1988, 0.1878, 0.1922],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.1950,  0.3101,  0.2218,  0.0085,  0.2512,  0.1034,  0.1164],
        [ 0.2399,  0.0105,  0.1227,  0.1208,  0.1821, -0.0360, -0.0620],
        [ 0.4157,  0.2181,  0.5717,  0.1753,  0.2960,  0.1654,  0.1591],
        [ 0.3270,  0.0675,  0.2046,  0.4948,  0.0755,  0.1299,  0.0641],
        [ 0.3546,  0.1882,  0.2435,  0.0744,  0.5363,  0.0738,  0.0631],
        [ 0.3535,  0.5049,  0.3004,  0.2733,  0.1790,  0.5797,  0.5961],
        [ 0.3455,  0.2511,  0.1756,  0.3947,  0.0612,  0.4427,  0.2918]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.1414, -0.0820, -0.0690,  0.1019, -0.1722, -0.1813,  0.0255],
        [ 0.1656, -0.2045,  0.0456,  0.0577,  0.1215,  0.0428,  0.1467],
        [-0.1000, -0.0595, -0.2201,  0.1355, -0.1934,  0.1907, -0.1400],
        [ 0.2225, -0.0245, -0.1525,  0.1523, -0.0800,  0.0682, -0.1410],
        [ 0.1903, -0.0240,  0.0854,  0.0687,  0.0413, -0.0776, -0.0228],
        [-0.4253,  0.2174, -0.1928, -0.3082, -0.0534, -0.1615, -0.0728],
        [-0.2772,  0.0919, -0.2968, -0.3261, -0.2242, -0.2145, -0.0903]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.1417,  0.2481,  0.1730,  0.0237,  0.1810,  0.0584,  0.1018],
        [ 0.2282, -0.0235,  0.1098,  0.1105,  0.1721, -0.0235, -0.0284],
        [ 0.3340,  0.1742,  0.4388,  0.1688,  0.2148,  0.1694,  0.1109],
        [ 0.3104,  0.0529,  0.1446,  0.4390,  0.0497,  0.1202,  0.0311],
        [ 0.3286,  0.1547,  0.2170,  0.0734,  0.4542,  0.0499,  0.0493],
        [ 0.2301,  0.4595,  0.2176,  0.1786,  0.1405,  0.4625,  0.4883],
        [ 0.2469,  0.2259,  0.0963,  0.2773,  0.0139,  0.3388,  0.2302]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 13 Loss: 0.6958 Alpha: tensor([0.1872, 0.1867, 0.2010, 0.1935, 0.1978, 0.1867, 0.1911],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.4032,  0.2014,  0.5535,  0.1630,  0.2805,  0.1488,  0.1422],
        [ 0.3329,  0.2386,  0.1627,  0.3790,  0.0509,  0.4298,  0.2789],
        [ 0.2288, -0.0036,  0.1076,  0.1091,  0.1660, -0.0504, -0.0753],
        [ 0.3390,  0.4964,  0.2909,  0.2621,  0.1720,  0.5686,  0.5847],
        [ 0.3170,  0.0532,  0.1891,  0.4804,  0.0664,  0.1155,  0.0500],
        [ 0.3413,  0.1711,  0.2278,  0.0617,  0.5192,  0.0562,  0.0469],
        [ 0.1827,  0.2981,  0.2070, -0.0014,  0.2358,  0.0950,  0.1063]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.0957, -0.0609, -0.2191,  0.1389, -0.1934,  0.1905, -0.1398],
        [-0.2764,  0.0867, -0.2985, -0.3285, -0.2252, -0.2150, -0.0924],
        [ 0.1708, -0.2083,  0.0469,  0.0610,  0.1208,  0.0432,  0.1481],
        [-0.4296,  0.2184, -0.1926, -0.3101, -0.0538, -0.1644, -0.0733],
        [ 0.2288, -0.0323, -0.1503,  0.1565, -0.0802,  0.0691, -0.1436],
        [ 0.1996, -0.0288,  0.0906,  0.0742,  0.0467, -0.0749, -0.0191],
        [-0.1436, -0.0837, -0.0716,  0.1017, -0.1746, -0.1848,  0.0245]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.3245,  0.1601,  0.4242,  0.1591,  0.2023,  0.1554,  0.0970],
        [ 0.2368,  0.2147,  0.0855,  0.2643,  0.0053,  0.3283,  0.2193],
        [ 0.2196, -0.0359,  0.0975,  0.1013,  0.1586, -0.0357, -0.0394],
        [ 0.2178,  0.4526,  0.2099,  0.1694,  0.1348,  0.4533,  0.4792],
        [ 0.3031,  0.0397,  0.1323,  0.4279,  0.0422,  0.1082,  0.0190],
        [ 0.3189,  0.1397,  0.2049,  0.0638,  0.4411,  0.0356,  0.0363],
        [ 0.1312,  0.2380,  0.1604,  0.0153,  0.1680,  0.0510,  0.0932]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 14 Loss: 0.6925 Alpha: tensor([0.1863, 0.1857, 0.2002, 0.1924, 0.1969, 0.1857, 0.1901],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.3906,  0.1841,  0.5342,  0.1509,  0.2655,  0.1313,  0.1242],
        [ 0.3193,  0.2240,  0.1490,  0.3624,  0.0409,  0.4148,  0.2637],
        [ 0.2207, -0.0156,  0.0946,  0.0989,  0.1528, -0.0619, -0.0863],
        [ 0.3072,  0.0387,  0.1741,  0.4649,  0.0569,  0.1002,  0.0355],
        [ 0.3309,  0.1562,  0.2134,  0.0528,  0.5034,  0.0411,  0.0319],
        [ 0.1700,  0.2867,  0.1925, -0.0110,  0.2200,  0.0864,  0.0967],
        [ 0.3242,  0.4873,  0.2811,  0.2503,  0.1658,  0.5570,  0.5728]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.0880, -0.0635, -0.2150,  0.1440, -0.1920,  0.1923, -0.1379],
        [-0.2771,  0.0810, -0.2999, -0.3314, -0.2272, -0.2149, -0.0915],
        [ 0.1751, -0.2115,  0.0470,  0.0641,  0.1202,  0.0414,  0.1494],
        [ 0.2342, -0.0367, -0.1473,  0.1652, -0.0794,  0.0703, -0.1437],
        [ 0.2085, -0.0319,  0.0941,  0.0798,  0.0500, -0.0751, -0.0180],
        [-0.1447, -0.0842, -0.0736,  0.1021, -0.1756, -0.1872,  0.0252],
        [-0.4331,  0.2174, -0.1924, -0.3104, -0.0553, -0.1664, -0.0738]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.3154,  0.1453,  0.4092,  0.1497,  0.1903,  0.1409,  0.0824],
        [ 0.2257,  0.2016,  0.0741,  0.2504, -0.0032,  0.3162,  0.2070],
        [ 0.2135, -0.0463,  0.0867,  0.0932,  0.1474, -0.0457, -0.0486],
        [ 0.2957,  0.0269,  0.1204,  0.4165,  0.0345,  0.0955,  0.0069],
        [ 0.3117,  0.1268,  0.1935,  0.0571,  0.4289,  0.0229,  0.0239],
        [ 0.1206,  0.2286,  0.1481,  0.0073,  0.1549,  0.0436,  0.0853],
        [ 0.2053,  0.4450,  0.2021,  0.1599,  0.1295,  0.4437,  0.4696]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 15 Loss: 0.6894 Alpha: tensor([0.1853, 0.1846, 0.1993, 0.1914, 0.1959, 0.1846, 0.1890],
       device='cuda:0', grad_fn=<SelectBackward0>)
Traceback (most recent call last):
  File "few_shot.py", line 82, in <module>
    main(configs[args.method], args.method)
  File "few_shot.py", line 58, in main
    metrics = method(dataset)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yongxuan/SurgVLP/methods/linear_probe_plus.py", line 283, in forward
    logits = self.compute_training_logits(image_features)
  File "/home/yongxuan/SurgVLP/methods/linear_probe_plus.py", line 144, in compute_training_logits
    print("===================vision logits")
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/lib/redirect.py", line 643, in write
    cb(data)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 1907, in <lambda>
    lambda data: self._console_raw_callback("stdout", data),
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 1291, in _console_raw_callback
    self._backend.interface.publish_output_raw(name, data)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 628, in publish_output_raw
    self._publish_output_raw(o)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 75, in _publish_output_raw
    self._publish(rec)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
KeyboardInterrupt
===================vision logits
tensor([[ 0.3791,  0.1686,  0.5159,  0.1398,  0.2509,  0.1160,  0.1085],
        [ 0.3088,  0.2112,  0.1377,  0.3487,  0.0322,  0.4014,  0.2493],
        [ 0.1592,  0.2760,  0.1796, -0.0192,  0.2057,  0.0781,  0.0873],
        [ 0.3215,  0.1421,  0.1998,  0.0458,  0.4881,  0.0271,  0.0185],
        [ 0.2133, -0.0265,  0.0824,  0.0891,  0.1406, -0.0729, -0.0966],
        [ 0.2993,  0.0262,  0.1620,  0.4510,  0.0485,  0.0879,  0.0239],
        [ 0.3103,  0.4786,  0.2726,  0.2398,  0.1601,  0.5460,  0.5612]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.0838, -0.0662, -0.2141,  0.1469, -0.1916,  0.1924, -0.1363],
        [-0.2759,  0.0767, -0.3021, -0.3289, -0.2265, -0.2163, -0.0919],
        [-0.1466, -0.0856, -0.0768,  0.1034, -0.1773, -0.1918,  0.0249],
        [ 0.2177, -0.0340,  0.0999,  0.0858,  0.0554, -0.0707, -0.0142],
        [ 0.1791, -0.2146,  0.0479,  0.0665,  0.1180,  0.0403,  0.1490],
        [ 0.2366, -0.0406, -0.1473,  0.1690, -0.0790,  0.0723, -0.1435],
        [-0.4357,  0.2186, -0.1909, -0.3120, -0.0548, -0.1654, -0.0719]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.3068,  0.1320,  0.3946,  0.1409,  0.1784,  0.1279,  0.0695],
        [ 0.2174,  0.1903,  0.0646,  0.2399, -0.0102,  0.3051,  0.1951],
        [ 0.1114,  0.2196,  0.1370,  0.0005,  0.1430,  0.0361,  0.0773],
        [ 0.3052,  0.1146,  0.1832,  0.0522,  0.4172,  0.0118,  0.0133],
        [ 0.2079, -0.0558,  0.0766,  0.0855,  0.1369, -0.0553, -0.0575],
        [ 0.2895,  0.0158,  0.1106,  0.4057,  0.0276,  0.0855, -0.0027],
        [ 0.1937,  0.4380,  0.1956,  0.1512,  0.1249,  0.4351,  0.4606]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 16 Loss: 0.6866 Alpha: tensor([0.1844, 0.1836, 0.1984, 0.1903, 0.1949, 0.1836, 0.1879],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.2066, -0.0310,  0.0747,  0.0765,  0.1302, -0.0775, -0.0983],
        [ 0.3114,  0.1283,  0.1852,  0.0375,  0.4731,  0.0147,  0.0073],
        [ 0.3001,  0.2007,  0.1298,  0.3351,  0.0248,  0.3904,  0.2385],
        [ 0.3688,  0.1545,  0.4997,  0.1302,  0.2368,  0.1018,  0.0940],
        [ 0.2947,  0.0175,  0.1518,  0.4401,  0.0431,  0.0797,  0.0157],
        [ 0.2960,  0.4684,  0.2621,  0.2305,  0.1515,  0.5360,  0.5491],
        [ 0.1476,  0.2664,  0.1683, -0.0313,  0.1933,  0.0703,  0.0794]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.1626, -0.2200,  0.0338,  0.0411,  0.0925,  0.0376,  0.1578],
        [ 0.2252, -0.0350,  0.1078,  0.0927,  0.0646, -0.0678, -0.0121],
        [-0.2734,  0.0686, -0.3061, -0.3296, -0.2286, -0.2149, -0.0989],
        [-0.0774, -0.0615, -0.2142,  0.1523, -0.1845,  0.1942, -0.1357],
        [ 0.2433, -0.0459, -0.1414,  0.1747, -0.0777,  0.0717, -0.1469],
        [-0.4409,  0.2208, -0.1930, -0.3141, -0.0534, -0.1703, -0.0760],
        [-0.1502, -0.0856, -0.0768,  0.1095, -0.1767, -0.1942,  0.0212]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.1998, -0.0603,  0.0679,  0.0708,  0.1240, -0.0596, -0.0578],
        [ 0.2980,  0.1030,  0.1724,  0.0463,  0.4065,  0.0019,  0.0042],
        [ 0.2108,  0.1802,  0.0577,  0.2288, -0.0165,  0.2965,  0.1851],
        [ 0.2994,  0.1210,  0.3815,  0.1337,  0.1681,  0.1161,  0.0577],
        [ 0.2867,  0.0077,  0.1032,  0.3977,  0.0234,  0.0785, -0.0100],
        [ 0.1813,  0.4300,  0.1868,  0.1435,  0.1181,  0.4264,  0.4502],
        [ 0.1013,  0.2118,  0.1277, -0.0088,  0.1329,  0.0293,  0.0702]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 17 Loss: 0.6842 Alpha: tensor([0.1834, 0.1825, 0.1974, 0.1892, 0.1938, 0.1825, 0.1868],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 2.8773e-01,  6.3409e-03,  1.3743e-01,  4.2884e-01,  3.6538e-02,
          6.9203e-02,  5.0179e-03],
        [ 3.0036e-01,  1.1330e-01,  1.7105e-01,  2.8964e-02,  4.5702e-01,
         -3.3387e-04, -6.1481e-03],
        [ 1.3714e-01,  2.5717e-01,  1.5670e-01, -4.1782e-02,  1.8067e-01,
          6.2591e-02,  7.1288e-02],
        [ 2.8992e-01,  1.8851e-01,  1.1956e-01,  3.2167e-01,  1.6043e-02,
          3.7816e-01,  2.2574e-01],
        [ 2.0698e-01, -2.9542e-02,  7.2539e-02,  6.5976e-02,  1.2900e-01,
         -7.7087e-02, -9.4129e-02],
        [ 3.6013e-01,  1.4319e-01,  4.8536e-01,  1.2133e-01,  2.2487e-01,
          8.9457e-02,  8.2266e-02],
        [ 2.8180e-01,  4.5910e-01,  2.5256e-01,  2.2260e-01,  1.4356e-01,
          5.2560e-01,  5.3701e-01]], device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.2481, -0.0431, -0.1366,  0.1823, -0.0782,  0.0698, -0.1491],
        [ 0.2324, -0.0381,  0.1142,  0.0993,  0.0717, -0.0637, -0.0098],
        [-0.1507, -0.0845, -0.0779,  0.1149, -0.1726, -0.1927,  0.0211],
        [-0.2685,  0.0634, -0.3078, -0.3280, -0.2277, -0.2119, -0.1020],
        [ 0.1542, -0.2294,  0.0276,  0.0255,  0.0736,  0.0375,  0.1582],
        [-0.0702, -0.0590, -0.2140,  0.1578, -0.1764,  0.1987, -0.1340],
        [-0.4434,  0.2263, -0.1927, -0.3128, -0.0510, -0.1735, -0.0775]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.2816, -0.0013,  0.0923,  0.3896,  0.0179,  0.0693, -0.0192],
        [ 0.2898,  0.0899,  0.1617,  0.0402,  0.3945, -0.0101, -0.0067],
        [ 0.0925,  0.2044,  0.1180, -0.0169,  0.1233,  0.0232,  0.0634],
        [ 0.2034,  0.1692,  0.0491,  0.2183, -0.0235,  0.2871,  0.1742],
        [ 0.1988, -0.0604,  0.0651,  0.0595,  0.1200, -0.0594, -0.0544],
        [ 0.2934,  0.1120,  0.3701,  0.1271,  0.1597,  0.1063,  0.0482],
        [ 0.1694,  0.4232,  0.1792,  0.1374,  0.1120,  0.4177,  0.4403]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 18 Loss: 0.6820 Alpha: tensor([0.1825, 0.1814, 0.1964, 0.1881, 0.1928, 0.1814, 0.1857],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.1271,  0.2479,  0.1459, -0.0506,  0.1676,  0.0546,  0.0628],
        [ 0.2701,  0.4515,  0.2453,  0.2161,  0.1385,  0.5164,  0.5261],
        [ 0.2070, -0.0310,  0.0685,  0.0608,  0.1225, -0.0773, -0.0928],
        [ 0.2816,  0.1784,  0.1102,  0.3102,  0.0093,  0.3673,  0.2151],
        [ 0.2925,  0.1001,  0.1596,  0.0233,  0.4428, -0.0128, -0.0177],
        [ 0.2820, -0.0038,  0.1262,  0.4186,  0.0295,  0.0614, -0.0037],
        [ 0.3512,  0.1315,  0.4699,  0.1118,  0.2136,  0.0773,  0.0705]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.1518, -0.0830, -0.0813,  0.1190, -0.1689, -0.1916,  0.0216],
        [-0.4438,  0.2314, -0.1909, -0.3122, -0.0512, -0.1726, -0.0793],
        [ 0.1476, -0.2317,  0.0201,  0.0135,  0.0639,  0.0350,  0.1542],
        [-0.2626,  0.0580, -0.3086, -0.3279, -0.2284, -0.2063, -0.1064],
        [ 0.2406, -0.0388,  0.1211,  0.1024,  0.0772, -0.0576, -0.0052],
        [ 0.2522, -0.0468, -0.1314,  0.1851, -0.0780,  0.0701, -0.1527],
        [-0.0661, -0.0588, -0.2120,  0.1617, -0.1728,  0.2000, -0.1331]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.0841,  0.1971,  0.1086, -0.0237,  0.1132,  0.0168,  0.0564],
        [ 0.1599,  0.4177,  0.1737,  0.1324,  0.1078,  0.4106,  0.4312],
        [ 0.1978, -0.0618,  0.0606,  0.0533,  0.1130, -0.0601, -0.0541],
        [ 0.1976,  0.1599,  0.0415,  0.2092, -0.0291,  0.2792,  0.1647],
        [ 0.2845,  0.0788,  0.1532,  0.0358,  0.3837, -0.0196, -0.0157],
        [ 0.2774, -0.0104,  0.0839,  0.3816,  0.0121,  0.0627, -0.0270],
        [ 0.2868,  0.1023,  0.3580,  0.1197,  0.1511,  0.0961,  0.0386]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 19 Loss: 0.6801 Alpha: tensor([0.1816, 0.1804, 0.1954, 0.1870, 0.1917, 0.1803, 0.1846],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.3431,  0.1199,  0.4547,  0.1036,  0.2015,  0.0656,  0.0584],
        [ 0.1175,  0.2397,  0.1345, -0.0591,  0.1558,  0.0481,  0.0555],
        [ 0.2766,  0.1710,  0.1039,  0.3004,  0.0052,  0.3583,  0.2062],
        [ 0.2080, -0.0330,  0.0665,  0.0542,  0.1178, -0.0795, -0.0945],
        [ 0.2853,  0.0875,  0.1490,  0.0176,  0.4282, -0.0243, -0.0283],
        [ 0.2760, -0.0143,  0.1145,  0.4080,  0.0228,  0.0520, -0.0128],
        [ 0.2579,  0.4433,  0.2369,  0.2084,  0.1330,  0.5066,  0.5151]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.0619, -0.0596, -0.2123,  0.1638, -0.1723,  0.2008, -0.1338],
        [-0.1536, -0.0826, -0.0844,  0.1240, -0.1651, -0.1930,  0.0221],
        [-0.2592,  0.0540, -0.3101, -0.3256, -0.2263, -0.2036, -0.1085],
        [ 0.1425, -0.2316,  0.0138,  0.0089,  0.0568,  0.0338,  0.1540],
        [ 0.2485, -0.0385,  0.1236,  0.1061,  0.0832, -0.0510, -0.0018],
        [ 0.2564, -0.0477, -0.1285,  0.1915, -0.0784,  0.0668, -0.1590],
        [-0.4466,  0.2345, -0.1901, -0.3135, -0.0514, -0.1743, -0.0788]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.2809,  0.0925,  0.3457,  0.1131,  0.1414,  0.0862,  0.0284],
        [ 0.0759,  0.1905,  0.0987, -0.0302,  0.1042,  0.0113,  0.0503],
        [ 0.1943,  0.1532,  0.0363,  0.2018, -0.0321,  0.2725,  0.1571],
        [ 0.1979, -0.0633,  0.0579,  0.0471,  0.1080, -0.0622, -0.0557],
        [ 0.2797,  0.0683,  0.1448,  0.0316,  0.3727, -0.0284, -0.0241],
        [ 0.2730, -0.0194,  0.0748,  0.3739,  0.0065,  0.0543, -0.0356],
        [ 0.1497,  0.4114,  0.1671,  0.1261,  0.1033,  0.4026,  0.4225]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 20 Loss: 0.6781 Alpha: tensor([0.1806, 0.1793, 0.1943, 0.1859, 0.1906, 0.1792, 0.1835],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.2777,  0.0768,  0.1378,  0.0108,  0.4145, -0.0354, -0.0378],
        [ 0.3355,  0.1084,  0.4401,  0.0949,  0.1913,  0.0538,  0.0467],
        [ 0.2723, -0.0207,  0.1059,  0.3982,  0.0168,  0.0461, -0.0187],
        [ 0.2454,  0.4343,  0.2278,  0.2011,  0.1274,  0.4968,  0.5037],
        [ 0.2693,  0.1616,  0.0953,  0.2887, -0.0013,  0.3472,  0.1951],
        [ 0.2055, -0.0408,  0.0612,  0.0453,  0.1094, -0.0864, -0.1021],
        [ 0.1081,  0.2308,  0.1231, -0.0676,  0.1444,  0.0408,  0.0474]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.2552, -0.0425,  0.1297,  0.1093,  0.0875, -0.0467,  0.0030],
        [-0.0551, -0.0611, -0.2098,  0.1670, -0.1709,  0.2027, -0.1338],
        [ 0.2594, -0.0504, -0.1279,  0.1950, -0.0788,  0.0618, -0.1617],
        [-0.4478,  0.2386, -0.1881, -0.3126, -0.0506, -0.1745, -0.0791],
        [-0.2552,  0.0468, -0.3110, -0.3272, -0.2278, -0.2006, -0.1098],
        [ 0.1410, -0.2372,  0.0085,  0.0083,  0.0484,  0.0311,  0.1520],
        [-0.1565, -0.0834, -0.0865,  0.1271, -0.1621, -0.1953,  0.0242]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.2743,  0.0587,  0.1365,  0.0262,  0.3621, -0.0371, -0.0315],
        [ 0.2757,  0.0826,  0.3344,  0.1062,  0.1333,  0.0764,  0.0187],
        [ 0.2703, -0.0252,  0.0678,  0.3664,  0.0015,  0.0485, -0.0409],
        [ 0.1394,  0.4045,  0.1602,  0.1206,  0.0989,  0.3948,  0.4134],
        [ 0.1890,  0.1442,  0.0292,  0.1921, -0.0376,  0.2639,  0.1478],
        [ 0.1957, -0.0707,  0.0526,  0.0395,  0.0996, -0.0685, -0.0627],
        [ 0.0676,  0.1830,  0.0890, -0.0371,  0.0953,  0.0050,  0.0438]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 21 Loss: 0.6761 Alpha: tensor([0.1797, 0.1782, 0.1932, 0.1848, 0.1895, 0.1782, 0.1824],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.2712,  0.0664,  0.1289,  0.0052,  0.4020, -0.0458, -0.0465],
        [ 0.2616,  0.1514,  0.0869,  0.2767, -0.0075,  0.3361,  0.1837],
        [ 0.2717, -0.0250,  0.0991,  0.3908,  0.0143,  0.0428, -0.0217],
        [ 0.3282,  0.0985,  0.4258,  0.0865,  0.1814,  0.0433,  0.0370],
        [ 0.2031, -0.0459,  0.0555,  0.0371,  0.1008, -0.0920, -0.1076],
        [ 0.2332,  0.4259,  0.2198,  0.1934,  0.1217,  0.4870,  0.4924],
        [ 0.0993,  0.2222,  0.1123, -0.0744,  0.1328,  0.0334,  0.0392]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[ 0.2625, -0.0440,  0.1339,  0.1153,  0.0920, -0.0401,  0.0050],
        [-0.2519,  0.0432, -0.3095, -0.3274, -0.2279, -0.1986, -0.1088],
        [ 0.2611, -0.0502, -0.1252,  0.1954, -0.0818,  0.0580, -0.1677],
        [-0.0495, -0.0616, -0.2081,  0.1705, -0.1688,  0.2034, -0.1344],
        [ 0.1377, -0.2427,  0.0014,  0.0066,  0.0417,  0.0287,  0.1499],
        [-0.4486,  0.2411, -0.1865, -0.3110, -0.0500, -0.1746, -0.0780],
        [-0.1575, -0.0839, -0.0880,  0.1292, -0.1580, -0.1982,  0.0268]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.2698,  0.0497,  0.1297,  0.0224,  0.3527, -0.0449, -0.0385],
        [ 0.1834,  0.1350,  0.0227,  0.1825, -0.0426,  0.2553,  0.1386],
        [ 0.2701, -0.0289,  0.0628,  0.3604, -0.0010,  0.0451, -0.0442],
        [ 0.2707,  0.0743,  0.3231,  0.0996,  0.1256,  0.0675,  0.0105],
        [ 0.1931, -0.0757,  0.0467,  0.0324,  0.0914, -0.0738, -0.0678],
        [ 0.1293,  0.3980,  0.1540,  0.1148,  0.0943,  0.3870,  0.4044],
        [ 0.0601,  0.1759,  0.0798, -0.0427,  0.0865, -0.0017,  0.0373]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 22 Loss: 0.6743 Alpha: tensor([0.1788, 0.1771, 0.1922, 0.1837, 0.1884, 0.1771, 0.1813],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits
tensor([[ 0.3215,  0.0889,  0.4120,  0.0786,  0.1719,  0.0334,  0.0274],
        [ 0.2678, -0.0332,  0.0915,  0.3797,  0.0091,  0.0353, -0.0298],
        [ 0.2555,  0.1412,  0.0792,  0.2657, -0.0136,  0.3246,  0.1722],
        [ 0.2642,  0.0561,  0.1187, -0.0005,  0.3884, -0.0554, -0.0552],
        [ 0.2018, -0.0505,  0.0511,  0.0293,  0.0932, -0.0968, -0.1121],
        [ 0.0914,  0.2142,  0.1016, -0.0800,  0.1216,  0.0271,  0.0321],
        [ 0.2212,  0.4173,  0.2122,  0.1859,  0.1159,  0.4775,  0.4816]],
       device='cuda:0', grad_fn=<MmBackward0>)
===================text logits
tensor([[-0.0452, -0.0637, -0.2050,  0.1726, -0.1666,  0.2050, -0.1324],
        [ 0.2620, -0.0518, -0.1248,  0.1950, -0.0841,  0.0544, -0.1695],
        [-0.2482,  0.0358, -0.3101, -0.3250, -0.2306, -0.1947, -0.1120],
        [ 0.2674, -0.0463,  0.1362,  0.1186,  0.0940, -0.0379,  0.0059],
        [ 0.1314, -0.2473, -0.0060,  0.0063,  0.0347,  0.0254,  0.1498],
        [-0.1581, -0.0837, -0.0894,  0.1320, -0.1550, -0.2009,  0.0276],
        [-0.4496,  0.2423, -0.1840, -0.3115, -0.0482, -0.1736, -0.0761]],
       device='cuda:0', grad_fn=<MmBackward0>)
tensor([[ 0.2659,  0.0659,  0.3125,  0.0932,  0.1182,  0.0593,  0.0029],
        [ 0.2669, -0.0360,  0.0566,  0.3511, -0.0057,  0.0382, -0.0512],
        [ 0.1791,  0.1253,  0.0165,  0.1740, -0.0480,  0.2465,  0.1286],
        [ 0.2647,  0.0407,  0.1215,  0.0179,  0.3417, -0.0527, -0.0458],
        [ 0.1911, -0.0801,  0.0419,  0.0258,  0.0840, -0.0784, -0.0719],
        [ 0.0536,  0.1694,  0.0708, -0.0471,  0.0777, -0.0072,  0.0314],
        [ 0.1194,  0.3910,  0.1483,  0.1087,  0.0899,  0.3795,  0.3960]],
       device='cuda:0', grad_fn=<DivBackward0>)
Epoch 23 Loss: 0.6725 Alpha: tensor([0.1779, 0.1761, 0.1911, 0.1826, 0.1873, 0.1760, 0.1802],
       device='cuda:0', grad_fn=<SelectBackward0>)
===================vision logits