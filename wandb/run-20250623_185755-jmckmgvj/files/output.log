/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.8667
Epoch 1 best val f1 0.4194 test f1 0.30578282475471497
Epoch 2 Loss: 0.7619
Epoch 3 Loss: 0.7186
Epoch 4 Loss: 0.6894
Epoch 5 Loss: 0.6678
Epoch 6 Loss: 0.6497
Epoch 7 Loss: 0.6339
Epoch 8 Loss: 0.6205
Epoch 9 Loss: 0.6083
Epoch 10 Loss: 0.5976
Epoch 11 Loss: 0.5879
Epoch 12 Loss: 0.5786
Epoch 13 Loss: 0.5693
Epoch 14 Loss: 0.5608
Epoch 15 Loss: 0.5529
Epoch 16 Loss: 0.5451
Epoch 17 Loss: 0.5444
Epoch 18 Loss: 0.5456
Epoch 19 Loss: 0.5468
Epoch 20 Loss: 0.5478
Epoch 21 Loss: 0.5482
Epoch 21 best val f1 0.4262 test f1 0.3088906407356262
Epoch 22 Loss: 0.5482
Epoch 23 Loss: 0.5476
Epoch 23 best val f1 0.4333 test f1 0.31065741181373596
Epoch 24 Loss: 0.5467
Epoch 24 best val f1 0.4407 test f1 0.31180211901664734
Epoch 25 Loss: 0.5454
Epoch 26 Loss: 0.5435
Epoch 26 best val f1 0.4483 test f1 0.3147686719894409
Epoch 27 Loss: 0.5410
Epoch 27 best val f1 0.4561 test f1 0.31659141182899475
Epoch 28 Loss: 0.5385
Epoch 28 best val f1 0.4815 test f1 0.31862908601760864
Epoch 29 Loss: 0.5358
Epoch 29 best val f1 0.5098 test f1 0.32080212235450745
Epoch 30 Loss: 0.5331
Epoch 30 best val f1 0.5200 test f1 0.3234076499938965
Epoch 31 Loss: 0.5301
Epoch 32 Loss: 0.5270
Epoch 33 Loss: 0.5240
Epoch 34 Loss: 0.5210
Epoch 35 Loss: 0.5178
Epoch 36 Loss: 0.5150
Epoch 37 Loss: 0.5121
Epoch 38 Loss: 0.5091
Epoch 39 Loss: 0.5062
Epoch 40 Loss: 0.5032
Epoch 41 Loss: 0.5005
Epoch 42 Loss: 0.4978
Epoch 42 best val f1 0.5217 test f1 0.37385115027427673
Epoch 43 Loss: 0.4950
Epoch 44 Loss: 0.4924
Epoch 45 Loss: 0.4900
Epoch 45 best val f1 0.5455 test f1 0.389447420835495
Epoch 46 Loss: 0.4872
Epoch 47 Loss: 0.4848
Epoch 48 Loss: 0.4824
Epoch 49 Loss: 0.4801
Epoch 50 Loss: 0.4778
Epoch 51 Loss: 0.4755
Epoch 52 Loss: 0.4733
Epoch 53 Loss: 0.4712
Epoch 54 Loss: 0.4691
Epoch 55 Loss: 0.4670
Epoch 56 Loss: 0.4650
Epoch 57 Loss: 0.4629
Epoch 58 Loss: 0.4607
Epoch 59 Loss: 0.4585
Epoch 60 Loss: 0.4564
Epoch 61 Loss: 0.4543
Epoch 62 Loss: 0.4522
Epoch 63 Loss: 0.4502
Epoch 64 Loss: 0.4483
Epoch 65 Loss: 0.4462
Epoch 66 Loss: 0.4440
Epoch 67 Loss: 0.4421
Epoch 68 Loss: 0.4400
Epoch 69 Loss: 0.4382
Epoch 70 Loss: 0.4365
Epoch 71 Loss: 0.4352
Epoch 72 Loss: 0.4339
Epoch 73 Loss: 0.4325
Epoch 73 best val f1 0.5714 test f1 0.48886120319366455
Epoch 74 Loss: 0.4310
Epoch 75 Loss: 0.4292
Epoch 76 Loss: 0.4274
Epoch 77 Loss: 0.4256
Epoch 78 Loss: 0.4239
Epoch 79 Loss: 0.4222
Epoch 80 Loss: 0.4203
Epoch 81 Loss: 0.4185
Epoch 82 Loss: 0.4167
Epoch 83 Loss: 0.4150
Epoch 84 Loss: 0.4134
Epoch 85 Loss: 0.4117
Epoch 86 Loss: 0.4100
Epoch 87 Loss: 0.4083
Epoch 88 Loss: 0.4066
Epoch 89 Loss: 0.4051
Epoch 90 Loss: 0.4036
Epoch 91 Loss: 0.4021
Epoch 92 Loss: 0.4007
Epoch 93 Loss: 0.3993
Epoch 94 Loss: 0.3979
Epoch 95 Loss: 0.3966
Epoch 96 Loss: 0.3952
Epoch 97 Loss: 0.3939
Epoch 98 Loss: 0.3926
Epoch 99 Loss: 0.3913
Epoch 100 Loss: 0.3900