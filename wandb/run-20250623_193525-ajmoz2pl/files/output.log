/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
loading num parts:  2
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  3
loading num parts:  4
num_shots is  4
num_shots is  4
Epoch 1 Loss: 1.1046
Epoch 1 best val f1 0.4065 test f1 0.299802303314209
Epoch 2 Loss: 0.7582
Epoch 3 Loss: 0.6468
Epoch 4 Loss: 0.5993
Epoch 5 Loss: 0.5795
Epoch 6 Loss: 0.5683
Epoch 7 Loss: 0.5595
Epoch 8 Loss: 0.5507
Epoch 9 Loss: 0.5422
Epoch 9 best val f1 0.4118 test f1 0.3059716522693634
Epoch 10 Loss: 0.5345
Epoch 10 best val f1 0.4286 test f1 0.3069431483745575
Epoch 11 Loss: 0.5277
Epoch 12 Loss: 0.5216
Epoch 13 Loss: 0.5157
Epoch 14 Loss: 0.5098
Epoch 15 Loss: 0.5039
Epoch 16 Loss: 0.4983
Epoch 17 Loss: 0.4929
Epoch 18 Loss: 0.4872
Epoch 19 Loss: 0.4811
Epoch 20 Loss: 0.4747
Epoch 21 Loss: 0.4682
Epoch 22 Loss: 0.4614
Epoch 23 Loss: 0.4544
Epoch 24 Loss: 0.4476
Epoch 25 Loss: 0.4407
Epoch 26 Loss: 0.4334
Epoch 27 Loss: 0.4259
Epoch 28 Loss: 0.4187
Epoch 29 Loss: 0.4114
Epoch 30 Loss: 0.4040
Epoch 31 Loss: 0.3968
Epoch 32 Loss: 0.3899
Epoch 33 Loss: 0.3830
Epoch 34 Loss: 0.3761
Epoch 35 Loss: 0.3693
Epoch 36 Loss: 0.3630
Epoch 37 Loss: 0.3568
Epoch 37 best val f1 0.4675 test f1 0.32532963156700134
Epoch 38 Loss: 0.3509
Epoch 39 Loss: 0.3450
Epoch 40 Loss: 0.3394
Epoch 41 Loss: 0.3337
Epoch 42 Loss: 0.3281
Epoch 43 Loss: 0.3229
Epoch 44 Loss: 0.3205
Epoch 45 Loss: 0.3382
Epoch 46 Loss: 0.3481
Epoch 47 Loss: 0.3483
Epoch 48 Loss: 0.3420
Epoch 49 Loss: 0.3338
Epoch 50 Loss: 0.3276
Epoch 51 Loss: 0.3299
Epoch 52 Loss: 0.3281
Epoch 53 Loss: 0.3231
Epoch 54 Loss: 0.3175
Epoch 55 Loss: 0.3135
Epoch 56 Loss: 0.3090
Epoch 57 Loss: 0.3061
Epoch 58 Loss: 0.3030
Epoch 59 Loss: 0.2991
Epoch 60 Loss: 0.2950
Epoch 61 Loss: 0.2913
Epoch 62 Loss: 0.2886
Epoch 63 Loss: 0.3211
Epoch 64 Loss: 0.3573
Epoch 65 Loss: 0.3687
Epoch 66 Loss: 0.3682
Epoch 67 Loss: 0.3664
Epoch 68 Loss: 0.3635
Epoch 69 Loss: 0.3669
Epoch 70 Loss: 0.3683
Epoch 71 Loss: 0.3671
Epoch 72 Loss: 0.3633
Epoch 73 Loss: 0.3583
Epoch 74 Loss: 0.3535
Epoch 75 Loss: 0.3484
Epoch 76 Loss: 0.3436
Epoch 77 Loss: 0.3394
Epoch 78 Loss: 0.3354
Epoch 79 Loss: 0.3312
Epoch 80 Loss: 0.3275
Epoch 81 Loss: 0.3242
Epoch 82 Loss: 0.3209
Epoch 83 Loss: 0.3175
Epoch 84 Loss: 0.3139
Epoch 85 Loss: 0.3108
Epoch 86 Loss: 0.3079
Epoch 87 Loss: 0.3051
Epoch 88 Loss: 0.3023
Epoch 89 Loss: 0.3042
Epoch 90 Loss: 0.3057
Epoch 91 Loss: 0.3045
Epoch 92 Loss: 0.3027
Epoch 93 Loss: 0.3008
Epoch 94 Loss: 0.2983
Epoch 95 Loss: 0.2957
Epoch 96 Loss: 0.2933
Epoch 97 Loss: 0.2909
Epoch 98 Loss: 0.2882
Epoch 99 Loss: 0.2858
Epoch 100 Loss: 0.2836