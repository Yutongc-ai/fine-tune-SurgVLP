/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.9049
Epoch 1 best val f1 0.3934 test f1 0.30572307109832764
Epoch 2 Loss: 0.7325
Epoch 3 Loss: 0.6745
Epoch 4 Loss: 0.6353
Epoch 5 Loss: 0.6040
Epoch 6 Loss: 0.5774
Epoch 7 Loss: 0.5549
Epoch 8 Loss: 0.5367
Epoch 8 best val f1 0.4000 test f1 0.3057916760444641
Epoch 9 Loss: 0.5222
Epoch 9 best val f1 0.4151 test f1 0.30582448840141296
Epoch 10 Loss: 0.5108
Epoch 11 Loss: 0.5008
Epoch 12 Loss: 0.4921
Epoch 13 Loss: 0.4838
Epoch 14 Loss: 0.4758
Epoch 15 Loss: 0.4679
Epoch 16 Loss: 0.4639
Epoch 17 Loss: 0.4602
Epoch 18 Loss: 0.4554
Epoch 19 Loss: 0.4493
Epoch 20 Loss: 0.4429
Epoch 21 Loss: 0.4363
Epoch 22 Loss: 0.4297
Epoch 23 Loss: 0.4226
Epoch 24 Loss: 0.4155
Epoch 25 Loss: 0.4086
Epoch 26 Loss: 0.4020
Epoch 27 Loss: 0.3956
Epoch 28 Loss: 0.3893
Epoch 29 Loss: 0.3832
Epoch 30 Loss: 0.3773
Epoch 31 Loss: 0.3714
Epoch 32 Loss: 0.3658
Epoch 33 Loss: 0.3610
Epoch 34 Loss: 0.3686
Epoch 35 Loss: 0.3855
Epoch 36 Loss: 0.3979
Epoch 37 Loss: 0.3985
Epoch 38 Loss: 0.3900
Epoch 39 Loss: 0.3788
Epoch 40 Loss: 0.3697
Epoch 41 Loss: 0.3613
Epoch 42 Loss: 0.3542
Epoch 43 Loss: 0.3472
Epoch 44 Loss: 0.3413
Epoch 45 Loss: 0.3361
Epoch 46 Loss: 0.3313
Epoch 47 Loss: 0.3266
Epoch 48 Loss: 0.3221
Epoch 49 Loss: 0.3180
Epoch 50 Loss: 0.3140
Epoch 51 Loss: 0.3106
Epoch 52 Loss: 0.3082
Epoch 53 Loss: 0.3057
Epoch 54 Loss: 0.3031
Epoch 55 Loss: 0.3004
Epoch 56 Loss: 0.2977
Epoch 57 Loss: 0.2953
Epoch 58 Loss: 0.2929
Epoch 59 Loss: 0.2905
Epoch 60 Loss: 0.2884
Epoch 61 Loss: 0.2864
Epoch 62 Loss: 0.2844
Epoch 63 Loss: 0.2824
Epoch 64 Loss: 0.2810
Epoch 65 Loss: 0.2799
Epoch 66 Loss: 0.2786
Epoch 67 Loss: 0.2770
Epoch 68 Loss: 0.2752
Epoch 69 Loss: 0.2736
Epoch 70 Loss: 0.2735
Epoch 71 Loss: 0.2732
Epoch 72 Loss: 0.2724
Epoch 73 Loss: 0.2709
Epoch 74 Loss: 0.2692
Epoch 75 Loss: 0.2675
Epoch 76 Loss: 0.2657
Epoch 77 Loss: 0.2643
Epoch 78 Loss: 0.2644
Epoch 79 Loss: 0.2648
Epoch 80 Loss: 0.2632
Epoch 81 Loss: 0.2615
Epoch 82 Loss: 0.2600
Epoch 83 Loss: 0.2588
Epoch 84 Loss: 0.2577
Epoch 85 Loss: 0.2565
Epoch 86 Loss: 0.2556
Epoch 87 Loss: 0.2549
Epoch 88 Loss: 0.2537
Epoch 89 Loss: 0.2529
Epoch 90 Loss: 0.2518
Epoch 91 Loss: 0.2509
Epoch 92 Loss: 0.2500
Epoch 93 Loss: 0.2491
Epoch 94 Loss: 0.2485
Epoch 95 Loss: 0.2475
Epoch 96 Loss: 0.2467
Epoch 97 Loss: 0.2460
Epoch 98 Loss: 0.2451
Epoch 99 Loss: 0.2445
Epoch 100 Loss: 0.2437