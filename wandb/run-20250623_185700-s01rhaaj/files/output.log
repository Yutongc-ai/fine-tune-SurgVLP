/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.8360
Epoch 1 best val f1 0.4444 test f1 0.3057870864868164
Epoch 2 Loss: 0.8138
Epoch 3 Loss: 0.7962
Epoch 4 Loss: 0.7814
Epoch 5 Loss: 0.7688
Epoch 6 Loss: 0.7573
Epoch 7 Loss: 0.7468
Epoch 8 Loss: 0.7374
Epoch 9 Loss: 0.7288
Epoch 10 Loss: 0.7218
Epoch 11 Loss: 0.7152
Epoch 12 Loss: 0.7088
Epoch 13 Loss: 0.7027
Epoch 14 Loss: 0.6969
Epoch 15 Loss: 0.6910
Epoch 16 Loss: 0.6855
Epoch 17 Loss: 0.6804
Epoch 18 Loss: 0.6752
Epoch 19 Loss: 0.6704
Epoch 20 Loss: 0.6656
Epoch 21 Loss: 0.6611
Epoch 22 Loss: 0.6567
Epoch 22 best val f1 0.4516 test f1 0.30595681071281433
Epoch 23 Loss: 0.6527
Epoch 24 Loss: 0.6487
Epoch 25 Loss: 0.6450
Epoch 26 Loss: 0.6411
Epoch 27 Loss: 0.6373
Epoch 27 best val f1 0.4590 test f1 0.3062548339366913
Epoch 28 Loss: 0.6336
Epoch 28 best val f1 0.4667 test f1 0.3063788414001465
Epoch 29 Loss: 0.6311
Epoch 30 Loss: 0.6283
Epoch 30 best val f1 0.4746 test f1 0.3066926896572113
Epoch 31 Loss: 0.6255
Epoch 31 best val f1 0.4912 test f1 0.30691850185394287
Epoch 32 Loss: 0.6222
Epoch 33 Loss: 0.6188
Epoch 34 Loss: 0.6152
Epoch 35 Loss: 0.6112
Epoch 35 best val f1 0.5098 test f1 0.3084675073623657
Epoch 36 Loss: 0.6073
Epoch 36 best val f1 0.5306 test f1 0.30904504656791687
Epoch 37 Loss: 0.6035
Epoch 37 best val f1 0.5417 test f1 0.3098355531692505
Epoch 38 Loss: 0.5998
Epoch 38 best val f1 0.5652 test f1 0.3107104003429413
Epoch 39 Loss: 0.5959
Epoch 39 best val f1 0.5778 test f1 0.31171464920043945
Epoch 40 Loss: 0.5921
Epoch 41 Loss: 0.5883
Epoch 42 Loss: 0.5844
Epoch 43 Loss: 0.5806
Epoch 44 Loss: 0.5766
Epoch 45 Loss: 0.5729
Epoch 46 Loss: 0.5690
Epoch 47 Loss: 0.5652
Epoch 48 Loss: 0.5612
Epoch 49 Loss: 0.5572
Epoch 50 Loss: 0.5532
Epoch 51 Loss: 0.5493
Epoch 52 Loss: 0.5455
Epoch 53 Loss: 0.5416
Epoch 54 Loss: 0.5379
Epoch 55 Loss: 0.5339
Epoch 56 Loss: 0.5301
Epoch 57 Loss: 0.5262
Epoch 58 Loss: 0.5250
Epoch 59 Loss: 0.5239
Epoch 60 Loss: 0.5219
Epoch 61 Loss: 0.5190
Epoch 62 Loss: 0.5152
Epoch 63 Loss: 0.5110
Epoch 64 Loss: 0.5067
Epoch 65 Loss: 0.5026
Epoch 66 Loss: 0.4982
Epoch 67 Loss: 0.4942
Epoch 68 Loss: 0.4898
Epoch 69 Loss: 0.4855
Epoch 70 Loss: 0.4814
Epoch 71 Loss: 0.4775
Epoch 72 Loss: 0.4736
Epoch 73 Loss: 0.4699
Epoch 74 Loss: 0.4665
Epoch 75 Loss: 0.4631
Epoch 76 Loss: 0.4597
Epoch 77 Loss: 0.4563
Epoch 78 Loss: 0.4533
Epoch 79 Loss: 0.4502
Epoch 80 Loss: 0.4472
Epoch 81 Loss: 0.4444
Epoch 82 Loss: 0.4416
Epoch 83 Loss: 0.4388
Epoch 84 Loss: 0.4361
Epoch 85 Loss: 0.4336
Epoch 86 Loss: 0.4310
Epoch 87 Loss: 0.4287
Epoch 88 Loss: 0.4265
Epoch 89 Loss: 0.4244
Epoch 90 Loss: 0.4223
Epoch 91 Loss: 0.4202
Epoch 92 Loss: 0.4180
Epoch 93 Loss: 0.4157
Epoch 94 Loss: 0.4137
Epoch 95 Loss: 0.4117
Epoch 96 Loss: 0.4095
Epoch 97 Loss: 0.4074
Epoch 98 Loss: 0.4053
Epoch 99 Loss: 0.4032
Epoch 100 Loss: 0.4012