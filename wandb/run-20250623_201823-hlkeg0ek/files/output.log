/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
loading num parts:  2
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 1.0196
Epoch 1 best val f1 0.4241 test f1 0.2963685989379883
Epoch 2 Loss: 0.6101
Epoch 3 Loss: 0.5520
Epoch 4 Loss: 0.5161
Epoch 4 best val f1 0.4467 test f1 0.29905790090560913
Epoch 5 Loss: 0.4930
Epoch 6 Loss: 0.4782
Epoch 7 Loss: 0.4653
Epoch 8 Loss: 0.4498
Epoch 9 Loss: 0.4383
Epoch 10 Loss: 0.4246
Epoch 10 best val f1 0.4866 test f1 0.309150367975235
Epoch 11 Loss: 0.4167
Epoch 11 best val f1 0.4984 test f1 0.3109930157661438
Epoch 12 Loss: 0.3985
Epoch 13 Loss: 0.3832
Epoch 14 Loss: 0.3682
Epoch 14 best val f1 0.5843 test f1 0.31512451171875
Epoch 15 Loss: 0.3878
Epoch 16 Loss: 0.4036
Epoch 17 Loss: 0.3966
Epoch 18 Loss: 0.3852
Epoch 19 Loss: 0.3757
Epoch 20 Loss: 0.3680
Epoch 21 Loss: 0.3647
Epoch 22 Loss: 0.3456
Epoch 23 Loss: 0.3384
Epoch 24 Loss: 0.3338
Epoch 25 Loss: 0.3249
Epoch 26 Loss: 0.3213
Epoch 27 Loss: 0.3127
Epoch 28 Loss: 0.3129
Epoch 29 Loss: 0.3007
Epoch 30 Loss: 0.3003
Epoch 31 Loss: 0.3441
Epoch 32 Loss: 0.3523
Epoch 33 Loss: 0.3554
Epoch 34 Loss: 0.3485
Epoch 35 Loss: 0.3578
Epoch 36 Loss: 0.4209
Epoch 37 Loss: 0.4318
Epoch 38 Loss: 0.4349
Epoch 39 Loss: 0.4331
Epoch 40 Loss: 0.4260
Epoch 41 Loss: 0.4121
Epoch 42 Loss: 0.4027
Epoch 43 Loss: 0.4238
Epoch 44 Loss: 0.4290
Epoch 45 Loss: 0.4177
Epoch 46 Loss: 0.4099
Epoch 47 Loss: 0.4022
Epoch 48 Loss: 0.3949
Epoch 49 Loss: 0.3894
Epoch 50 Loss: 0.3805
Epoch 51 Loss: 0.3687
Epoch 52 Loss: 0.3615
Epoch 53 Loss: 0.3568
Epoch 53 best val f1 0.6150 test f1 0.38458430767059326
Epoch 54 Loss: 0.3668
Epoch 54 best val f1 0.6174 test f1 0.38521644473075867
Epoch 55 Loss: 0.3661
Epoch 56 Loss: 0.3867
Epoch 57 Loss: 0.3957
Epoch 58 Loss: 0.3953
Epoch 59 Loss: 0.3862
Epoch 60 Loss: 0.3751
Epoch 61 Loss: 0.3850
Epoch 62 Loss: 0.4325
Epoch 63 Loss: 0.4510
Epoch 64 Loss: 0.4581
Epoch 65 Loss: 0.4744
Epoch 66 Loss: 0.4858
Epoch 67 Loss: 0.4938
Epoch 68 Loss: 0.4919
Epoch 69 Loss: 0.4863
Epoch 70 Loss: 0.4871
Epoch 71 Loss: 0.4783
Epoch 72 Loss: 0.4807
Epoch 73 Loss: 0.4780
Epoch 74 Loss: 0.4712
Epoch 75 Loss: 0.4702
Epoch 76 Loss: 0.4746
Epoch 77 Loss: 0.4763
Epoch 78 Loss: 0.4671
Epoch 79 Loss: 0.4638
Epoch 80 Loss: 0.4598
Epoch 81 Loss: 0.4541
Epoch 82 Loss: 0.4505
Epoch 83 Loss: 0.4466
Epoch 84 Loss: 0.4392
Epoch 85 Loss: 0.4335
Epoch 86 Loss: 0.4285
Epoch 87 Loss: 0.4237
Epoch 88 Loss: 0.4186
Epoch 89 Loss: 0.4296
Epoch 90 Loss: 0.4320
Epoch 91 Loss: 0.4307
Epoch 92 Loss: 0.4311
Epoch 93 Loss: 0.4439
Epoch 94 Loss: 0.4421
Epoch 95 Loss: 0.4421
Epoch 96 Loss: 0.4432
Epoch 97 Loss: 0.4431
Epoch 98 Loss: 0.4387
Epoch 99 Loss: 0.4430
Epoch 100 Loss: 0.4390