/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  4
num_shots is  4
Epoch 1 Loss: 1.0205
Epoch 1 best val f1 0.4194 test f1 0.3052578568458557
Epoch 2 Loss: 0.7684
Epoch 3 Loss: 0.7091
Epoch 4 Loss: 0.6746
Epoch 5 Loss: 0.6467
Epoch 6 Loss: 0.6229
Epoch 7 Loss: 0.6025
Epoch 8 Loss: 0.5851
Epoch 9 Loss: 0.5710
Epoch 10 Loss: 0.5593
Epoch 11 Loss: 0.5493
Epoch 12 Loss: 0.5406
Epoch 13 Loss: 0.5322
Epoch 14 Loss: 0.5242
Epoch 14 best val f1 0.4878 test f1 0.30593350529670715
Epoch 15 Loss: 0.5164
Epoch 16 Loss: 0.5087
Epoch 17 Loss: 0.5014
Epoch 18 Loss: 0.4941
Epoch 19 Loss: 0.4870
Epoch 20 Loss: 0.4800
Epoch 21 Loss: 0.4731
Epoch 22 Loss: 0.4661
Epoch 23 Loss: 0.4591
Epoch 24 Loss: 0.4521
Epoch 25 Loss: 0.4452
Epoch 26 Loss: 0.4384
Epoch 27 Loss: 0.4339
Epoch 28 Loss: 0.4323
Epoch 29 Loss: 0.4313
Epoch 30 Loss: 0.4302
Epoch 31 Loss: 0.4283
Epoch 31 best val f1 0.5263 test f1 0.3081043064594269
Epoch 32 Loss: 0.4252
Epoch 32 best val f1 0.5455 test f1 0.3078683912754059
Epoch 33 Loss: 0.4208
Epoch 34 Loss: 0.4155
Epoch 35 Loss: 0.4099
Epoch 36 Loss: 0.4046
Epoch 37 Loss: 0.3990
Epoch 38 Loss: 0.3935
Epoch 39 Loss: 0.3879
Epoch 40 Loss: 0.3828
Epoch 41 Loss: 0.3779
Epoch 42 Loss: 0.3736
Epoch 43 Loss: 0.3694
Epoch 44 Loss: 0.3654
Epoch 45 Loss: 0.3618
Epoch 46 Loss: 0.3584
Epoch 47 Loss: 0.3552
Epoch 48 Loss: 0.3521
Epoch 49 Loss: 0.3492
Epoch 50 Loss: 0.3466
Epoch 51 Loss: 0.3441
Epoch 52 Loss: 0.3418
Epoch 53 Loss: 0.3396
Epoch 54 Loss: 0.3378
Epoch 55 Loss: 0.3364
Epoch 56 Loss: 0.3349
Epoch 57 Loss: 0.3333
Epoch 58 Loss: 0.3314
Epoch 59 Loss: 0.3304
Epoch 60 Loss: 0.3291
Epoch 61 Loss: 0.3277
Epoch 62 Loss: 0.3260
Epoch 63 Loss: 0.3240
Epoch 64 Loss: 0.3219
Epoch 65 Loss: 0.3199
Epoch 66 Loss: 0.3181
Epoch 67 Loss: 0.3168
Epoch 68 Loss: 0.3155
Epoch 69 Loss: 0.3136
Epoch 70 Loss: 0.3115
Epoch 71 Loss: 0.3096
Epoch 72 Loss: 0.3088
Epoch 73 Loss: 0.3077
Epoch 74 Loss: 0.3060
Epoch 75 Loss: 0.3054
Epoch 76 Loss: 0.3036
Epoch 77 Loss: 0.3017
Epoch 78 Loss: 0.2999
Epoch 79 Loss: 0.2980
Epoch 80 Loss: 0.2960
Epoch 81 Loss: 0.2944
Epoch 82 Loss: 0.2933
Epoch 83 Loss: 0.2919
Epoch 84 Loss: 0.2905
Epoch 85 Loss: 0.2893
Epoch 86 Loss: 0.2879
Epoch 87 Loss: 0.2867
Epoch 88 Loss: 0.2860
Epoch 89 Loss: 0.2850
Epoch 90 Loss: 0.2836
Epoch 91 Loss: 0.2826
Epoch 92 Loss: 0.2816
Epoch 93 Loss: 0.2806
Epoch 94 Loss: 0.2796
Epoch 95 Loss: 0.2786
Epoch 96 Loss: 0.2788
Epoch 97 Loss: 0.2793
Epoch 98 Loss: 0.2796
Epoch 99 Loss: 0.2789
Epoch 100 Loss: 0.2777