/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  4
num_shots is  4
Epoch 1 Loss: 0.8638
Epoch 1 best val f1 0.4065 test f1 0.2812453508377075
Epoch 2 Loss: 0.5988
Epoch 3 Loss: 0.5798
Epoch 4 Loss: 0.5683
Epoch 5 Loss: 0.5589
Epoch 6 Loss: 0.5520
Epoch 7 Loss: 0.5465
Epoch 7 best val f1 0.4153 test f1 0.2885337769985199
Epoch 8 Loss: 0.5412
Epoch 9 Loss: 0.5363
Epoch 9 best val f1 0.4267 test f1 0.29175883531570435
Epoch 10 Loss: 0.5317
Epoch 11 Loss: 0.5273
Epoch 12 Loss: 0.5231
Epoch 13 Loss: 0.5189
Epoch 14 Loss: 0.5146
Epoch 15 Loss: 0.5102
Epoch 16 Loss: 0.5055
Epoch 17 Loss: 0.5009
Epoch 18 Loss: 0.4962
Epoch 19 Loss: 0.4914
Epoch 20 Loss: 0.4867
Epoch 21 Loss: 0.4887
Epoch 22 Loss: 0.4946
Epoch 23 Loss: 0.4976
Epoch 24 Loss: 0.4978
Epoch 25 Loss: 0.4968
Epoch 26 Loss: 0.4953
Epoch 27 Loss: 0.4927
Epoch 28 Loss: 0.4901
Epoch 29 Loss: 0.4875
Epoch 30 Loss: 0.4845
Epoch 31 Loss: 0.4817
Epoch 32 Loss: 0.4786
Epoch 33 Loss: 0.4755
Epoch 34 Loss: 0.4722
Epoch 35 Loss: 0.4690
Epoch 36 Loss: 0.4659
Epoch 36 best val f1 0.4471 test f1 0.37088558077812195
Epoch 37 Loss: 0.4631
Epoch 38 Loss: 0.4599
Epoch 38 best val f1 0.4557 test f1 0.37755849957466125
Epoch 39 Loss: 0.4567
Epoch 40 Loss: 0.4536
Epoch 41 Loss: 0.4504
Epoch 42 Loss: 0.4474
Epoch 43 Loss: 0.4445
Epoch 44 Loss: 0.4418
Epoch 45 Loss: 0.4392
Epoch 46 Loss: 0.4366
Epoch 47 Loss: 0.4343
Epoch 48 Loss: 0.4323
Epoch 49 Loss: 0.4302
Epoch 50 Loss: 0.4280
Epoch 51 Loss: 0.4261
Epoch 52 Loss: 0.4244
Epoch 53 Loss: 0.4227
Epoch 54 Loss: 0.4211
Epoch 55 Loss: 0.4196
Epoch 55 best val f1 0.4615 test f1 0.41712191700935364
Epoch 56 Loss: 0.4183
Epoch 57 Loss: 0.4170
Epoch 58 Loss: 0.4159
Epoch 59 Loss: 0.4148
Epoch 60 Loss: 0.4135
Epoch 61 Loss: 0.4122
Epoch 62 Loss: 0.4110
Epoch 63 Loss: 0.4101
Epoch 64 Loss: 0.4090
Epoch 65 Loss: 0.4079
Epoch 66 Loss: 0.4069
Epoch 67 Loss: 0.4058
Epoch 68 Loss: 0.4051
Epoch 69 Loss: 0.4041
Epoch 70 Loss: 0.4031
Epoch 71 Loss: 0.4015
Epoch 72 Loss: 0.3999
Epoch 73 Loss: 0.3982
Epoch 74 Loss: 0.3968
Epoch 75 Loss: 0.3951
Epoch 76 Loss: 0.3937
Epoch 77 Loss: 0.3918
Epoch 78 Loss: 0.3901
Epoch 79 Loss: 0.3883
Epoch 80 Loss: 0.3866
Epoch 81 Loss: 0.3843
Epoch 82 Loss: 0.4195
Epoch 83 Loss: 0.4233
Epoch 84 Loss: 0.4206
Epoch 85 Loss: 0.4190
Epoch 86 Loss: 0.4121
Epoch 87 Loss: 0.4090
Epoch 88 Loss: 0.4077
Epoch 89 Loss: 0.4053
Epoch 90 Loss: 0.4011
Epoch 91 Loss: 0.3977
Epoch 92 Loss: 0.3958
Epoch 93 Loss: 0.4223
Epoch 94 Loss: 0.4419
Epoch 95 Loss: 0.4489
Epoch 96 Loss: 0.4512
Epoch 97 Loss: 0.4495
Epoch 98 Loss: 0.4477
Epoch 99 Loss: 0.4455
Epoch 100 Loss: 0.4431