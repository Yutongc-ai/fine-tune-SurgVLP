/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 2.1017
Epoch 1 best val f1 0.4049 test f1 0.29883086681365967
Epoch 2 Loss: 0.6923
Epoch 3 Loss: 0.6117
Epoch 4 Loss: 0.5859
Epoch 5 Loss: 0.5827
Epoch 5 best val f1 0.4092 test f1 0.3050672113895416
Epoch 6 Loss: 0.5741
Epoch 7 Loss: 0.5657
Epoch 8 Loss: 0.5621
Epoch 9 Loss: 0.5532
Epoch 10 Loss: 0.5485
Epoch 11 Loss: 0.5429
Epoch 12 Loss: 0.5350
Epoch 13 Loss: 0.5311
Epoch 14 Loss: 0.5274
Epoch 15 Loss: 0.5210
Epoch 16 Loss: 0.5161
Epoch 17 Loss: 0.5170
Epoch 17 best val f1 0.4584 test f1 0.34018373489379883
Epoch 18 Loss: 0.5140
Epoch 18 best val f1 0.4663 test f1 0.34264910221099854
Epoch 19 Loss: 0.5105
Epoch 20 Loss: 0.5090
Epoch 20 best val f1 0.4771 test f1 0.34521064162254333
Epoch 21 Loss: 0.5097
Epoch 21 best val f1 0.4868 test f1 0.3469899296760559
Epoch 22 Loss: 0.5063
Epoch 22 best val f1 0.4943 test f1 0.3471606373786926
Epoch 23 Loss: 0.5034
Epoch 23 best val f1 0.5000 test f1 0.3466727137565613
Epoch 24 Loss: 0.5038
Epoch 24 best val f1 0.5047 test f1 0.3465442657470703
Epoch 25 Loss: 0.5040
Epoch 25 best val f1 0.5047 test f1 0.3459623456001282
Epoch 26 Loss: 0.5051
Epoch 26 best val f1 0.5064 test f1 0.3457810878753662
Epoch 27 Loss: 0.5030
Epoch 28 Loss: 0.5047
Epoch 29 Loss: 0.5040
Epoch 30 Loss: 0.5022
Epoch 31 Loss: 0.5062
Epoch 32 Loss: 0.5042
Epoch 33 Loss: 0.5036
Epoch 34 Loss: 0.5049
Epoch 35 Loss: 0.5038
Epoch 36 Loss: 0.5040
Epoch 37 Loss: 0.5009
Epoch 38 Loss: 0.5031
Epoch 39 Loss: 0.5035
Epoch 40 Loss: 0.5018
Epoch 41 Loss: 0.5043
Epoch 42 Loss: 0.5002
Epoch 43 Loss: 0.5033
Epoch 44 Loss: 0.5007
Epoch 45 Loss: 0.5012
Epoch 46 Loss: 0.5034
Epoch 47 Loss: 0.4997
Epoch 48 Loss: 0.5012
Epoch 49 Loss: 0.4996
Epoch 50 Loss: 0.4996
Epoch 51 Loss: 0.5003
Epoch 52 Loss: 0.5019
Epoch 53 Loss: 0.4994
Epoch 54 Loss: 0.4993
Epoch 55 Loss: 0.4987
Epoch 56 Loss: 0.5006
Epoch 57 Loss: 0.4992
Epoch 58 Loss: 0.4994
Epoch 59 Loss: 0.4968
Epoch 60 Loss: 0.4989
Epoch 61 Loss: 0.4984
Epoch 62 Loss: 0.4992
Epoch 63 Loss: 0.4978
Epoch 64 Loss: 0.4993
Epoch 65 Loss: 0.4990
Epoch 66 Loss: 0.5004
Epoch 67 Loss: 0.4965
Epoch 68 Loss: 0.4957
Epoch 69 Loss: 0.4974
Epoch 70 Loss: 0.4980
Epoch 71 Loss: 0.4928
Epoch 72 Loss: 0.4945
Epoch 73 Loss: 0.4929
Epoch 74 Loss: 0.4915
Epoch 75 Loss: 0.4928
Epoch 76 Loss: 0.4932
Epoch 77 Loss: 0.4933
Epoch 78 Loss: 0.4900
Epoch 79 Loss: 0.4866
Epoch 80 Loss: 0.4844
Epoch 81 Loss: 0.4818
Epoch 82 Loss: 0.4789
Epoch 83 Loss: 0.4779
Epoch 84 Loss: 0.4765
Epoch 85 Loss: 0.4747
Epoch 86 Loss: 0.4700
Epoch 87 Loss: 0.4702
Epoch 88 Loss: 0.4696
Epoch 89 Loss: 0.4650
Epoch 90 Loss: 0.4643
Epoch 91 Loss: 0.4594
Epoch 92 Loss: 0.4545
Epoch 93 Loss: 0.4507
Epoch 94 Loss: 0.4488
Epoch 95 Loss: 0.5060
Epoch 96 Loss: 0.5061
Epoch 97 Loss: 0.5079
Epoch 98 Loss: 0.5049
Epoch 99 Loss: 0.5038
Epoch 100 Loss: 0.5047
Epoch 101 Loss: 0.5064
Epoch 102 Loss: 0.4994
Epoch 103 Loss: 0.4985
Epoch 104 Loss: 0.4998
Epoch 105 Loss: 0.4993
Epoch 106 Loss: 0.4984
Epoch 107 Loss: 0.5008
Epoch 108 Loss: 0.4976
Epoch 109 Loss: 0.4986
Epoch 110 Loss: 0.5003
Epoch 111 Loss: 0.5000
Epoch 112 Loss: 0.5019
Epoch 113 Loss: 0.4982
Epoch 114 Loss: 0.4991
Epoch 115 Loss: 0.4997
Epoch 116 Loss: 0.4978
Epoch 117 Loss: 0.4990
Epoch 118 Loss: 0.4964
Epoch 119 Loss: 0.4973
Epoch 120 Loss: 0.4974
Epoch 121 Loss: 0.4938
Epoch 122 Loss: 0.4983
Epoch 123 Loss: 0.4960
Epoch 124 Loss: 0.4958
Epoch 125 Loss: 0.4954
Epoch 126 Loss: 0.4962
Epoch 127 Loss: 0.4979
Epoch 128 Loss: 0.4961
Epoch 129 Loss: 0.4945
Epoch 130 Loss: 0.4944
Epoch 131 Loss: 0.4924
Epoch 132 Loss: 0.4919
Epoch 133 Loss: 0.4919
Epoch 134 Loss: 0.4928
Epoch 135 Loss: 0.4943
Epoch 136 Loss: 0.4949
Epoch 137 Loss: 0.4950
Epoch 138 Loss: 0.4938
Epoch 139 Loss: 0.4932
Epoch 140 Loss: 0.4931
Epoch 141 Loss: 0.4926
Epoch 142 Loss: 0.4911
Epoch 143 Loss: 0.4933
Epoch 144 Loss: 0.4922
Epoch 145 Loss: 0.4931
Epoch 146 Loss: 0.4920
Epoch 147 Loss: 0.4911
Epoch 148 Loss: 0.4896
Epoch 149 Loss: 0.4897
Epoch 150 Loss: 0.4891
Epoch 151 Loss: 0.4891
Epoch 152 Loss: 0.4866
Epoch 153 Loss: 0.4903
Epoch 154 Loss: 0.4856
Epoch 155 Loss: 0.4862
Epoch 156 Loss: 0.4846
Epoch 157 Loss: 0.4855
Epoch 158 Loss: 0.4864
Epoch 159 Loss: 0.4828
Epoch 160 Loss: 0.4791
Epoch 161 Loss: 0.4786
Epoch 162 Loss: 0.4756
Epoch 163 Loss: 0.4715
Epoch 164 Loss: 0.4681
Epoch 165 Loss: 0.4637
Epoch 166 Loss: 0.4589
Epoch 167 Loss: 0.4541
Epoch 168 Loss: 0.4500
Epoch 169 Loss: 0.4456
Epoch 170 Loss: 0.4400
Epoch 171 Loss: 0.4387
Epoch 172 Loss: 0.4366
Epoch 173 Loss: 0.4303
Epoch 174 Loss: 0.4259
Epoch 175 Loss: 0.4218
Epoch 176 Loss: 0.4197
Epoch 177 Loss: 0.4178
Epoch 178 Loss: 0.4169
Epoch 179 Loss: 0.4127
Epoch 179 best val f1 0.5080 test f1 0.31939026713371277
Epoch 180 Loss: 0.4084
Epoch 181 Loss: 0.4088
Epoch 182 Loss: 0.4098
Epoch 183 Loss: 0.4057
Epoch 184 Loss: 0.4050
Epoch 185 Loss: 0.4031
Epoch 186 Loss: 0.4018
Epoch 187 Loss: 0.4028
Epoch 188 Loss: 0.4068
Epoch 189 Loss: 0.4042
Epoch 190 Loss: 0.4036
Epoch 191 Loss: 0.4009
Epoch 192 Loss: 0.4059
Epoch 193 Loss: 0.4075
Epoch 194 Loss: 0.4069
Epoch 195 Loss: 0.4045
Epoch 196 Loss: 0.4188
Epoch 197 Loss: 0.4649
Epoch 198 Loss: 0.4704
Epoch 199 Loss: 0.4641
Epoch 200 Loss: 0.4574