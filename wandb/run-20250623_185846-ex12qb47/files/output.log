/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.8976
Epoch 1 best val f1 0.4194 test f1 0.30591025948524475
Epoch 2 Loss: 0.7595
Epoch 3 Loss: 0.7077
Epoch 4 Loss: 0.6773
Epoch 5 Loss: 0.6546
Epoch 6 Loss: 0.6364
Epoch 7 Loss: 0.6212
Epoch 8 Loss: 0.6084
Epoch 9 Loss: 0.5985
Epoch 10 Loss: 0.5893
Epoch 11 Loss: 0.5799
Epoch 12 Loss: 0.5712
Epoch 13 Loss: 0.5624
Epoch 14 Loss: 0.5543
Epoch 15 Loss: 0.5467
Epoch 16 Loss: 0.5393
Epoch 17 Loss: 0.5322
Epoch 18 Loss: 0.5268
Epoch 18 best val f1 0.4348 test f1 0.30928778648376465
Epoch 19 Loss: 0.5216
Epoch 19 best val f1 0.4390 test f1 0.31026336550712585
Epoch 20 Loss: 0.5162
Epoch 20 best val f1 0.5000 test f1 0.3112070858478546
Epoch 21 Loss: 0.5107
Epoch 22 Loss: 0.5051
Epoch 23 Loss: 0.4992
Epoch 24 Loss: 0.4932
Epoch 25 Loss: 0.4871
Epoch 26 Loss: 0.4810
Epoch 27 Loss: 0.4751
Epoch 28 Loss: 0.4697
Epoch 29 Loss: 0.4642
Epoch 30 Loss: 0.4581
Epoch 31 Loss: 0.4519
Epoch 31 best val f1 0.5714 test f1 0.3364414870738983
Epoch 32 Loss: 0.4458
Epoch 33 Loss: 0.4395
Epoch 34 Loss: 0.4333
Epoch 35 Loss: 0.4281
Epoch 36 Loss: 0.4228
Epoch 37 Loss: 0.4172
Epoch 38 Loss: 0.4116
Epoch 39 Loss: 0.4058
Epoch 40 Loss: 0.4002
Epoch 41 Loss: 0.3948
Epoch 42 Loss: 0.3897
Epoch 43 Loss: 0.3847
Epoch 44 Loss: 0.3800
Epoch 45 Loss: 0.3753
Epoch 46 Loss: 0.3711
Epoch 47 Loss: 0.3669
Epoch 48 Loss: 0.3623
Epoch 49 Loss: 0.3580
Epoch 50 Loss: 0.3540
Epoch 51 Loss: 0.3500
Epoch 52 Loss: 0.3462
Epoch 53 Loss: 0.3429
Epoch 54 Loss: 0.3398
Epoch 55 Loss: 0.3368
Epoch 56 Loss: 0.3338
Epoch 57 Loss: 0.3348
Epoch 58 Loss: 0.3332
Epoch 59 Loss: 0.3298
Epoch 60 Loss: 0.3261
Epoch 61 Loss: 0.3229
Epoch 62 Loss: 0.3195
Epoch 63 Loss: 0.3166
Epoch 64 Loss: 0.3147
Epoch 65 Loss: 0.3168
Epoch 66 Loss: 0.3159
Epoch 67 Loss: 0.3143
Epoch 68 Loss: 0.3112
Epoch 69 Loss: 0.3076
Epoch 70 Loss: 0.3043
Epoch 71 Loss: 0.3015
Epoch 72 Loss: 0.2996
Epoch 73 Loss: 0.2976
Epoch 74 Loss: 0.2954
Epoch 75 Loss: 0.2928
Epoch 76 Loss: 0.2902
Epoch 77 Loss: 0.2879
Epoch 78 Loss: 0.2859
Epoch 79 Loss: 0.2843
Epoch 80 Loss: 0.2828
Epoch 81 Loss: 0.2813
Epoch 82 Loss: 0.2800
Epoch 83 Loss: 0.2785
Epoch 84 Loss: 0.2769
Epoch 85 Loss: 0.2753
Epoch 86 Loss: 0.2744
Epoch 87 Loss: 0.2732
Epoch 88 Loss: 0.2721
Epoch 89 Loss: 0.2706
Epoch 90 Loss: 0.2692
Epoch 91 Loss: 0.2863
Epoch 92 Loss: 0.2894
Epoch 93 Loss: 0.2836
Epoch 94 Loss: 0.2787
Epoch 95 Loss: 0.2750
Epoch 96 Loss: 0.2718
Epoch 97 Loss: 0.2695
Epoch 98 Loss: 0.2672
Epoch 99 Loss: 0.2648
Epoch 100 Loss: 0.2627