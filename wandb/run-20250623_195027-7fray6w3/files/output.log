/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 1.4503
Epoch 1 best val f1 0.4209 test f1 0.30244895815849304
Epoch 2 Loss: 0.6771
Epoch 3 Loss: 0.6068
Epoch 4 Loss: 0.5903
Epoch 5 Loss: 0.5824
Epoch 6 Loss: 0.5737
Epoch 7 Loss: 0.5675
Epoch 8 Loss: 0.5610
Epoch 9 Loss: 0.5544
Epoch 10 Loss: 0.5488
Epoch 11 Loss: 0.5424
Epoch 12 Loss: 0.5370
Epoch 13 Loss: 0.5309
Epoch 14 Loss: 0.5247
Epoch 15 Loss: 0.5217
Epoch 16 Loss: 0.5198
Epoch 17 Loss: 0.5176
Epoch 18 Loss: 0.5143
Epoch 19 Loss: 0.5096
Epoch 20 Loss: 0.5062
Epoch 20 best val f1 0.4984 test f1 0.31124457716941833
Epoch 21 Loss: 0.5026
Epoch 21 best val f1 0.5047 test f1 0.31071609258651733
Epoch 22 Loss: 0.4995
Epoch 23 Loss: 0.4979
Epoch 24 Loss: 0.5002
Epoch 25 Loss: 0.4973
Epoch 26 Loss: 0.4933
Epoch 27 Loss: 0.4889
Epoch 28 Loss: 0.4948
Epoch 29 Loss: 0.4875
Epoch 30 Loss: 0.4783
Epoch 31 Loss: 0.4669
Epoch 32 Loss: 0.4611
Epoch 33 Loss: 0.4534
Epoch 34 Loss: 0.4521
Epoch 35 Loss: 0.4404
Epoch 36 Loss: 0.4390
Epoch 37 Loss: 0.4365
Epoch 38 Loss: 0.4309
Epoch 38 best val f1 0.5093 test f1 0.26303187012672424
Epoch 39 Loss: 0.4208
Epoch 39 best val f1 0.5123 test f1 0.2598481774330139
Epoch 40 Loss: 0.4155
Epoch 41 Loss: 0.4117
Epoch 42 Loss: 0.4003
Epoch 43 Loss: 0.3913
Epoch 44 Loss: 0.3858
Epoch 44 best val f1 0.5181 test f1 0.2468768060207367
Epoch 45 Loss: 0.3755
Epoch 45 best val f1 0.5305 test f1 0.24467048048973083
Epoch 46 Loss: 0.3765
Epoch 47 Loss: 0.3717
Epoch 48 Loss: 0.3661
Epoch 49 Loss: 0.3624
Epoch 50 Loss: 0.3682
Epoch 51 Loss: 0.3644
Epoch 52 Loss: 0.3502
Epoch 52 best val f1 0.5341 test f1 0.23543310165405273
Epoch 53 Loss: 0.3410
Epoch 54 Loss: 0.3349
Epoch 55 Loss: 0.3298
Epoch 56 Loss: 0.3224
Epoch 57 Loss: 0.3149
Epoch 58 Loss: 0.3109
Epoch 59 Loss: 0.3160
Epoch 59 best val f1 0.5641 test f1 0.23523367941379547
Epoch 60 Loss: 0.3115
Epoch 61 Loss: 0.3020
Epoch 62 Loss: 0.3095
Epoch 63 Loss: 0.3081
Epoch 64 Loss: 0.3139
Epoch 65 Loss: 0.3151
Epoch 66 Loss: 0.3123
Epoch 67 Loss: 0.3027
Epoch 68 Loss: 0.3044
Epoch 69 Loss: 0.3291
Epoch 70 Loss: 0.3847
Epoch 71 Loss: 0.3638
Epoch 72 Loss: 0.3451
Epoch 73 Loss: 0.3285
Epoch 74 Loss: 0.3186
Epoch 75 Loss: 0.3192
Epoch 76 Loss: 0.3226
Epoch 77 Loss: 0.3138
Epoch 78 Loss: 0.3071
Epoch 79 Loss: 0.2983
Epoch 80 Loss: 0.2943
Epoch 81 Loss: 0.2860
Epoch 82 Loss: 0.2802
Epoch 83 Loss: 0.2757
Epoch 84 Loss: 0.2716
Epoch 85 Loss: 0.2682
Epoch 86 Loss: 0.2664
Epoch 87 Loss: 0.2639
Epoch 88 Loss: 0.2629
Epoch 89 Loss: 0.2620
Epoch 90 Loss: 0.2783
Epoch 91 Loss: 0.3123
Epoch 92 Loss: 0.3243
Epoch 93 Loss: 0.3338
Epoch 94 Loss: 0.3239
Epoch 95 Loss: 0.3099
Epoch 96 Loss: 0.2990
Epoch 97 Loss: 0.2939
Epoch 98 Loss: 0.2933
Epoch 99 Loss: 0.2870
Epoch 100 Loss: 0.2801