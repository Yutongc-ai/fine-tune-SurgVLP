/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 1.3684
Epoch 1 best val f1 0.4049 test f1 0.30574533343315125
Epoch 2 Loss: 0.8717
Epoch 3 Loss: 0.6999
Epoch 4 Loss: 0.6621
Epoch 5 Loss: 0.6391
Epoch 6 Loss: 0.6252
Epoch 6 best val f1 0.4076 test f1 0.3058023154735565
Epoch 7 Loss: 0.6134
Epoch 8 Loss: 0.6030
Epoch 9 Loss: 0.5937
Epoch 10 Loss: 0.5872
Epoch 11 Loss: 0.5804
Epoch 12 Loss: 0.5735
Epoch 13 Loss: 0.5687
Epoch 14 Loss: 0.5646
Epoch 15 Loss: 0.5616
Epoch 16 Loss: 0.5566
Epoch 16 best val f1 0.4437 test f1 0.30884742736816406
Epoch 17 Loss: 0.5530
Epoch 17 best val f1 0.5016 test f1 0.30974018573760986
Epoch 18 Loss: 0.5515
Epoch 19 Loss: 0.5482
Epoch 20 Loss: 0.5440
Epoch 21 Loss: 0.5437
Epoch 22 Loss: 0.5414
Epoch 23 Loss: 0.5380
Epoch 24 Loss: 0.5372
Epoch 25 Loss: 0.5363
Epoch 26 Loss: 0.5329
Epoch 27 Loss: 0.5342
Epoch 28 Loss: 0.5307
Epoch 29 Loss: 0.5298
Epoch 30 Loss: 0.5273
Epoch 31 Loss: 0.5276
Epoch 32 Loss: 0.5263
Epoch 33 Loss: 0.5256
Epoch 34 Loss: 0.5245
Epoch 35 Loss: 0.5223
Epoch 36 Loss: 0.5246
Epoch 37 Loss: 0.5204
Epoch 38 Loss: 0.5174
Epoch 39 Loss: 0.5161
Epoch 40 Loss: 0.5175
Epoch 41 Loss: 0.5150
Epoch 42 Loss: 0.5269
Epoch 43 Loss: 0.5269
Epoch 44 Loss: 0.5264
Epoch 45 Loss: 0.5297
Epoch 46 Loss: 0.5273
Epoch 47 Loss: 0.5278
Epoch 48 Loss: 0.5272
Epoch 49 Loss: 0.5269
Epoch 50 Loss: 0.5258
Epoch 51 Loss: 0.5262
Epoch 52 Loss: 0.5242
Epoch 53 Loss: 0.5245
Epoch 54 Loss: 0.5256
Epoch 55 Loss: 0.5224
Epoch 56 Loss: 0.5237
Epoch 57 Loss: 0.5216
Epoch 58 Loss: 0.5222
Epoch 59 Loss: 0.5195
Epoch 60 Loss: 0.5200
Epoch 61 Loss: 0.5255
Epoch 62 Loss: 0.5271
Epoch 63 Loss: 0.5280
Epoch 64 Loss: 0.5252
Epoch 65 Loss: 0.5284
Epoch 66 Loss: 0.5276
Epoch 67 Loss: 0.5238
Epoch 68 Loss: 0.5234
Epoch 69 Loss: 0.5241
Epoch 70 Loss: 0.5238
Epoch 71 Loss: 0.5238
Epoch 72 Loss: 0.5220
Epoch 73 Loss: 0.5238
Epoch 74 Loss: 0.5205
Epoch 75 Loss: 0.5193
Epoch 76 Loss: 0.5172
Epoch 77 Loss: 0.5187
Epoch 78 Loss: 0.5192
Epoch 79 Loss: 0.5171
Epoch 80 Loss: 0.5183
Epoch 81 Loss: 0.5159
Epoch 82 Loss: 0.5147
Epoch 83 Loss: 0.5155
Epoch 84 Loss: 0.5174
Epoch 85 Loss: 0.5149
Epoch 86 Loss: 0.5169
Epoch 87 Loss: 0.5150
Epoch 88 Loss: 0.5131
Epoch 89 Loss: 0.5117
Epoch 90 Loss: 0.5147
Epoch 91 Loss: 0.5121
Epoch 92 Loss: 0.5132
Epoch 93 Loss: 0.5127
Epoch 94 Loss: 0.5107
Epoch 95 Loss: 0.5115
Epoch 96 Loss: 0.5106
Epoch 97 Loss: 0.5099
Epoch 98 Loss: 0.5123
Epoch 99 Loss: 0.5132
Epoch 100 Loss: 0.5103