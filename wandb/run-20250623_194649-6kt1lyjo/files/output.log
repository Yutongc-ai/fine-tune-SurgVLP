/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 1.6890
Epoch 1 best val f1 0.4241 test f1 0.28450000286102295
Epoch 2 Loss: 0.8138
Epoch 3 Loss: 0.6348
Epoch 4 Loss: 0.6080
Epoch 5 Loss: 0.5972
Epoch 5 best val f1 0.4259 test f1 0.2885163426399231
Epoch 6 Loss: 0.5887
Epoch 7 Loss: 0.5813
Epoch 8 Loss: 0.5744
Epoch 9 Loss: 0.5686
Epoch 10 Loss: 0.5614
Epoch 11 Loss: 0.5559
Epoch 12 Loss: 0.5510
Epoch 13 Loss: 0.5449
Epoch 14 Loss: 0.5422
Epoch 15 Loss: 0.5366
Epoch 16 Loss: 0.5298
Epoch 17 Loss: 0.5257
Epoch 18 Loss: 0.5214
Epoch 19 Loss: 0.5180
Epoch 19 best val f1 0.4889 test f1 0.33414220809936523
Epoch 20 Loss: 0.5150
Epoch 20 best val f1 0.5077 test f1 0.3356536328792572
Epoch 21 Loss: 0.5126
Epoch 22 Loss: 0.5107
Epoch 23 Loss: 0.5079
Epoch 24 Loss: 0.5065
Epoch 25 Loss: 0.5061
Epoch 26 Loss: 0.5066
Epoch 27 Loss: 0.5073
Epoch 28 Loss: 0.5046
Epoch 29 Loss: 0.5045
Epoch 30 Loss: 0.5031
Epoch 31 Loss: 0.5045
Epoch 32 Loss: 0.5024
Epoch 33 Loss: 0.5030
Epoch 34 Loss: 0.5043
Epoch 35 Loss: 0.5018
Epoch 36 Loss: 0.5022
Epoch 37 Loss: 0.5024
Epoch 38 Loss: 0.5008
Epoch 39 Loss: 0.5045
Epoch 40 Loss: 0.5071
Epoch 41 Loss: 0.5063
Epoch 42 Loss: 0.5074
Epoch 43 Loss: 0.5053
Epoch 44 Loss: 0.5048
Epoch 45 Loss: 0.5075
Epoch 46 Loss: 0.5050
Epoch 47 Loss: 0.5032
Epoch 48 Loss: 0.5004
Epoch 49 Loss: 0.5008
Epoch 50 Loss: 0.5016
Epoch 51 Loss: 0.5006
Epoch 52 Loss: 0.4975
Epoch 53 Loss: 0.4971
Epoch 54 Loss: 0.4965
Epoch 55 Loss: 0.4986
Epoch 56 Loss: 0.4971
Epoch 57 Loss: 0.4966
Epoch 58 Loss: 0.4946
Epoch 59 Loss: 0.4942
Epoch 60 Loss: 0.4952
Epoch 61 Loss: 0.4969
Epoch 62 Loss: 0.4948
Epoch 63 Loss: 0.4949
Epoch 64 Loss: 0.4948
Epoch 65 Loss: 0.4920
Epoch 66 Loss: 0.4940
Epoch 67 Loss: 0.4922
Epoch 68 Loss: 0.4926
Epoch 69 Loss: 0.4916
Epoch 70 Loss: 0.4917
Epoch 71 Loss: 0.4906
Epoch 72 Loss: 0.4887
Epoch 73 Loss: 0.4884
Epoch 74 Loss: 0.4891
Epoch 75 Loss: 0.4893
Epoch 76 Loss: 0.4891
Epoch 77 Loss: 0.4900
Epoch 78 Loss: 0.4877
Epoch 79 Loss: 0.4872
Epoch 80 Loss: 0.4869
Epoch 81 Loss: 0.4852
Epoch 82 Loss: 0.4840
Epoch 83 Loss: 0.4832
Epoch 84 Loss: 0.4821
Epoch 85 Loss: 0.4799
Epoch 86 Loss: 0.4799
Epoch 87 Loss: 0.4768
Epoch 88 Loss: 0.4723
Epoch 89 Loss: 0.4717
Epoch 90 Loss: 0.4706
Epoch 91 Loss: 0.4700
Epoch 92 Loss: 0.4702
Epoch 93 Loss: 0.4639
Epoch 94 Loss: 0.4608
Epoch 95 Loss: 0.4591
Epoch 96 Loss: 0.4582
Epoch 97 Loss: 0.4567
Epoch 98 Loss: 0.4535
Epoch 99 Loss: 0.4513
Epoch 100 Loss: 0.4482