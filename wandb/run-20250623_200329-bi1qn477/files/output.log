/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 0.8653
Epoch 1 best val f1 0.3951 test f1 0.3057743310928345
Epoch 2 Loss: 0.8461
Epoch 3 Loss: 0.8292
Epoch 4 Loss: 0.8136
Epoch 5 Loss: 0.7986
Epoch 6 Loss: 0.7859
Epoch 7 Loss: 0.7738
Epoch 8 Loss: 0.7642
Epoch 9 Loss: 0.7541
Epoch 10 Loss: 0.7454
Epoch 11 Loss: 0.7372
Epoch 12 Loss: 0.7293
Epoch 12 best val f1 0.3955 test f1 0.3057743310928345
Epoch 13 Loss: 0.7220
Epoch 14 Loss: 0.7152
Epoch 14 best val f1 0.3963 test f1 0.3057757318019867
Epoch 15 Loss: 0.7090
Epoch 15 best val f1 0.3975 test f1 0.30577999353408813
Epoch 16 Loss: 0.7033
Epoch 16 best val f1 0.4017 test f1 0.3057842552661896
Epoch 17 Loss: 0.6963
Epoch 17 best val f1 0.4042 test f1 0.30581405758857727
Epoch 18 Loss: 0.6905
Epoch 18 best val f1 0.4069 test f1 0.30587655305862427
Epoch 19 Loss: 0.6849
Epoch 19 best val f1 0.4100 test f1 0.30596891045570374
Epoch 20 Loss: 0.6795
Epoch 20 best val f1 0.4160 test f1 0.3061111569404602
Epoch 21 Loss: 0.6745
Epoch 21 best val f1 0.4172 test f1 0.3063852787017822
Epoch 22 Loss: 0.6689
Epoch 22 best val f1 0.4224 test f1 0.3068012297153473
Epoch 23 Loss: 0.6632
Epoch 23 best val f1 0.4284 test f1 0.30748748779296875
Epoch 24 Loss: 0.6578
Epoch 24 best val f1 0.4361 test f1 0.3086076080799103
Epoch 25 Loss: 0.6517
Epoch 25 best val f1 0.4493 test f1 0.31024694442749023
Epoch 26 Loss: 0.6459
Epoch 27 Loss: 0.6405
Epoch 28 Loss: 0.6348
Epoch 29 Loss: 0.6282
Epoch 30 Loss: 0.6217
Epoch 31 Loss: 0.6146
Epoch 32 Loss: 0.6098
Epoch 33 Loss: 0.6016
Epoch 34 Loss: 0.5948
Epoch 35 Loss: 0.5882
Epoch 36 Loss: 0.5823
Epoch 36 best val f1 0.4516 test f1 0.3643158972263336
Epoch 37 Loss: 0.5753
Epoch 37 best val f1 0.4545 test f1 0.3687524199485779
Epoch 38 Loss: 0.5684
Epoch 39 Loss: 0.5644
Epoch 40 Loss: 0.5579
Epoch 41 Loss: 0.5536
Epoch 42 Loss: 0.5496
Epoch 43 Loss: 0.5459
Epoch 44 Loss: 0.5441
Epoch 45 Loss: 0.5428
Epoch 46 Loss: 0.5393
Epoch 47 Loss: 0.5359
Epoch 48 Loss: 0.5340
Epoch 49 Loss: 0.5346
Epoch 50 Loss: 0.5322
Epoch 51 Loss: 0.5310
Epoch 52 Loss: 0.5311
Epoch 53 Loss: 0.5297
Epoch 54 Loss: 0.5291
Epoch 55 Loss: 0.5273
Epoch 56 Loss: 0.5268
Epoch 57 Loss: 0.5263
Epoch 58 Loss: 0.5253
Epoch 59 Loss: 0.5238
Epoch 60 Loss: 0.5244
Epoch 61 Loss: 0.5212
Epoch 62 Loss: 0.5245
Epoch 63 Loss: 0.5226
Epoch 64 Loss: 0.5252
Epoch 65 Loss: 0.5199
Epoch 66 Loss: 0.5201
Epoch 67 Loss: 0.5206
Epoch 68 Loss: 0.5179
Epoch 69 Loss: 0.5185
Epoch 70 Loss: 0.5178
Epoch 71 Loss: 0.5182
Epoch 72 Loss: 0.5173
Epoch 73 Loss: 0.5164
Epoch 74 Loss: 0.5168
Epoch 75 Loss: 0.5166
Epoch 76 Loss: 0.5167
Epoch 77 Loss: 0.5168
Epoch 78 Loss: 0.5168
Epoch 79 Loss: 0.5162
Epoch 80 Loss: 0.5186
Epoch 81 Loss: 0.5167
Epoch 82 Loss: 0.5140
Epoch 83 Loss: 0.5147
Epoch 84 Loss: 0.5120
Epoch 85 Loss: 0.5134
Epoch 86 Loss: 0.5131
Epoch 87 Loss: 0.5117
Epoch 88 Loss: 0.5141
Epoch 89 Loss: 0.5193
Epoch 90 Loss: 0.5237
Epoch 91 Loss: 0.5181
Epoch 92 Loss: 0.5163
Epoch 93 Loss: 0.5177
Epoch 94 Loss: 0.5150
Epoch 95 Loss: 0.5133
Epoch 96 Loss: 0.5120
Epoch 97 Loss: 0.5128
Epoch 98 Loss: 0.5134
Epoch 99 Loss: 0.5127
Epoch 100 Loss: 0.5158