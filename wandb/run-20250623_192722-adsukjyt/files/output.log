/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  4
num_shots is  4
Epoch 1 Loss: 1.4654
Epoch 1 best val f1 0.4000 test f1 0.30578848719596863
Epoch 2 Loss: 0.9962
Epoch 3 Loss: 0.7543
Epoch 4 Loss: 0.6464
Epoch 5 Loss: 0.6079
Epoch 6 Loss: 0.5906
Epoch 7 Loss: 0.5789
Epoch 8 Loss: 0.5703
Epoch 9 Loss: 0.5639
Epoch 9 best val f1 0.4130 test f1 0.305806964635849
Epoch 10 Loss: 0.5581
Epoch 11 Loss: 0.5532
Epoch 12 Loss: 0.5484
Epoch 13 Loss: 0.5436
Epoch 14 Loss: 0.5389
Epoch 15 Loss: 0.5343
Epoch 16 Loss: 0.5303
Epoch 17 Loss: 0.5261
Epoch 18 Loss: 0.5222
Epoch 19 Loss: 0.5185
Epoch 20 Loss: 0.5148
Epoch 21 Loss: 0.5109
Epoch 22 Loss: 0.5069
Epoch 23 Loss: 0.5032
Epoch 24 Loss: 0.4998
Epoch 25 Loss: 0.4964
Epoch 26 Loss: 0.4930
Epoch 27 Loss: 0.4898
Epoch 28 Loss: 0.4877
Epoch 29 Loss: 0.4858
Epoch 30 Loss: 0.4831
Epoch 31 Loss: 0.4805
Epoch 31 best val f1 0.4167 test f1 0.3063591420650482
Epoch 32 Loss: 0.4776
Epoch 32 best val f1 0.4416 test f1 0.30644312500953674
Epoch 33 Loss: 0.4747
Epoch 34 Loss: 0.4719
Epoch 35 Loss: 0.4690
Epoch 36 Loss: 0.4663
Epoch 37 Loss: 0.4636
Epoch 38 Loss: 0.4613
Epoch 39 Loss: 0.4587
Epoch 40 Loss: 0.4579
Epoch 41 Loss: 0.4570
Epoch 42 Loss: 0.4557
Epoch 43 Loss: 0.4539
Epoch 44 Loss: 0.4520
Epoch 45 Loss: 0.4493
Epoch 46 Loss: 0.4495
Epoch 47 Loss: 0.4515
Epoch 48 Loss: 0.4510
Epoch 49 Loss: 0.4528
Epoch 50 Loss: 0.4524
Epoch 51 Loss: 0.4509
Epoch 52 Loss: 0.4490
Epoch 53 Loss: 0.4538
Epoch 54 Loss: 0.4563
Epoch 55 Loss: 0.4571
Epoch 56 Loss: 0.4554
Epoch 57 Loss: 0.4543
Epoch 58 Loss: 0.4597
Epoch 59 Loss: 0.4635
Epoch 60 Loss: 0.4676
Epoch 61 Loss: 0.4710
Epoch 62 Loss: 0.4739
Epoch 63 Loss: 0.4758
Epoch 64 Loss: 0.4767
Epoch 65 Loss: 0.4766
Epoch 66 Loss: 0.4763
Epoch 67 Loss: 0.4755
Epoch 68 Loss: 0.4743
Epoch 69 Loss: 0.4729
Epoch 70 Loss: 0.4711
Epoch 71 Loss: 0.4698
Epoch 72 Loss: 0.4682
Epoch 73 Loss: 0.4667
Epoch 74 Loss: 0.4657
Epoch 75 Loss: 0.4643
Epoch 76 Loss: 0.4626
Epoch 77 Loss: 0.4614
Epoch 78 Loss: 0.4600
Epoch 79 Loss: 0.4589
Epoch 80 Loss: 0.4588
Epoch 81 Loss: 0.4578
Epoch 82 Loss: 0.4567
Epoch 83 Loss: 0.4549
Epoch 84 Loss: 0.4534
Epoch 85 Loss: 0.4523
Epoch 86 Loss: 0.4502
Epoch 87 Loss: 0.4482
Epoch 88 Loss: 0.4467
Epoch 89 Loss: 0.4451
Epoch 90 Loss: 0.4426
Epoch 91 Loss: 0.4418
Epoch 92 Loss: 0.4394
Epoch 93 Loss: 0.4375
Epoch 94 Loss: 0.4362
Epoch 95 Loss: 0.4340
Epoch 96 Loss: 0.4317
Epoch 97 Loss: 0.4302
Epoch 98 Loss: 0.4284
Epoch 99 Loss: 0.4267
Epoch 100 Loss: 0.4249