/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.8671
Epoch 1 best val f1 0.4194 test f1 0.3057856559753418
Epoch 2 Loss: 0.7823
Epoch 3 Loss: 0.7397
Epoch 4 Loss: 0.7096
Epoch 5 Loss: 0.6859
Epoch 6 Loss: 0.6662
Epoch 7 Loss: 0.6488
Epoch 8 Loss: 0.6335
Epoch 9 Loss: 0.6198
Epoch 10 Loss: 0.6075
Epoch 11 Loss: 0.5963
Epoch 12 Loss: 0.5861
Epoch 13 Loss: 0.5768
Epoch 14 Loss: 0.5674
Epoch 15 Loss: 0.5588
Epoch 16 Loss: 0.5504
Epoch 17 Loss: 0.5425
Epoch 18 Loss: 0.5345
Epoch 18 best val f1 0.4262 test f1 0.3068777918815613
Epoch 19 Loss: 0.5271
Epoch 19 best val f1 0.4333 test f1 0.3071662187576294
Epoch 20 Loss: 0.5200
Epoch 21 Loss: 0.5130
Epoch 21 best val f1 0.4407 test f1 0.3079765737056732
Epoch 22 Loss: 0.5067
Epoch 22 best val f1 0.4483 test f1 0.3084377646446228
Epoch 23 Loss: 0.5011
Epoch 23 best val f1 0.4561 test f1 0.30900776386260986
Epoch 24 Loss: 0.4952
Epoch 25 Loss: 0.4892
Epoch 26 Loss: 0.4832
Epoch 27 Loss: 0.4776
Epoch 28 Loss: 0.4721
Epoch 29 Loss: 0.4660
Epoch 30 Loss: 0.4600
Epoch 31 Loss: 0.4535
Epoch 32 Loss: 0.4471
Epoch 33 Loss: 0.4412
Epoch 34 Loss: 0.4351
Epoch 34 best val f1 0.4667 test f1 0.32317838072776794
Epoch 35 Loss: 0.4296
Epoch 35 best val f1 0.4828 test f1 0.3252795338630676
Epoch 36 Loss: 0.4243
Epoch 37 Loss: 0.4187
Epoch 37 best val f1 0.5000 test f1 0.32963669300079346
Epoch 38 Loss: 0.4137
Epoch 38 best val f1 0.5185 test f1 0.3317885994911194
Epoch 39 Loss: 0.4089
Epoch 40 Loss: 0.4038
Epoch 41 Loss: 0.3990
Epoch 42 Loss: 0.3949
Epoch 43 Loss: 0.3911
Epoch 44 Loss: 0.3871
Epoch 45 Loss: 0.3827
Epoch 46 Loss: 0.3788
Epoch 47 Loss: 0.3757
Epoch 48 Loss: 0.3724
Epoch 49 Loss: 0.3689
Epoch 50 Loss: 0.3654
Epoch 51 Loss: 0.3622
Epoch 52 Loss: 0.3588
Epoch 53 Loss: 0.3554
Epoch 54 Loss: 0.3518
Epoch 55 Loss: 0.3486
Epoch 56 Loss: 0.3459
Epoch 57 Loss: 0.3430
Epoch 58 Loss: 0.3404
Epoch 59 Loss: 0.3379
Epoch 60 Loss: 0.3349
Epoch 61 Loss: 0.3321
Epoch 62 Loss: 0.3481
Epoch 63 Loss: 0.3593
Epoch 64 Loss: 0.3565
Epoch 65 Loss: 0.3695
Epoch 66 Loss: 0.3733
Epoch 67 Loss: 0.3684
Epoch 68 Loss: 0.3603
Epoch 69 Loss: 0.3518
Epoch 70 Loss: 0.3437
Epoch 71 Loss: 0.3370
Epoch 72 Loss: 0.3314
Epoch 73 Loss: 0.3261
Epoch 74 Loss: 0.3214
Epoch 75 Loss: 0.3173
Epoch 76 Loss: 0.3137
Epoch 76 best val f1 0.5385 test f1 0.3838955760002136
Epoch 77 Loss: 0.3103
Epoch 78 Loss: 0.3070
Epoch 79 Loss: 0.3038
Epoch 80 Loss: 0.3008
Epoch 81 Loss: 0.2980
Epoch 82 Loss: 0.2957
Epoch 83 Loss: 0.2935
Epoch 84 Loss: 0.2916
Epoch 85 Loss: 0.2896
Epoch 86 Loss: 0.2879
Epoch 87 Loss: 0.2863
Epoch 88 Loss: 0.2848
Epoch 88 best val f1 0.5455 test f1 0.384179949760437
Epoch 89 Loss: 0.2835
Epoch 90 Loss: 0.2822
Epoch 91 Loss: 0.2811
Epoch 91 best val f1 0.5714 test f1 0.38531285524368286
Epoch 92 Loss: 0.2832
Epoch 93 Loss: 0.2864
Epoch 94 Loss: 0.2868
Epoch 95 Loss: 0.2859
Epoch 96 Loss: 0.2843
Epoch 97 Loss: 0.2826
Epoch 98 Loss: 0.2827
Epoch 99 Loss: 0.2821
Epoch 100 Loss: 0.2805