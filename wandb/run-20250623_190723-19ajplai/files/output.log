/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.9936
Epoch 1 best val f1 0.4194 test f1 0.3044293224811554
Epoch 2 Loss: 0.7522
Epoch 3 Loss: 0.6943
Epoch 4 Loss: 0.6640
Epoch 5 Loss: 0.6402
Epoch 6 Loss: 0.6213
Epoch 7 Loss: 0.6049
Epoch 8 Loss: 0.5918
Epoch 9 Loss: 0.5816
Epoch 10 Loss: 0.5730
Epoch 11 Loss: 0.5649
Epoch 11 best val f1 0.4615 test f1 0.303985059261322
Epoch 12 Loss: 0.5574
Epoch 13 Loss: 0.5502
Epoch 14 Loss: 0.5433
Epoch 15 Loss: 0.5365
Epoch 16 Loss: 0.5297
Epoch 17 Loss: 0.5229
Epoch 18 Loss: 0.5163
Epoch 19 Loss: 0.5096
Epoch 20 Loss: 0.5027
Epoch 21 Loss: 0.4956
Epoch 22 Loss: 0.4890
Epoch 23 Loss: 0.4822
Epoch 24 Loss: 0.4752
Epoch 25 Loss: 0.4684
Epoch 26 Loss: 0.4613
Epoch 27 Loss: 0.4541
Epoch 28 Loss: 0.4469
Epoch 29 Loss: 0.4396
Epoch 30 Loss: 0.4325
Epoch 31 Loss: 0.4268
Epoch 32 Loss: 0.4216
Epoch 33 Loss: 0.4165
Epoch 34 Loss: 0.4103
Epoch 34 best val f1 0.6000 test f1 0.32236555218696594
Epoch 35 Loss: 0.4035
Epoch 36 Loss: 0.3968
Epoch 37 Loss: 0.3900
Epoch 38 Loss: 0.3832
Epoch 39 Loss: 0.3775
Epoch 40 Loss: 0.3726
Epoch 41 Loss: 0.3677
Epoch 42 Loss: 0.3626
Epoch 43 Loss: 0.3572
Epoch 44 Loss: 0.3519
Epoch 45 Loss: 0.3467
Epoch 46 Loss: 0.3418
Epoch 47 Loss: 0.3372
Epoch 48 Loss: 0.3329
Epoch 49 Loss: 0.3286
Epoch 50 Loss: 0.3254
Epoch 51 Loss: 0.3226
Epoch 52 Loss: 0.3193
Epoch 53 Loss: 0.3161
Epoch 54 Loss: 0.3129
Epoch 55 Loss: 0.3101
Epoch 56 Loss: 0.3075
Epoch 57 Loss: 0.3049
Epoch 58 Loss: 0.3024
Epoch 59 Loss: 0.3004
Epoch 60 Loss: 0.2982
Epoch 61 Loss: 0.2962
Epoch 62 Loss: 0.2945
Epoch 63 Loss: 0.2932
Epoch 64 Loss: 0.2910
Epoch 65 Loss: 0.2896
Epoch 66 Loss: 0.2878
Epoch 67 Loss: 0.2861
Epoch 68 Loss: 0.2845
Epoch 69 Loss: 0.2838
Epoch 70 Loss: 0.2827
Epoch 71 Loss: 0.2821
Epoch 72 Loss: 0.2801
Epoch 73 Loss: 0.2783
Epoch 74 Loss: 0.2774
Epoch 75 Loss: 0.2761
Epoch 76 Loss: 0.2873
Epoch 77 Loss: 0.2913
Epoch 78 Loss: 0.2885
Epoch 79 Loss: 0.2842
Epoch 80 Loss: 0.3188
Epoch 81 Loss: 0.3355
Epoch 82 Loss: 0.3299
Epoch 83 Loss: 0.3216
Epoch 84 Loss: 0.3151
Epoch 85 Loss: 0.3086
Epoch 86 Loss: 0.3021
Epoch 87 Loss: 0.2965
Epoch 88 Loss: 0.2918
Epoch 89 Loss: 0.2874
Epoch 90 Loss: 0.2838
Epoch 91 Loss: 0.2817
Epoch 92 Loss: 0.2788
Epoch 93 Loss: 0.2760
Epoch 94 Loss: 0.2737
Epoch 95 Loss: 0.2710
Epoch 96 Loss: 0.2684
Epoch 97 Loss: 0.2662
Epoch 98 Loss: 0.2643
Epoch 99 Loss: 0.2627
Epoch 100 Loss: 0.2608