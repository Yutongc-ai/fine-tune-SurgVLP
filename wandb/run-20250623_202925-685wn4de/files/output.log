/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 1.3192
Epoch 1 best val f1 0.4049 test f1 0.30576983094215393
Epoch 2 Loss: 0.9023
Epoch 3 Loss: 0.7096
Epoch 4 Loss: 0.6667
Epoch 5 Loss: 0.6428
Epoch 6 Loss: 0.6270
Epoch 6 best val f1 0.4118 test f1 0.3058263957500458
Epoch 7 Loss: 0.6141
Epoch 8 Loss: 0.6065
Epoch 9 Loss: 0.5994
Epoch 10 Loss: 0.5924
Epoch 11 Loss: 0.5860
Epoch 12 Loss: 0.5798
Epoch 13 Loss: 0.5752
Epoch 14 Loss: 0.5705
Epoch 15 Loss: 0.5649
Epoch 16 Loss: 0.5626
Epoch 17 Loss: 0.5586
Epoch 17 best val f1 0.4421 test f1 0.30885812640190125
Epoch 18 Loss: 0.5563
Epoch 18 best val f1 0.5016 test f1 0.3092948794364929
Epoch 19 Loss: 0.5531
Epoch 20 Loss: 0.5517
Epoch 21 Loss: 0.5484
Epoch 22 Loss: 0.5452
Epoch 23 Loss: 0.5453
Epoch 24 Loss: 0.5445
Epoch 25 Loss: 0.5411
Epoch 26 Loss: 0.5423
Epoch 27 Loss: 0.5402
Epoch 28 Loss: 0.5390
Epoch 29 Loss: 0.5374
Epoch 30 Loss: 0.5361
Epoch 31 Loss: 0.5339
Epoch 32 Loss: 0.5342
Epoch 33 Loss: 0.5348
Epoch 34 Loss: 0.5347
Epoch 35 Loss: 0.5333
Epoch 36 Loss: 0.5319
Epoch 37 Loss: 0.5334
Epoch 38 Loss: 0.5310
Epoch 39 Loss: 0.5298
Epoch 40 Loss: 0.5304
Epoch 41 Loss: 0.5277
Epoch 42 Loss: 0.5288
Epoch 43 Loss: 0.5269
Epoch 44 Loss: 0.5263
Epoch 45 Loss: 0.5256
Epoch 46 Loss: 0.5250
Epoch 47 Loss: 0.5252
Epoch 48 Loss: 0.5262
Epoch 49 Loss: 0.5316
Epoch 50 Loss: 0.5277
Epoch 51 Loss: 0.5256
Epoch 52 Loss: 0.5256
Epoch 53 Loss: 0.5261
Epoch 54 Loss: 0.5260
Epoch 55 Loss: 0.5258
Epoch 56 Loss: 0.5209
Epoch 57 Loss: 0.5222
Epoch 58 Loss: 0.5210
Epoch 59 Loss: 0.5225
Epoch 60 Loss: 0.5224
Epoch 61 Loss: 0.5179
Epoch 62 Loss: 0.5188
Epoch 63 Loss: 0.5196
Epoch 64 Loss: 0.5185
Epoch 65 Loss: 0.5183
Epoch 66 Loss: 0.5167
Epoch 67 Loss: 0.5169
Epoch 68 Loss: 0.5182
Epoch 69 Loss: 0.5173
Epoch 70 Loss: 0.5163
Epoch 71 Loss: 0.5169
Epoch 72 Loss: 0.5145
Epoch 73 Loss: 0.5146
Epoch 74 Loss: 0.5148
Epoch 75 Loss: 0.5102
Epoch 76 Loss: 0.5092
Epoch 77 Loss: 0.5087
Epoch 78 Loss: 0.5088
Epoch 79 Loss: 0.5094
Epoch 80 Loss: 0.5065
Epoch 81 Loss: 0.5052
Epoch 82 Loss: 0.5059
Epoch 83 Loss: 0.5021
Epoch 84 Loss: 0.5017
Epoch 85 Loss: 0.4991
Epoch 86 Loss: 0.4968
Epoch 87 Loss: 0.4975
Epoch 88 Loss: 0.4915
Epoch 89 Loss: 0.4880
Epoch 90 Loss: 0.4876
Epoch 91 Loss: 0.4840
Epoch 92 Loss: 0.4806
Epoch 93 Loss: 0.4797
Epoch 94 Loss: 0.4758
Epoch 95 Loss: 0.4732
Epoch 96 Loss: 0.4691
Epoch 97 Loss: 0.4661
Epoch 98 Loss: 0.4624
Epoch 99 Loss: 0.4599
Epoch 100 Loss: 0.4578