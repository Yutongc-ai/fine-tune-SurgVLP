/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  16
num_shots is  16
Epoch 1 Loss: 0.9591
Epoch 1 best val f1 0.0724 test f1 0.3057743310928345
Epoch 2 Loss: 0.5782
Epoch 3 Loss: 0.5766
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
Epoch 4 Loss: 0.5825
Epoch 5 Loss: 0.5705
Epoch 6 Loss: 0.5522
Epoch 7 Loss: 0.5413
Epoch 8 Loss: 0.5370
Epoch 9 Loss: 0.5380
Epoch 9 best val f1 0.1511 test f1 0.3057743310928345
Epoch 10 Loss: 0.5353
Epoch 10 best val f1 0.3736 test f1 0.3057743310928345
Epoch 11 Loss: 0.5297
Epoch 11 best val f1 0.4271 test f1 0.3057743310928345
Epoch 12 Loss: 0.5223
Epoch 12 best val f1 0.4408 test f1 0.3057743310928345
Epoch 13 Loss: 0.5159
Epoch 14 Loss: 0.5057
Epoch 14 best val f1 0.4459 test f1 0.3057743310928345
Epoch 15 Loss: 0.5019
Epoch 16 Loss: 0.5001
Epoch 17 Loss: 0.4949
Epoch 18 Loss: 0.4951
Epoch 19 Loss: 0.4952
Epoch 20 Loss: 0.4936
Epoch 21 Loss: 0.4955
Epoch 22 Loss: 0.4992
Epoch 23 Loss: 0.4985
Epoch 24 Loss: 0.4983
Epoch 25 Loss: 0.5005
Epoch 26 Loss: 0.5017
Epoch 27 Loss: 0.4987
Epoch 28 Loss: 0.5016
Epoch 29 Loss: 0.5006
Epoch 30 Loss: 0.5002
Epoch 31 Loss: 0.4988
Epoch 32 Loss: 0.4995
Epoch 33 Loss: 0.4988
Epoch 34 Loss: 0.5025
Epoch 35 Loss: 0.5013
Epoch 36 Loss: 0.5030
Epoch 37 Loss: 0.5028
Epoch 38 Loss: 0.5028
Epoch 39 Loss: 0.5024
Epoch 40 Loss: 0.5027
Epoch 41 Loss: 0.5009
Epoch 42 Loss: 0.5013
Epoch 43 Loss: 0.5017
Epoch 44 Loss: 0.5009
Epoch 45 Loss: 0.5018
Epoch 46 Loss: 0.5023
Epoch 47 Loss: 0.5018
Epoch 48 Loss: 0.5025
Epoch 49 Loss: 0.5012
Epoch 50 Loss: 0.5013
Epoch 51 Loss: 0.5006
Epoch 52 Loss: 0.5026
Epoch 53 Loss: 0.5021
Epoch 54 Loss: 0.5013
Epoch 55 Loss: 0.5009
Epoch 56 Loss: 0.5005
Epoch 57 Loss: 0.5027
Epoch 58 Loss: 0.5021
Epoch 59 Loss: 0.5026
Epoch 60 Loss: 0.5044
Epoch 61 Loss: 0.5026
Epoch 62 Loss: 0.4992
Epoch 63 Loss: 0.5012
Epoch 64 Loss: 0.5024
Epoch 65 Loss: 0.4997
Epoch 66 Loss: 0.5012
Epoch 67 Loss: 0.5011
Epoch 68 Loss: 0.5009
Epoch 69 Loss: 0.5019
Epoch 70 Loss: 0.5015
Epoch 71 Loss: 0.5010
Epoch 72 Loss: 0.5028
Epoch 73 Loss: 0.5036
Epoch 74 Loss: 0.5031
Epoch 75 Loss: 0.5007
Epoch 76 Loss: 0.5023
Epoch 77 Loss: 0.5014
Epoch 78 Loss: 0.5015
Epoch 79 Loss: 0.4995
Epoch 80 Loss: 0.5032
Epoch 81 Loss: 0.4992
Epoch 82 Loss: 0.4995
Epoch 83 Loss: 0.5038
Epoch 84 Loss: 0.5027
Epoch 85 Loss: 0.5012
Epoch 86 Loss: 0.5001
Epoch 87 Loss: 0.5050
Epoch 88 Loss: 0.5013
Epoch 89 Loss: 0.5016
Epoch 90 Loss: 0.5015
Epoch 91 Loss: 0.5020
Epoch 92 Loss: 0.5014
Epoch 93 Loss: 0.5008
Epoch 94 Loss: 0.4988
Epoch 95 Loss: 0.5020
Epoch 96 Loss: 0.5001
Epoch 97 Loss: 0.5011
Epoch 98 Loss: 0.4999
Epoch 99 Loss: 0.5006
Epoch 100 Loss: 0.5011
Epoch 101 Loss: 0.5002
Epoch 102 Loss: 0.5032
Epoch 103 Loss: 0.5020
Epoch 104 Loss: 0.5016
Epoch 105 Loss: 0.5009
Epoch 106 Loss: 0.5011
Epoch 107 Loss: 0.5015
Epoch 108 Loss: 0.5031
Epoch 109 Loss: 0.5010
Epoch 110 Loss: 0.5026
Epoch 111 Loss: 0.5010
Epoch 112 Loss: 0.5036
Epoch 113 Loss: 0.5006
Epoch 114 Loss: 0.5038
Epoch 115 Loss: 0.5008
Epoch 116 Loss: 0.5033
Epoch 117 Loss: 0.5030
Epoch 118 Loss: 0.5009
Epoch 119 Loss: 0.5036
Epoch 120 Loss: 0.5026
Epoch 121 Loss: 0.5015
Epoch 122 Loss: 0.5007
Epoch 123 Loss: 0.5029
Epoch 124 Loss: 0.5020
Epoch 125 Loss: 0.5008
Epoch 126 Loss: 0.4996
Epoch 127 Loss: 0.5014
Epoch 128 Loss: 0.5017
Epoch 129 Loss: 0.5016
Epoch 130 Loss: 0.5019
Epoch 131 Loss: 0.5018
Epoch 132 Loss: 0.5016
Epoch 133 Loss: 0.5002
Epoch 134 Loss: 0.5006
Epoch 135 Loss: 0.5007
Epoch 136 Loss: 0.5006
Epoch 137 Loss: 0.5028
Epoch 138 Loss: 0.5031
Epoch 139 Loss: 0.5019
Epoch 140 Loss: 0.5026
Epoch 141 Loss: 0.5008
Epoch 142 Loss: 0.5016
Epoch 143 Loss: 0.5010
Epoch 144 Loss: 0.5013
Epoch 145 Loss: 0.5027
Epoch 146 Loss: 0.5012
Epoch 147 Loss: 0.5003
Epoch 148 Loss: 0.5017
Epoch 149 Loss: 0.5014
Epoch 150 Loss: 0.4999
Epoch 151 Loss: 0.5014
Epoch 152 Loss: 0.5016
Epoch 153 Loss: 0.5002
Epoch 154 Loss: 0.5010
Epoch 155 Loss: 0.5029
Epoch 156 Loss: 0.5000
Epoch 157 Loss: 0.5014
Epoch 158 Loss: 0.5003
Epoch 159 Loss: 0.5004
Epoch 160 Loss: 0.5003
Epoch 161 Loss: 0.4999
Epoch 162 Loss: 0.4984
Epoch 163 Loss: 0.5026
Epoch 164 Loss: 0.4986
Epoch 165 Loss: 0.5034
Epoch 166 Loss: 0.5006
Epoch 167 Loss: 0.5028
Epoch 168 Loss: 0.5024
Epoch 169 Loss: 0.5022
Epoch 170 Loss: 0.5023
Epoch 171 Loss: 0.4998
Epoch 172 Loss: 0.5001
Epoch 173 Loss: 0.5001
Epoch 174 Loss: 0.5005
Epoch 175 Loss: 0.5016
Epoch 176 Loss: 0.4996
Epoch 177 Loss: 0.5023
Epoch 178 Loss: 0.5024
Epoch 179 Loss: 0.4990
Epoch 180 Loss: 0.5016
Epoch 181 Loss: 0.5043
Epoch 182 Loss: 0.5019
Epoch 183 Loss: 0.5004
Epoch 184 Loss: 0.4993
Epoch 185 Loss: 0.5012
Epoch 186 Loss: 0.4993
Epoch 187 Loss: 0.5007
Epoch 188 Loss: 0.4989
Epoch 189 Loss: 0.5003
Epoch 190 Loss: 0.5012
Epoch 191 Loss: 0.5012
Epoch 192 Loss: 0.5022
Epoch 193 Loss: 0.5044
Epoch 194 Loss: 0.5034
Epoch 195 Loss: 0.5021
Epoch 196 Loss: 0.5011
Epoch 197 Loss: 0.4997
Epoch 198 Loss: 0.5028
Epoch 199 Loss: 0.5033
Epoch 200 Loss: 0.4983