/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.8360
Epoch 1 best val f1 0.4444 test f1 0.3057870864868164
Epoch 2 Loss: 0.8266
Epoch 3 Loss: 0.8202
Epoch 4 Loss: 0.8139
Epoch 5 Loss: 0.8063
Epoch 6 Loss: 0.7942
Epoch 7 Loss: 0.7792
Epoch 8 Loss: 0.7632
Epoch 9 Loss: 0.7468
Epoch 10 Loss: 0.7318
Epoch 11 Loss: 0.7197
Epoch 12 Loss: 0.7090
Epoch 13 Loss: 0.6993
Epoch 14 Loss: 0.6904
Epoch 15 Loss: 0.6824
Epoch 16 Loss: 0.6743
Epoch 17 Loss: 0.6663
Epoch 18 Loss: 0.6579
Epoch 19 Loss: 0.6488
Epoch 20 Loss: 0.6390
Epoch 21 Loss: 0.6282
Epoch 22 Loss: 0.6165
Epoch 23 Loss: 0.6037
Epoch 24 Loss: 0.5893
Epoch 25 Loss: 0.5728
Epoch 26 Loss: 0.5531
Epoch 27 Loss: 0.5307
Epoch 28 Loss: 0.5083
Epoch 28 best val f1 0.4667 test f1 0.30640095472335815
Epoch 29 Loss: 0.4922
Epoch 30 Loss: 0.4793
Epoch 31 Loss: 0.4642
Epoch 32 Loss: 0.4463
Epoch 33 Loss: 0.4268
Epoch 34 Loss: 0.4057
Epoch 35 Loss: 0.3837
Epoch 36 Loss: 0.3654
Epoch 37 Loss: 0.3437
Epoch 38 Loss: 0.3143
Epoch 39 Loss: 0.2799
Epoch 40 Loss: 0.2454
Epoch 41 Loss: 0.2210
Epoch 42 Loss: 0.2036
Epoch 43 Loss: 0.1869
Epoch 44 Loss: 0.1850
Epoch 45 Loss: 0.1847
Epoch 46 Loss: 0.1868
Epoch 47 Loss: 0.1922
Epoch 48 Loss: 0.1940
Epoch 49 Loss: 0.1908
Epoch 50 Loss: 0.1836
Epoch 51 Loss: 0.1742
Epoch 52 Loss: 0.1648
Epoch 53 Loss: 0.1549
Epoch 54 Loss: 0.1436
Epoch 55 Loss: 0.1297
Epoch 56 Loss: 0.1137
Epoch 57 Loss: 0.0965
Epoch 58 Loss: 0.0766
Epoch 59 Loss: 0.0537
Epoch 60 Loss: 0.0400
Epoch 61 Loss: 0.0304
Epoch 62 Loss: 0.0242
Epoch 63 Loss: 0.0218
Epoch 64 Loss: 0.0294
Epoch 65 Loss: 0.0400
Epoch 66 Loss: 0.0477
Epoch 67 Loss: 0.0499
Epoch 68 Loss: 0.0473
Epoch 69 Loss: 0.0407
Epoch 70 Loss: 0.0330
Epoch 71 Loss: 0.0262
Epoch 72 Loss: 0.0218
Epoch 73 Loss: 0.0199
Epoch 74 Loss: 0.0222
Epoch 75 Loss: 0.0293
Epoch 76 Loss: 0.0371
Epoch 77 Loss: 0.0409
Epoch 78 Loss: 0.0390
Epoch 79 Loss: 0.0334
Epoch 80 Loss: 0.0278
Epoch 81 Loss: 0.0244
Epoch 82 Loss: 0.0229
Epoch 83 Loss: 0.0231
Epoch 84 Loss: 0.0256
Epoch 85 Loss: 0.0294
Epoch 86 Loss: 0.0326
Epoch 87 Loss: 0.0335
Epoch 88 Loss: 0.0321
Epoch 89 Loss: 0.0298
Epoch 90 Loss: 0.0277
Epoch 91 Loss: 0.0267
Epoch 92 Loss: 0.0267
Epoch 93 Loss: 0.0281
Epoch 94 Loss: 0.0302
Epoch 95 Loss: 0.0317
Epoch 96 Loss: 0.0321
Epoch 97 Loss: 0.0314
Epoch 98 Loss: 0.0302
Epoch 99 Loss: 0.0294
Epoch 100 Loss: 0.0292
Epoch 101 Loss: 0.0297
Epoch 102 Loss: 0.0306
Epoch 103 Loss: 0.0314
Epoch 104 Loss: 0.0317
Epoch 105 Loss: 0.0316
Epoch 106 Loss: 0.0312
Epoch 107 Loss: 0.0309
Epoch 108 Loss: 0.0314
Epoch 109 Loss: 0.0323
Epoch 110 Loss: 0.0325
Epoch 111 Loss: 0.0320
Epoch 112 Loss: 0.0313
Epoch 113 Loss: 0.0308
Epoch 114 Loss: 0.0308
Epoch 115 Loss: 0.0313
Epoch 116 Loss: 0.0322
Epoch 117 Loss: 0.0331
Epoch 118 Loss: 0.0335
Epoch 119 Loss: 0.0336
Epoch 120 Loss: 0.0333
Epoch 121 Loss: 0.0332
Epoch 122 Loss: 0.0330
Epoch 123 Loss: 0.0329
Epoch 124 Loss: 0.0329
Epoch 125 Loss: 0.0328
Epoch 126 Loss: 0.0329
Epoch 127 Loss: 0.0335
Epoch 128 Loss: 0.0342
Epoch 129 Loss: 0.0344
Epoch 130 Loss: 0.0343
Epoch 131 Loss: 0.0339
Epoch 132 Loss: 0.0336
Epoch 133 Loss: 0.0334
Epoch 134 Loss: 0.0355
Epoch 135 Loss: 0.0369
Epoch 136 Loss: 0.0363
Epoch 137 Loss: 0.0347
Epoch 138 Loss: 0.0331
Epoch 139 Loss: 0.0323
Epoch 140 Loss: 0.0327
Epoch 141 Loss: 0.0338
Epoch 142 Loss: 0.0349
Epoch 143 Loss: 0.0358
Epoch 144 Loss: 0.0363
Epoch 145 Loss: 0.0362
Epoch 146 Loss: 0.0358
Epoch 147 Loss: 0.0356
Epoch 148 Loss: 0.0355
Epoch 149 Loss: 0.0354
Epoch 150 Loss: 0.0353
Epoch 151 Loss: 0.0351
Epoch 152 Loss: 0.0350
Epoch 153 Loss: 0.0354
Epoch 154 Loss: 0.0359
Epoch 155 Loss: 0.0361
Epoch 156 Loss: 0.0363
Epoch 157 Loss: 0.0364
Epoch 158 Loss: 0.0385
Epoch 159 Loss: 0.0398
Epoch 160 Loss: 0.0433
Epoch 161 Loss: 0.0442
Epoch 162 Loss: 0.0385
Epoch 163 Loss: 0.0307
Epoch 164 Loss: 0.0254
Epoch 165 Loss: 0.0243
Epoch 166 Loss: 0.0268
Epoch 167 Loss: 0.0316
Epoch 168 Loss: 0.0367
Epoch 169 Loss: 0.0409
Epoch 170 Loss: 0.0426
Epoch 171 Loss: 0.0420
Epoch 172 Loss: 0.0398
Epoch 173 Loss: 0.0369
Epoch 174 Loss: 0.0344
Epoch 175 Loss: 0.0330
Epoch 176 Loss: 0.0327
Epoch 177 Loss: 0.0337
Epoch 178 Loss: 0.0354
Epoch 179 Loss: 0.0373
Epoch 180 Loss: 0.0389
Epoch 181 Loss: 0.0396
Epoch 182 Loss: 0.0394
Epoch 183 Loss: 0.0385
Epoch 184 Loss: 0.0374
Epoch 185 Loss: 0.0363
Epoch 186 Loss: 0.0357
Epoch 187 Loss: 0.0355
Epoch 188 Loss: 0.0359
Epoch 189 Loss: 0.0365
Epoch 190 Loss: 0.0372
Epoch 191 Loss: 0.0379
Epoch 192 Loss: 0.0382
Epoch 193 Loss: 0.0382
Epoch 194 Loss: 0.0381
Epoch 195 Loss: 0.0377
Epoch 196 Loss: 0.0372
Epoch 197 Loss: 0.0368
Epoch 198 Loss: 0.0367
Epoch 199 Loss: 0.0367
Epoch 200 Loss: 0.0370