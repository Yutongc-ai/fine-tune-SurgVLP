/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  4
num_shots is  4
Epoch 1 Loss: 0.9074
Epoch 1 best val f1 0.4444 test f1 0.3057785630226135
Epoch 2 Loss: 0.8459
Epoch 3 Loss: 0.8070
Epoch 4 Loss: 0.7782
Epoch 5 Loss: 0.7562
Epoch 6 Loss: 0.7381
Epoch 7 Loss: 0.7232
Epoch 8 Loss: 0.7106
Epoch 9 Loss: 0.6996
Epoch 10 Loss: 0.6898
Epoch 11 Loss: 0.6811
Epoch 12 Loss: 0.6731
Epoch 13 Loss: 0.6657
Epoch 14 Loss: 0.6589
Epoch 15 Loss: 0.6526
Epoch 16 Loss: 0.6465
Epoch 17 Loss: 0.6407
Epoch 18 Loss: 0.6352
Epoch 19 Loss: 0.6299
Epoch 20 Loss: 0.6247
Epoch 21 Loss: 0.6196
Epoch 22 Loss: 0.6178
Epoch 22 best val f1 0.4462 test f1 0.30765849351882935
Epoch 23 Loss: 0.6169
Epoch 23 best val f1 0.4498 test f1 0.3080897033214569
Epoch 24 Loss: 0.6161
Epoch 24 best val f1 0.4534 test f1 0.30856993794441223
Epoch 25 Loss: 0.6149
Epoch 25 best val f1 0.4609 test f1 0.30908799171447754
Epoch 26 Loss: 0.6135
Epoch 26 best val f1 0.4641 test f1 0.30970990657806396
Epoch 27 Loss: 0.6116
Epoch 27 best val f1 0.4867 test f1 0.3103441894054413
Epoch 28 Loss: 0.6093
Epoch 28 best val f1 0.5023 test f1 0.31108081340789795
Epoch 29 Loss: 0.6065
Epoch 30 Loss: 0.6033
Epoch 30 best val f1 0.5025 test f1 0.3124934732913971
Epoch 31 Loss: 0.5997
Epoch 31 best val f1 0.5075 test f1 0.3134237229824066
Epoch 32 Loss: 0.5961
Epoch 33 Loss: 0.5923
Epoch 34 Loss: 0.5885
Epoch 34 best val f1 0.5128 test f1 0.3163243532180786
Epoch 35 Loss: 0.5847
Epoch 35 best val f1 0.5131 test f1 0.31731244921684265
Epoch 36 Loss: 0.5809
Epoch 36 best val f1 0.5275 test f1 0.31815460324287415
Epoch 37 Loss: 0.5771
Epoch 37 best val f1 0.5465 test f1 0.3191796541213989
Epoch 38 Loss: 0.5733
Epoch 38 best val f1 0.5697 test f1 0.3202610909938812
Epoch 39 Loss: 0.5695
Epoch 40 Loss: 0.5658
Epoch 41 Loss: 0.5621
Epoch 42 Loss: 0.5584
Epoch 43 Loss: 0.5548
Epoch 43 best val f1 0.5800 test f1 0.3248819410800934
Epoch 44 Loss: 0.5512
Epoch 44 best val f1 0.5806 test f1 0.3258540630340576
Epoch 45 Loss: 0.5477
Epoch 46 Loss: 0.5442
Epoch 47 Loss: 0.5407
Epoch 48 Loss: 0.5373
Epoch 49 Loss: 0.5339
Epoch 50 Loss: 0.5306
Epoch 51 Loss: 0.5275
Epoch 52 Loss: 0.5241
Epoch 53 Loss: 0.5209
Epoch 54 Loss: 0.5178
Epoch 55 Loss: 0.5146
Epoch 56 Loss: 0.5115
Epoch 57 Loss: 0.5084
Epoch 58 Loss: 0.5053
Epoch 59 Loss: 0.5022
Epoch 60 Loss: 0.4992
Epoch 61 Loss: 0.4961
Epoch 62 Loss: 0.4932
Epoch 63 Loss: 0.4902
Epoch 64 Loss: 0.4873
Epoch 65 Loss: 0.4844
Epoch 66 Loss: 0.4815
Epoch 67 Loss: 0.4787
Epoch 68 Loss: 0.4760
Epoch 69 Loss: 0.4734
Epoch 70 Loss: 0.4708
Epoch 71 Loss: 0.4681
Epoch 72 Loss: 0.4653
Epoch 73 Loss: 0.4626
Epoch 74 Loss: 0.4601
Epoch 75 Loss: 0.4577
Epoch 76 Loss: 0.4552
Epoch 77 Loss: 0.4536
Epoch 78 Loss: 0.4524
Epoch 79 Loss: 0.4508
Epoch 80 Loss: 0.4489
Epoch 81 Loss: 0.4466
Epoch 82 Loss: 0.4442
Epoch 83 Loss: 0.4415
Epoch 84 Loss: 0.4387
Epoch 85 Loss: 0.4358
Epoch 86 Loss: 0.4330
Epoch 87 Loss: 0.4302
Epoch 88 Loss: 0.4275
Epoch 89 Loss: 0.4248
Epoch 90 Loss: 0.4222
Epoch 91 Loss: 0.4197
Epoch 92 Loss: 0.4172
Epoch 93 Loss: 0.4147
Epoch 94 Loss: 0.4124
Epoch 95 Loss: 0.4102
Epoch 96 Loss: 0.4080
Epoch 97 Loss: 0.4059
Epoch 98 Loss: 0.4040
Epoch 99 Loss: 0.4020
Epoch 100 Loss: 0.3999