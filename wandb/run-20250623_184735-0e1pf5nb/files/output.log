/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.8035
Epoch 1 best val f1 0.4194 test f1 0.3057757318019867
Epoch 2 Loss: 0.7792
Epoch 3 Loss: 0.7610
Epoch 4 Loss: 0.7485
Epoch 5 Loss: 0.7380
Epoch 6 Loss: 0.7283
Epoch 7 Loss: 0.7189
Epoch 8 Loss: 0.7099
Epoch 9 Loss: 0.7014
Epoch 10 Loss: 0.6934
Epoch 11 Loss: 0.6863
Epoch 12 Loss: 0.6794
Epoch 13 Loss: 0.6728
Epoch 14 Loss: 0.6666
Epoch 15 Loss: 0.6609
Epoch 16 Loss: 0.6556
Epoch 17 Loss: 0.6506
Epoch 18 Loss: 0.6457
Epoch 19 Loss: 0.6413
Epoch 20 Loss: 0.6369
Epoch 20 best val f1 0.4262 test f1 0.3061138987541199
Epoch 21 Loss: 0.6326
Epoch 22 Loss: 0.6289
Epoch 23 Loss: 0.6254
Epoch 24 Loss: 0.6220
Epoch 25 Loss: 0.6186
Epoch 25 best val f1 0.4483 test f1 0.3069872558116913
Epoch 26 Loss: 0.6153
Epoch 27 Loss: 0.6120
Epoch 28 Loss: 0.6086
Epoch 29 Loss: 0.6057
Epoch 29 best val f1 0.4615 test f1 0.30862709879875183
Epoch 30 Loss: 0.6025
Epoch 31 Loss: 0.5996
Epoch 32 Loss: 0.5966
Epoch 33 Loss: 0.5937
Epoch 34 Loss: 0.5910
Epoch 35 Loss: 0.5880
Epoch 36 Loss: 0.5851
Epoch 37 Loss: 0.5823
Epoch 38 Loss: 0.5795
Epoch 39 Loss: 0.5767
Epoch 40 Loss: 0.5738
Epoch 41 Loss: 0.5709
Epoch 42 Loss: 0.5681
Epoch 43 Loss: 0.5654
Epoch 44 Loss: 0.5625
Epoch 45 Loss: 0.5596
Epoch 46 Loss: 0.5570
Epoch 47 Loss: 0.5542
Epoch 48 Loss: 0.5514
Epoch 49 Loss: 0.5488
Epoch 50 Loss: 0.5459
Epoch 51 Loss: 0.5429
Epoch 52 Loss: 0.5402
Epoch 53 Loss: 0.5409
Epoch 54 Loss: 0.5414
Epoch 55 Loss: 0.5408
Epoch 56 Loss: 0.5391
Epoch 57 Loss: 0.5364
Epoch 58 Loss: 0.5330
Epoch 59 Loss: 0.5293
Epoch 60 Loss: 0.5255
Epoch 61 Loss: 0.5215
Epoch 62 Loss: 0.5176
Epoch 63 Loss: 0.5137
Epoch 64 Loss: 0.5101
Epoch 65 Loss: 0.5064
Epoch 66 Loss: 0.5033
Epoch 67 Loss: 0.5001
Epoch 68 Loss: 0.4969
Epoch 69 Loss: 0.4938
Epoch 70 Loss: 0.4906
Epoch 71 Loss: 0.4875
Epoch 72 Loss: 0.4872
Epoch 73 Loss: 0.4872
Epoch 74 Loss: 0.4863
Epoch 75 Loss: 0.4848
Epoch 76 Loss: 0.4827
Epoch 77 Loss: 0.4800
Epoch 78 Loss: 0.4770
Epoch 79 Loss: 0.4737
Epoch 80 Loss: 0.4704
Epoch 81 Loss: 0.4668
Epoch 82 Loss: 0.4636
Epoch 83 Loss: 0.4603
Epoch 84 Loss: 0.4569
Epoch 85 Loss: 0.4535
Epoch 86 Loss: 0.4504
Epoch 87 Loss: 0.4474
Epoch 88 Loss: 0.4443
Epoch 89 Loss: 0.4414
Epoch 90 Loss: 0.4388
Epoch 91 Loss: 0.4360
Epoch 92 Loss: 0.4333
Epoch 93 Loss: 0.4306
Epoch 94 Loss: 0.4281
Epoch 95 Loss: 0.4257
Epoch 96 Loss: 0.4234
Epoch 97 Loss: 0.4212
Epoch 98 Loss: 0.4191
Epoch 99 Loss: 0.4168
Epoch 100 Loss: 0.4146