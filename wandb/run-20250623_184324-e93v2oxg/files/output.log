/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.9905
Epoch 1 best val f1 0.4194 test f1 0.3061457872390747
Epoch 2 Loss: 0.7637
Epoch 3 Loss: 0.7088
Epoch 4 Loss: 0.6712
Epoch 5 Loss: 0.6390
Epoch 6 Loss: 0.6074
Epoch 7 Loss: 0.5791
Epoch 8 Loss: 0.5526
Epoch 9 Loss: 0.5303
Epoch 10 Loss: 0.5135
Epoch 11 Loss: 0.5016
Epoch 12 Loss: 0.4920
Epoch 13 Loss: 0.4831
Epoch 14 Loss: 0.4742
Epoch 15 Loss: 0.4652
Epoch 15 best val f1 0.4667 test f1 0.3060149848461151
Epoch 16 Loss: 0.4558
Epoch 17 Loss: 0.4460
Epoch 18 Loss: 0.4348
Epoch 19 Loss: 0.4215
Epoch 20 Loss: 0.4053
Epoch 21 Loss: 0.3863
Epoch 22 Loss: 0.3650
Epoch 23 Loss: 0.3441
Epoch 23 best val f1 0.5556 test f1 0.3068012297153473
Epoch 24 Loss: 0.3264
Epoch 25 Loss: 0.3135
Epoch 25 best val f1 0.6000 test f1 0.3073080778121948
Epoch 26 Loss: 0.3030
Epoch 27 Loss: 0.2932
Epoch 28 Loss: 0.2840
Epoch 29 Loss: 0.2760
Epoch 30 Loss: 0.2697
Epoch 31 Loss: 0.2649
Epoch 32 Loss: 0.2615
Epoch 33 Loss: 0.2587
Epoch 34 Loss: 0.2563
Epoch 35 Loss: 0.2535
Epoch 36 Loss: 0.2513
Epoch 37 Loss: 0.2478
Epoch 38 Loss: 0.2430
Epoch 39 Loss: 0.2370
Epoch 40 Loss: 0.2301
Epoch 41 Loss: 0.2243
Epoch 42 Loss: 0.2176
Epoch 43 Loss: 0.2099
Epoch 43 best val f1 0.6316 test f1 0.31999728083610535
Epoch 44 Loss: 0.2025
Epoch 45 Loss: 0.1953
Epoch 46 Loss: 0.1888
Epoch 47 Loss: 0.1831
Epoch 48 Loss: 0.1781
Epoch 49 Loss: 0.1733
Epoch 50 Loss: 0.1688
Epoch 51 Loss: 0.1646
Epoch 52 Loss: 0.1623
Epoch 53 Loss: 0.1592
Epoch 54 Loss: 0.1558
Epoch 55 Loss: 0.1520
Epoch 56 Loss: 0.1479
Epoch 57 Loss: 0.1436
Epoch 58 Loss: 0.1393
Epoch 59 Loss: 0.1349
Epoch 60 Loss: 0.1302
Epoch 61 Loss: 0.1248
Epoch 62 Loss: 0.1192
Epoch 63 Loss: 0.1136
Epoch 64 Loss: 0.1078
Epoch 65 Loss: 0.1121
Epoch 66 Loss: 0.1070
Epoch 67 Loss: 0.0983
Epoch 68 Loss: 0.0948
Epoch 69 Loss: 0.0965
Epoch 70 Loss: 0.0959
Epoch 71 Loss: 0.0934
Epoch 72 Loss: 0.0903
Epoch 73 Loss: 0.0869
Epoch 74 Loss: 0.0839
Epoch 75 Loss: 0.0816
Epoch 76 Loss: 0.0805
Epoch 77 Loss: 0.0802
Epoch 78 Loss: 0.0804
Epoch 79 Loss: 0.0898
Epoch 80 Loss: 0.0890
Epoch 81 Loss: 0.0805
Epoch 82 Loss: 0.0740
Epoch 83 Loss: 0.0706
Epoch 84 Loss: 0.0695
Epoch 85 Loss: 0.0703
Epoch 86 Loss: 0.0715
Epoch 87 Loss: 0.0724
Epoch 88 Loss: 0.0724
Epoch 89 Loss: 0.0715
Epoch 90 Loss: 0.0703
Epoch 91 Loss: 0.0686
Epoch 92 Loss: 0.0673
Epoch 93 Loss: 0.0665
Epoch 94 Loss: 0.0662
Epoch 95 Loss: 0.0669
Epoch 96 Loss: 0.0673
Epoch 97 Loss: 0.0666
Epoch 98 Loss: 0.0651
Epoch 99 Loss: 0.0634
Epoch 100 Loss: 0.0626
Epoch 101 Loss: 0.0623
Epoch 102 Loss: 0.0626
Epoch 103 Loss: 0.0627
Epoch 104 Loss: 0.0631
Epoch 105 Loss: 0.0625
Epoch 106 Loss: 0.0660
Epoch 107 Loss: 0.0651
Epoch 108 Loss: 0.0606
Epoch 109 Loss: 0.0587
Epoch 110 Loss: 0.0586
Epoch 111 Loss: 0.0597
Epoch 112 Loss: 0.0597
Epoch 113 Loss: 0.0588
Epoch 114 Loss: 0.0599
Epoch 115 Loss: 0.0579
Epoch 116 Loss: 0.0563
Epoch 117 Loss: 0.0561
Epoch 118 Loss: 0.0569
Epoch 119 Loss: 0.0580
Epoch 120 Loss: 0.0579
Epoch 121 Loss: 0.0633
Epoch 122 Loss: 0.0579
Epoch 123 Loss: 0.0518
Epoch 124 Loss: 0.0505
Epoch 125 Loss: 0.0541
Epoch 126 Loss: 0.0577
Epoch 127 Loss: 0.0571
Epoch 128 Loss: 0.0538
Epoch 129 Loss: 0.0510
Epoch 130 Loss: 0.0509
Epoch 131 Loss: 0.0530
Epoch 132 Loss: 0.0544
Epoch 133 Loss: 0.0551
Epoch 134 Loss: 0.0522
Epoch 135 Loss: 0.0499
Epoch 136 Loss: 0.0496
Epoch 137 Loss: 0.0503
Epoch 138 Loss: 0.0507
Epoch 139 Loss: 0.0496
Epoch 140 Loss: 0.0484
Epoch 141 Loss: 0.0482
Epoch 142 Loss: 0.0491
Epoch 143 Loss: 0.0498
Epoch 144 Loss: 0.0490
Epoch 145 Loss: 0.0481
Epoch 146 Loss: 0.0479
Epoch 147 Loss: 0.0476
Epoch 148 Loss: 0.0474
Epoch 149 Loss: 0.0469
Epoch 150 Loss: 0.0476
Epoch 151 Loss: 0.0461
Epoch 152 Loss: 0.0451
Epoch 153 Loss: 0.0457
Epoch 154 Loss: 0.0460
Epoch 155 Loss: 0.1084
Epoch 156 Loss: 0.0672
Epoch 157 Loss: 0.0386
Epoch 158 Loss: 0.0241
Epoch 159 Loss: 0.0279
Epoch 160 Loss: 0.0421
Epoch 161 Loss: 0.0542
Epoch 162 Loss: 0.0557
Epoch 163 Loss: 0.0493
Epoch 164 Loss: 0.0420
Epoch 165 Loss: 0.0382
Epoch 166 Loss: 0.0393
Epoch 167 Loss: 0.0435
Epoch 168 Loss: 0.0466
Epoch 169 Loss: 0.0464
Epoch 170 Loss: 0.0438
Epoch 171 Loss: 0.0409
Epoch 172 Loss: 0.0385
Epoch 173 Loss: 0.0376
Epoch 174 Loss: 0.0381
Epoch 175 Loss: 0.0392
Epoch 176 Loss: 0.0402
Epoch 177 Loss: 0.0410
Epoch 178 Loss: 0.0415
Epoch 179 Loss: 0.0417
Epoch 180 Loss: 0.0409
Epoch 181 Loss: 0.0398
Epoch 182 Loss: 0.0387
Epoch 183 Loss: 0.0385
Epoch 184 Loss: 0.0387
Epoch 185 Loss: 0.0388
Epoch 186 Loss: 0.0388
Epoch 187 Loss: 0.0385
Epoch 188 Loss: 0.0383
Epoch 189 Loss: 0.0384
Epoch 190 Loss: 0.0387
Epoch 191 Loss: 0.0391
Epoch 192 Loss: 0.0394
Epoch 193 Loss: 0.0392
Epoch 194 Loss: 0.0387
Epoch 195 Loss: 0.0381
Epoch 196 Loss: 0.0377
Epoch 197 Loss: 0.0375
Epoch 198 Loss: 0.0377
Epoch 199 Loss: 0.0378
Epoch 200 Loss: 0.0381