/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  4
num_shots is  4
Epoch 1 Loss: 0.8505
Epoch 1 best val f1 0.3934 test f1 0.3057743310928345
Epoch 2 Loss: 0.8364
Epoch 3 Loss: 0.8237
Epoch 4 Loss: 0.8121
Epoch 5 Loss: 0.8015
Epoch 6 Loss: 0.7919
Epoch 7 Loss: 0.7828
Epoch 8 Loss: 0.7743
Epoch 9 Loss: 0.7665
Epoch 10 Loss: 0.7593
Epoch 11 Loss: 0.7525
Epoch 12 Loss: 0.7461
Epoch 13 Loss: 0.7402
Epoch 14 Loss: 0.7344
Epoch 15 Loss: 0.7290
Epoch 16 Loss: 0.7239
Epoch 17 Loss: 0.7191
Epoch 18 Loss: 0.7146
Epoch 19 Loss: 0.7102
Epoch 20 Loss: 0.7061
Epoch 21 Loss: 0.7029
Epoch 21 best val f1 0.3951 test f1 0.3057771325111389
Epoch 22 Loss: 0.7002
Epoch 22 best val f1 0.4000 test f1 0.3057771325111389
Epoch 23 Loss: 0.6978
Epoch 24 Loss: 0.6952
Epoch 25 Loss: 0.6926
Epoch 26 Loss: 0.6898
Epoch 27 Loss: 0.6870
Epoch 28 Loss: 0.6842
Epoch 29 Loss: 0.6814
Epoch 29 best val f1 0.4034 test f1 0.3058154881000519
Epoch 30 Loss: 0.6786
Epoch 30 best val f1 0.4052 test f1 0.3058311343193054
Epoch 31 Loss: 0.6756
Epoch 32 Loss: 0.6727
Epoch 33 Loss: 0.6698
Epoch 33 best val f1 0.4071 test f1 0.3059447407722473
Epoch 34 Loss: 0.6670
Epoch 34 best val f1 0.4107 test f1 0.30599310994148254
Epoch 35 Loss: 0.6642
Epoch 35 best val f1 0.4163 test f1 0.30608126521110535
Epoch 36 Loss: 0.6614
Epoch 36 best val f1 0.4220 test f1 0.3062100410461426
Epoch 37 Loss: 0.6586
Epoch 38 Loss: 0.6561
Epoch 38 best val f1 0.4265 test f1 0.3065391778945923
Epoch 39 Loss: 0.6535
Epoch 40 Loss: 0.6510
Epoch 41 Loss: 0.6485
Epoch 42 Loss: 0.6461
Epoch 43 Loss: 0.6436
Epoch 43 best val f1 0.4316 test f1 0.30856388807296753
Epoch 44 Loss: 0.6412
Epoch 45 Loss: 0.6388
Epoch 45 best val f1 0.4348 test f1 0.3099563717842102
Epoch 46 Loss: 0.6365
Epoch 47 Loss: 0.6341
Epoch 47 best val f1 0.4458 test f1 0.3119315803050995
Epoch 48 Loss: 0.6317
Epoch 48 best val f1 0.4540 test f1 0.313138872385025
Epoch 49 Loss: 0.6294
Epoch 49 best val f1 0.4557 test f1 0.31448251008987427
Epoch 50 Loss: 0.6271
Epoch 50 best val f1 0.4575 test f1 0.31600692868232727
Epoch 51 Loss: 0.6249
Epoch 51 best val f1 0.4698 test f1 0.3176957666873932
Epoch 52 Loss: 0.6227
Epoch 52 best val f1 0.4828 test f1 0.31973007321357727
Epoch 53 Loss: 0.6205
Epoch 54 Loss: 0.6183
Epoch 54 best val f1 0.4853 test f1 0.3238844871520996
Epoch 55 Loss: 0.6161
Epoch 55 best val f1 0.5038 test f1 0.3266089856624603
Epoch 56 Loss: 0.6139
Epoch 57 Loss: 0.6117
Epoch 58 Loss: 0.6095
Epoch 58 best val f1 0.5120 test f1 0.33575862646102905
Epoch 59 Loss: 0.6074
Epoch 59 best val f1 0.5289 test f1 0.3390209972858429
Epoch 60 Loss: 0.6052
Epoch 60 best val f1 0.5378 test f1 0.34253785014152527
Epoch 61 Loss: 0.6031
Epoch 61 best val f1 0.5439 test f1 0.3462376892566681
Epoch 62 Loss: 0.6009
Epoch 63 Loss: 0.5988
Epoch 63 best val f1 0.5556 test f1 0.3537421226501465
Epoch 64 Loss: 0.5966
Epoch 65 Loss: 0.5945
Epoch 66 Loss: 0.5925
Epoch 66 best val f1 0.5631 test f1 0.3661375343799591
Epoch 67 Loss: 0.5904
Epoch 68 Loss: 0.5883
Epoch 69 Loss: 0.5862
Epoch 70 Loss: 0.5842
Epoch 71 Loss: 0.5821
Epoch 72 Loss: 0.5807
Epoch 73 Loss: 0.5797
Epoch 74 Loss: 0.5786
Epoch 75 Loss: 0.5776
Epoch 76 Loss: 0.5763
Epoch 77 Loss: 0.5749
Epoch 78 Loss: 0.5734
Epoch 79 Loss: 0.5717
Epoch 80 Loss: 0.5699
Epoch 81 Loss: 0.5679
Epoch 82 Loss: 0.5658
Epoch 83 Loss: 0.5635
Epoch 84 Loss: 0.5611
Epoch 85 Loss: 0.5588
Epoch 86 Loss: 0.5566
Epoch 87 Loss: 0.5543
Epoch 88 Loss: 0.5520
Epoch 89 Loss: 0.5497
Epoch 90 Loss: 0.5474
Epoch 91 Loss: 0.5451
Epoch 92 Loss: 0.5428
Epoch 93 Loss: 0.5404
Epoch 94 Loss: 0.5380
Epoch 95 Loss: 0.5356
Epoch 96 Loss: 0.5332
Epoch 97 Loss: 0.5308
Epoch 98 Loss: 0.5285
Epoch 99 Loss: 0.5262
Epoch 100 Loss: 0.5239