/home/yongxuan/anaconda3/envs/clip/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yongxuan/SurgVLP/datasets/utils.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(f"{cfg.cache_dir}/{split}local_f_part{part_idx}.pt")
loading num parts:  0
loading num parts:  1
/home/yongxuan/SurgVLP/datasets/utils.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  labels = torch.load(f"{cfg.cache_dir}/{split}local_l_part{part_idx}.pt")
loading num parts:  2
loading num parts:  3
loading num parts:  4
num_shots is  1
num_shots is  1
Epoch 1 Loss: 0.8369
Epoch 1 best val f1 0.4444 test f1 0.3057870864868164
Epoch 2 Loss: 0.8146
Epoch 3 Loss: 0.7972
Epoch 4 Loss: 0.7823
Epoch 5 Loss: 0.7697
Epoch 6 Loss: 0.7581
Epoch 7 Loss: 0.7480
Epoch 8 Loss: 0.7385
Epoch 9 Loss: 0.7299
Epoch 10 Loss: 0.7220
Epoch 11 Loss: 0.7147
Epoch 12 Loss: 0.7081
Epoch 13 Loss: 0.7019
Epoch 14 Loss: 0.6960
Epoch 15 Loss: 0.6906
Epoch 16 Loss: 0.6854
Epoch 17 Loss: 0.6806
Epoch 18 Loss: 0.6759
Epoch 19 Loss: 0.6712
Epoch 20 Loss: 0.6667
Epoch 21 Loss: 0.6625
Epoch 21 best val f1 0.4516 test f1 0.30591273307800293
Epoch 22 Loss: 0.6585
Epoch 23 Loss: 0.6545
Epoch 24 Loss: 0.6506
Epoch 25 Loss: 0.6468
Epoch 26 Loss: 0.6430
Epoch 27 Loss: 0.6392
Epoch 27 best val f1 0.4590 test f1 0.30622637271881104
Epoch 28 Loss: 0.6355
Epoch 28 best val f1 0.4667 test f1 0.30633893609046936
Epoch 29 Loss: 0.6318
Epoch 30 Loss: 0.6283
Traceback (most recent call last):
  File "few_shot.py", line 76, in <module>
    main(configs[args.method], args.method)
  File "few_shot.py", line 52, in main
    metrics = method(dataset)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yongxuan/SurgVLP/methods/linear_probe_plus.py", line 308, in forward
    test_metrics = self.get_test_metrics()
  File "/home/yongxuan/SurgVLP/methods/linear_probe_plus.py", line 78, in get_test_metrics
    attn_batch_features = self.model.attention_pooling(batch_features, self.templates)  # (bs, 768)
  File "/home/yongxuan/SurgVLP/surgvlp/codes/models/algorithms/SurgVLP.py", line 105, in attention_pooling
    _, feats_templates, _ = self.extract_feat_text(ids=input_ids, attn_mask=attention_masks, token_type=token_type_ids)
  File "/home/yongxuan/SurgVLP/surgvlp/codes/models/algorithms/SurgVLP.py", line 73, in extract_feat_text
    text_emb_l, text_emb_g, sents = self.backbone_text(ids, attn_mask, token_type)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yongxuan/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yongxuan/SurgVLP/surgvlp/codes/models/backbones/text_backbones.py", line 119, in forward
    embeddings, sents = self.aggregate_tokens(embeddings, ids)
  File "/home/yongxuan/SurgVLP/surgvlp/codes/models/backbones/text_backbones.py", line 60, in aggregate_tokens
    word = self.idxtoword[word_id.item()]
KeyboardInterrupt